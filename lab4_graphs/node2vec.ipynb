{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-YkV7fjUZLa",
        "outputId": "e0c68593-c680-4c30-af21-c8fd8985d906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "%matplotlib inline\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q pyg-lib -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import glob\n",
        "from torch_geometric.nn import Node2Vec\n",
        "from google.colab import drive\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxexcXYIjeDk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9835d8ce-a4f6-4fe6-963f-1d93265a2490"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LNa5bNNjgMT"
      },
      "outputs": [],
      "source": [
        "path_edges = \"/content/drive/My Drive/UChicago/Tesis/node2vec/edges_node2vec_tfidf10/\"\n",
        "# Use glob to find all the part files\n",
        "path_edges = glob.glob(path_edges + \"part-*.csv\")\n",
        "\n",
        "nodes_path = '/content/drive/My Drive/UChicago/Tesis/node2vec/nodes_node2vec_tfidf10.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32HhdMdejyc4"
      },
      "source": [
        "Create graph from data, following Pytorch geometric documentation: https://pytorch-geometric.readthedocs.io/en/latest/tutorial/load_csv.html\n",
        "\n",
        "First, load nodes into pytorch objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfa_h9cWjrtR"
      },
      "outputs": [],
      "source": [
        "#Function from documentation\n",
        "def load_node_parquet(path, index_col, encoders=None, **kwargs):\n",
        "    df = pd.read_csv(path, index_col=index_col, **kwargs)\n",
        "    mapping = {index: i for i, index in enumerate(df.index.unique())}\n",
        "\n",
        "    x = None\n",
        "    if encoders is not None:\n",
        "        xs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
        "        x = torch.cat(xs, dim=-1)\n",
        "\n",
        "    return x, mapping\n",
        "\n",
        "#Class from documentation\n",
        "class IdentityEncoder:\n",
        "    def __init__(self, dtype=None):\n",
        "        self.dtype = dtype\n",
        "\n",
        "    def __call__(self, df):\n",
        "        return torch.from_numpy(df.values).view(-1, 1).to(self.dtype)\n",
        "\n",
        "words_x, words_mapping = load_node_parquet(nodes_path,\n",
        "                                           index_col='node',\n",
        "                                           encoders={'emo_pos': IdentityEncoder(dtype=torch.long),\n",
        "                                                     'emo_anx': IdentityEncoder(dtype=torch.long),\n",
        "                                                     'emo_sad': IdentityEncoder(dtype=torch.long),\n",
        "                                                     'emo_anger': IdentityEncoder(dtype=torch.long),\n",
        "                                                     'moral': IdentityEncoder(dtype=torch.long)})\n",
        "\n",
        "#Function from documentation, modified to read multiple csv\n",
        "def load_edge_csv(src_index_col, src_mapping, dst_index_col, dst_mapping,\n",
        "                  encoders=None, **kwargs):\n",
        "    df = pd.concat((pd.read_csv(f, **kwargs) for f in path_edges))\n",
        "\n",
        "    src = [src_mapping[index] for index in df[src_index_col]]\n",
        "    dst = [dst_mapping[index] for index in df[dst_index_col]]\n",
        "    edge_index = torch.tensor([src, dst])\n",
        "\n",
        "    edge_attr = None\n",
        "    if encoders is not None:\n",
        "        edge_attrs = [encoder(df[col]) for col, encoder in encoders.items()]\n",
        "        edge_attr = torch.cat(edge_attrs, dim=-1)\n",
        "\n",
        "    return edge_index, edge_attr\n",
        "\n",
        "\n",
        "edge_index, edge_label = load_edge_csv(\n",
        "    src_index_col='node1_norm',\n",
        "    src_mapping=words_mapping,\n",
        "    dst_index_col='node2_norm',\n",
        "    dst_mapping=words_mapping,\n",
        "    encoders={'weight': IdentityEncoder(dtype=torch.long)},\n",
        ")\n",
        "\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "undirected_edge_index = to_undirected(edge_index)\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "data = Data(x=words_x, edge_index=undirected_edge_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4gWYr_Hl-KB"
      },
      "source": [
        "## Train Node2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cwEuOsOG2ZL"
      },
      "outputs": [],
      "source": [
        "#Function from documentation\n",
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for pos_rw, neg_rw in loader:\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Viqu2JQNl8dN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca71e42-74f4-4534-d364-f11e442d9d49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Loss: 9.6302\n",
            "Epoch: 002, Loss: 8.7561\n",
            "Epoch: 003, Loss: 7.9589\n",
            "Epoch: 004, Loss: 7.2244\n",
            "Epoch: 005, Loss: 6.5272\n",
            "Epoch: 006, Loss: 5.9150\n",
            "Epoch: 007, Loss: 5.3974\n",
            "Epoch: 008, Loss: 4.9157\n",
            "Epoch: 009, Loss: 4.4864\n",
            "Epoch: 010, Loss: 4.1276\n",
            "Epoch: 011, Loss: 3.7837\n",
            "Epoch: 012, Loss: 3.4934\n",
            "Epoch: 013, Loss: 3.2496\n",
            "Epoch: 014, Loss: 3.0248\n",
            "Epoch: 015, Loss: 2.8203\n",
            "Epoch: 016, Loss: 2.6461\n",
            "Epoch: 017, Loss: 2.4930\n",
            "Epoch: 018, Loss: 2.3617\n",
            "Epoch: 019, Loss: 2.2431\n",
            "Epoch: 020, Loss: 2.1367\n",
            "Epoch: 021, Loss: 2.0453\n",
            "Epoch: 022, Loss: 1.9705\n",
            "Epoch: 023, Loss: 1.8985\n",
            "Epoch: 024, Loss: 1.8364\n",
            "Epoch: 025, Loss: 1.7798\n",
            "Epoch: 026, Loss: 1.7331\n",
            "Epoch: 027, Loss: 1.6920\n",
            "Epoch: 028, Loss: 1.6531\n",
            "Epoch: 029, Loss: 1.6233\n",
            "Epoch: 030, Loss: 1.5943\n",
            "Epoch: 031, Loss: 1.5715\n",
            "Epoch: 032, Loss: 1.5500\n",
            "Epoch: 033, Loss: 1.5320\n",
            "Epoch: 034, Loss: 1.5146\n",
            "Epoch: 035, Loss: 1.5001\n",
            "Epoch: 036, Loss: 1.4887\n",
            "Epoch: 037, Loss: 1.4780\n",
            "Epoch: 038, Loss: 1.4679\n",
            "Epoch: 039, Loss: 1.4589\n",
            "Epoch: 040, Loss: 1.4511\n",
            "Epoch: 041, Loss: 1.4431\n",
            "Epoch: 042, Loss: 1.4365\n",
            "Epoch: 043, Loss: 1.4316\n",
            "Epoch: 044, Loss: 1.4245\n",
            "Epoch: 045, Loss: 1.4236\n",
            "Epoch: 046, Loss: 1.4194\n",
            "Epoch: 047, Loss: 1.4159\n",
            "Epoch: 048, Loss: 1.4116\n",
            "Epoch: 049, Loss: 1.4080\n",
            "Epoch: 050, Loss: 1.4063\n",
            "Epoch: 051, Loss: 1.4033\n",
            "Epoch: 052, Loss: 1.4023\n",
            "Epoch: 053, Loss: 1.3987\n",
            "Epoch: 054, Loss: 1.3981\n",
            "Epoch: 055, Loss: 1.3940\n",
            "Epoch: 056, Loss: 1.3956\n",
            "Epoch: 057, Loss: 1.3921\n",
            "Epoch: 058, Loss: 1.3905\n",
            "Epoch: 059, Loss: 1.3889\n",
            "Epoch: 060, Loss: 1.3880\n",
            "Epoch: 061, Loss: 1.3881\n",
            "Epoch: 062, Loss: 1.3865\n",
            "Epoch: 063, Loss: 1.3847\n",
            "Epoch: 064, Loss: 1.3842\n",
            "Epoch: 065, Loss: 1.3836\n",
            "Epoch: 066, Loss: 1.3828\n",
            "Epoch: 067, Loss: 1.3822\n",
            "Epoch: 068, Loss: 1.3813\n",
            "Epoch: 069, Loss: 1.3811\n",
            "Epoch: 070, Loss: 1.3814\n",
            "Epoch: 071, Loss: 1.3794\n",
            "Epoch: 072, Loss: 1.3786\n",
            "Epoch: 073, Loss: 1.3779\n",
            "Epoch: 074, Loss: 1.3786\n",
            "Epoch: 075, Loss: 1.3770\n",
            "Epoch: 076, Loss: 1.3777\n",
            "Epoch: 077, Loss: 1.3763\n",
            "Epoch: 078, Loss: 1.3761\n",
            "Epoch: 079, Loss: 1.3754\n",
            "Epoch: 080, Loss: 1.3761\n",
            "Epoch: 081, Loss: 1.3760\n",
            "Epoch: 082, Loss: 1.3757\n",
            "Epoch: 083, Loss: 1.3740\n",
            "Epoch: 084, Loss: 1.3742\n",
            "Epoch: 085, Loss: 1.3737\n",
            "Epoch: 086, Loss: 1.3732\n",
            "Epoch: 087, Loss: 1.3735\n",
            "Epoch: 088, Loss: 1.3739\n",
            "Epoch: 089, Loss: 1.3742\n",
            "Epoch: 090, Loss: 1.3740\n",
            "Epoch: 091, Loss: 1.3726\n",
            "Epoch: 092, Loss: 1.3730\n",
            "Epoch: 093, Loss: 1.3735\n",
            "Epoch: 094, Loss: 1.3726\n",
            "Epoch: 095, Loss: 1.3708\n",
            "Epoch: 096, Loss: 1.3720\n",
            "Epoch: 097, Loss: 1.3711\n",
            "Epoch: 098, Loss: 1.3723\n",
            "Epoch: 099, Loss: 1.3723\n",
            "Epoch: 100, Loss: 1.3720\n",
            "Epoch: 101, Loss: 1.3711\n",
            "Epoch: 102, Loss: 1.3713\n",
            "Epoch: 103, Loss: 1.3718\n",
            "Epoch: 104, Loss: 1.3709\n",
            "Epoch: 105, Loss: 1.3710\n",
            "Epoch: 106, Loss: 1.3709\n",
            "Epoch: 107, Loss: 1.3699\n",
            "Epoch: 108, Loss: 1.3704\n",
            "Epoch: 109, Loss: 1.3703\n",
            "Epoch: 110, Loss: 1.3699\n",
            "Epoch: 111, Loss: 1.3714\n",
            "Epoch: 112, Loss: 1.3706\n",
            "Epoch: 113, Loss: 1.3696\n",
            "Epoch: 114, Loss: 1.3705\n"
          ]
        }
      ],
      "source": [
        "model = Node2Vec(\n",
        "    data.edge_index,\n",
        "    embedding_dim=128,\n",
        "    walk_length=20,\n",
        "    context_size=10,\n",
        "    walks_per_node=10,\n",
        "    num_negative_samples=1,\n",
        "    p=1.0,\n",
        "    q=1.0,\n",
        "    sparse=True,\n",
        ").to(device)\n",
        "\n",
        "loader = model.loader(batch_size=128, shuffle=True)\n",
        "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
        "\n",
        "for epoch in range(1, 115):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings from the trained model\n",
        "embeddings = model().detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "v6JY25OCw_yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply dimensionality reduction before clustering\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "tsne_result = tsne.fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "bu5QPyjR3Xya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(2, 20):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42).fit(tsne_result)\n",
        "    silhouette_avg = silhouette_score(tsne_result, kmeans.labels_)\n",
        "    print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=42).fit(tsne_result)\n",
        "cluster_labels = kmeans.labels_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiqQspSmzUTj",
        "outputId": "618e63fd-19f4-4615-af78-12092da4d3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.4637\n",
            "Silhouette Score: 0.3757\n",
            "Silhouette Score: 0.3459\n",
            "Silhouette Score: 0.3455\n",
            "Silhouette Score: 0.3147\n",
            "Silhouette Score: 0.3316\n",
            "Silhouette Score: 0.3354\n",
            "Silhouette Score: 0.3232\n",
            "Silhouette Score: 0.3482\n",
            "Silhouette Score: 0.3291\n",
            "Silhouette Score: 0.3336\n",
            "Silhouette Score: 0.3235\n",
            "Silhouette Score: 0.3244\n",
            "Silhouette Score: 0.3308\n",
            "Silhouette Score: 0.3320\n",
            "Silhouette Score: 0.3281\n",
            "Silhouette Score: 0.3257\n",
            "Silhouette Score: 0.3306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node_df = pd.read_csv(nodes_path)\n",
        "node_df['cluster'] = cluster_labels\n",
        "\n",
        "clusters = node_df.groupby('cluster')['node'].apply(list)\n",
        "\n",
        "# Display nodes in each cluster\n",
        "for cluster_id, nodes in clusters.items():\n",
        "    print(f\"Cluster {cluster_id}:\")\n",
        "    print(nodes)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46TOjVHLxTSM",
        "outputId": "8efad34d-2317-410d-9bf8-9881e8410b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0:\n",
            "['travel', 'hope', 'slow', 'ready', 'graduate', 'worried', 'instagram', 'crazy', 'dopamine', 'ahead', 'highly', 'extra', 'affect', 'anyways', 'tend', 'childhood', 'appreciate', 'shame', 'field', 'promise', 'account', 'somehow', 'regardless', 'apps', 'constant', 'hopefully', 'rid', 'ruin', 'luck', 'shape', 'serious', 'insecure', 'wife', 'touch', 'consistently', 'option', 'horrible', 'solve', 'honestly', 'unhealthy', 'kinda', 'ex', 'workout', 'stick', 'suck', 'exact', 'lol', 'gain', 'forget', 'fine', 'none', 'totally', 'bro', 'op', 'suggest', 'respond', 'stay', 'lost', 'skin', 'education', 'ignore', 'recently', 'girlfriend', 'balance', 'boundaries', 'main', 'mention', 'per', 'soon', 'please', 'safe', 'hear', 'program', 'boring', 'sick', 'average', 'friendships', 'mood', 'yeah', 'major', 'tv', 'anymore', 'heres', 'bear', 'count', 'dude', 'personally', 'content', 'mine', 'doubt', 'handle', 'amazing', 'influence', 'setting', 'super', 'relationship', 'sad', 'plus', 'pressure', 'passion', 'uncomfortable', 'responsibility', 'head', 'hair', 'therapy', 'mess', 'whenever', 'upset', 'edit', 'fun', 'contact', 'physically', 'resource', 'youtube', 'somewhere', 'immediately', 'deserve', 'useful', 'general', 'actively', 'journaling', 'definitely', 'otherwise', 'nature', 'active', 'wait', 'forever', 'cycle', 'admit', 'excuse', 'certainly', 'clothes', 'mentally', 'circumstance', 'depend', 'describe', 'relate', 'depressed', 'train', 'remove', 'remind', 'boyfriend', 'proud', 'completely', 'adult', 'ass', 'obviously', 'far', 'catch', 'history', 'comment', 'app', 'waking', 'regret', 'tough', 'wish', 'miserable', 'offer', 'necessary', 'clearly', 'dark', 'basic', 'fulfil', 'wonder', 'public', 'student', 'truth', 'difference', 'bother', 'slowly', 'joke', 'whole', 'absolutely', 'wake', 'quickly', 'overall', 'zone', 'possibly', 'hobby', 'surround', 'actual', 'peace', 'anxious', 'similar', 'entire', 'blame', 'relax', 'moving', 'country', 'switch', 'badly', 'win', 'helpful', 'necessarily', 'reply', 'return', 'jump', 'sort', 'save', 'young', 'sorry', 'ugly', 'reflect', 'send', 'explore', 'dream', 'caring', 'therapist', 'decent', 'encourage', 'missing', 'impossible', 'broke', 'fill', 'period', 'hang', 'attractive', 'loss', 'cry', 'terrible', 'picture', 'male', 'sound', 'shower', 'fake', 'generally', 'exercising', 'acknowledge', 'naturally', 'genuinely', 'random', 'suggestion', 'shitty', 'position', 'finish', 'walk', 'esteem', 'middle', 'weird', 'continue', 'trouble', 'search', 'calm', 'club', 'several', 'lift', 'feed', 'computer', 'delete', 'unless', 'consistent', 'diet', 'connect', 'professional', 'quick', 'waiting', 'carry', 'sign', 'connection', 'regular', 'walking', 'putting', 'incredibly', 'awesome', 'fully', 'direction', 'idk', 'nobody', 'deep', 'mentioned', 'currently', 'agree', 'prove', 'abuse', 'marry', 'fitness', 'standard', 'attitude', 'text', 'beat', 'smile', 'discover', 'regularly', 'brother', 'especially', 'ton', 'define', 'laugh', 'fit', 'repeat', 'suppose', 'busy', 'although', 'addicted', 'struggled', 'half', 'barely', 'recommend', 'meaningful', 'hobbies', 'anyway', 'line', 'open', 'emotionally', 'ultimately', 'lonely', 'alcohol', 'kill', 'funny', 'meditate', 'hold', 'doctor', 'awkward', 'multiple', 'pretty', 'unfortunately', 'experienced', 'assume', 'socially', 'means', 'cool', 'version', 'google', 'aspect', 'guess', 'pull', 'seriously', 'straight', 'damn', 'weed', 'related', 'glad', 'lately', 'plenty', 'bunch']\n",
            "\n",
            "Cluster 1:\n",
            "['online', 'recognize', 'inner', 'often', 'conversation', 'productive', 'include', 'growth', 'explain', 'achieve', 'watch', 'perspective', 'character', 'trauma', 'leave', 'grow', 'sense', 'basically', 'happen', 'space', 'whether', 'two', 'lazy', 'present', 'lack', 'anyone', 'hit', 'purpose', 'college', 'act', 'understanding', 'grateful', 'university', 'always', 'set', 'progress', 'toxic', 'buy', 'perfect', 'joy', 'healthy', 'fact', 'name', 'low', 'certain', 'show', 'fat', 'accomplish', 'extremely', 'sure', 'process', 'lose', 'partner', 'pattern', 'capable', 'hand', 'phone', 'attract', 'power', 'success', 'everything', 'journal', 'couple', 'exactly', 'girl', 'exercise', 'man', 'afraid', 'choice', 'back', 'sit', 'opportunity', 'community', 'allow', 'limit', 'research', 'add', 'eat', 'deal', 'lesson', 'face', 'else', 'grade', 'apply', 'struggle', 'level', 'stand', 'bring', 'rather', 'stress', 'motivated', 'friend', 'develop', 'system', 'difficult', 'top', 'guide', 'porn', 'body', 'desire', 'money', 'example', 'worth', 'kind', 'reach', 'response', 'fix', 'accept', 'impact', 'maintain', 'actually', 'speak', 'attempt', 'area', 'study', 'physical', 'become', 'normal', 'tell', 'human', 'read', 'task', 'say', 'group', 'food', 'angry', 'energy', 'method', 'saying', 'quite', 'woman', 'future', 'cause', 'already', 'treat', 'story', 'specific', 'light', 'shit', 'course', 'drug', 'great', 'left', 'lifestyle', 'instead', 'fast', 'eye', 'result', 'create', 'answer', 'parent', 'waste', 'throw', 'decide', 'path', 'reason', 'honest', 'water', 'hate', 'interest', 'source', 'job', 'motivation', 'close', 'school', 'project', 'realize', 'miss', 'sport', 'notice', 'natural', 'family', 'free', 'pain', 'however', 'gym', 'mother', 'figure', 'successful', 'routine', 'perhaps', 'wrong', 'engage', 'late', 'aware', 'practice', 'enough', 'suffer', 'check', 'prepare', 'move', 'mistakes', 'end', 'remember', 'turn', 'trust', 'emotion', 'enjoy', 'provide', 'spend', 'confident', 'real', 'expect', 'party', 'live', 'view', 'front', 'track', 'room', 'usually', 'challenge', 'support', 'approach', 'whatever', 'city', 'able', 'probably', 'full', 'willing', 'personality', 'music', 'media', 'possible', 'social', 'break', 'respect', 'reduce', 'teach', 'drive', 'failure', 'poor', 'long', 'forward', 'thought', 'movie', 'second', 'choose', 'sometimes', 'rest', 'journey', 'receive', 'hurt', 'anger', 'avoid', 'note', 'current', 'expectation', 'addiction', 'activity', 'find', 'due', 'tool', 'finally', 'discipline', 'date', 'together', 'strong', 'consider', 'fear', 'write', 'comfort', 'ask', 'meal', 'see', 'health', 'high', 'internet', 'age', 'language', 'writing', 'confidence', 'order', 'quit', 'depression', 'word', 'simply', 'kid', 'potential', 'cold', 'skill', 'happiness', 'stupid', 'improve', 'attention', 'plan', 'piece', 'fight', 'clear', 'talk', 'degree', 'never', 'video', 'effort', 'type', 'follow', 'scared', 'list', 'succeed', 'die', 'fall', 'single', 'lie', 'fail', 'voice', 'decided', 'schedule', 'class', 'behavior', 'child', 'force', 'build', 'three', 'god', 'gone', 'meditation', 'need', 'important', 'away', 'pursue', 'amount', 'training', 'guy', 'look', 'personal', 'mindset', 'meet', 'part', 'clean', 'benefit', 'eventually', 'fault', 'moment', 'happy', 'case', 'rule', 'made', 'exist', 'constantly', 'call', 'okay', 'solution', 'state', 'coffee', 'literally', 'dad', 'judge', 'ability', 'house', 'bed', 'common', 'key', 'concept', 'mom', 'lead', 'development', 'strength', 'least', 'try', 'heart', 'easy', 'sleep', 'cut', 'comfortable', 'feeling', 'chance', 'worry', 'increase', 'tip', 'understand', 'opinion', 'run', 'come', 'short', 'home', 'interesting', 'career', 'easily', 'company', 'listen', 'begin', 'alone', 'identify', 'value', 'working', 'mental', 'smart', 'nice', 'seek', 'event', 'car', 'early', 'drink', 'either', 'pick', 'play', 'emotional', 'base', 'business', 'game', 'thinking', 'learn', 'overcome', 'knowledgeable', 'drop', 'push', 'muscle', 'information', 'meaning', 'another', 'though', 'simple', 'sex', 'message', 'side', 'environment', 'wear', 'tired', 'large', 'give', 'imagine', 'number', 'first', 'truly', 'mind', 'interested', 'society', 'belief', 'believe', 'require', 'test', 'reality', 'pay', 'term', 'true', 'seem', 'memory', 'weight', 'butt', 'last', 'place', 'individual', 'little', 'complete', 'pass', 'book', 'quality', 'brain', 'decision', 'effect', 'share', 'risk', 'ever', 'keep', 'manage', 'eating', 'anxiety', 'less', 'stop', 'making', 'almost', 'likely', 'big', 'smoke', 'beautiful', 'huge', 'fuck', 'compare', 'living']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster 0:\n",
        "['travel', 'hope', 'slow', 'ready', 'graduate', 'worried', 'instagram', 'crazy', 'dopamine', 'ahead', 'highly', 'extra', 'affect', 'anyways', 'tend', 'childhood', 'appreciate', 'shame', 'field', 'promise', 'account', 'somehow', 'regardless', 'apps', 'constant', 'hopefully', 'rid', 'ruin', 'luck', 'shape', 'serious', 'insecure', 'wife', 'touch', 'consistently', 'option', 'horrible', 'solve', 'honestly', 'unhealthy', 'kinda', 'ex', 'workout', 'stick', 'suck', 'exact', 'lol', 'gain', 'forget', 'fine', 'none', 'totally', 'bro', 'op', 'suggest', 'respond', 'stay', 'lost', 'skin', 'education', 'ignore', 'recently', 'girlfriend', 'balance', 'boundaries', 'main', 'mention', 'per', 'soon', 'please', 'safe', 'hear', 'program', 'boring', 'sick', 'average', 'friendships', 'mood', 'yeah', 'major', 'tv', 'anymore', 'heres', 'bear', 'count', 'dude', 'personally', 'content', 'mine', 'doubt', 'handle', 'amazing', 'influence', 'setting', 'super', 'relationship', 'sad', 'plus', 'pressure', 'passion', 'uncomfortable', 'responsibility', 'head', 'hair', 'therapy', 'mess', 'whenever', 'upset', 'edit', 'fun', 'contact', 'physically', 'resource', 'youtube', 'somewhere', 'immediately', 'deserve', 'useful', 'general', 'actively', 'journaling', 'definitely', 'otherwise', 'nature', 'active', 'wait', 'forever', 'cycle', 'admit', 'excuse', 'certainly', 'clothes', 'mentally', 'circumstance', 'depend', 'describe', 'relate', 'depressed', 'train', 'remove', 'remind', 'boyfriend', 'proud', 'completely', 'adult', 'ass', 'obviously', 'far', 'catch', 'history', 'comment', 'app', 'waking', 'regret', 'tough', 'wish', 'miserable', 'offer', 'necessary', 'clearly', 'dark', 'basic', 'fulfil', 'wonder', 'public', 'student', 'truth', 'difference', 'bother', 'slowly', 'joke', 'whole', 'absolutely', 'wake', 'quickly', 'overall', 'zone', 'possibly', 'hobby', 'surround', 'actual', 'peace', 'anxious', 'similar', 'entire', 'blame', 'relax', 'moving', 'country', 'switch', 'badly', 'win', 'helpful', 'necessarily', 'reply', 'return', 'jump', 'sort', 'save', 'young', 'sorry', 'ugly', 'reflect', 'send', 'explore', 'dream', 'caring', 'therapist', 'decent', 'encourage', 'missing', 'impossible', 'broke', 'fill', 'period', 'hang', 'attractive', 'loss', 'cry', 'terrible', 'picture', 'male', 'sound', 'shower', 'fake', 'generally', 'exercising', 'acknowledge', 'naturally', 'genuinely', 'random', 'suggestion', 'shitty', 'position', 'finish', 'walk', 'esteem', 'middle', 'weird', 'continue', 'trouble', 'search', 'calm', 'club', 'several', 'lift', 'feed', 'computer', 'delete', 'unless', 'consistent', 'diet', 'connect', 'professional', 'quick', 'waiting', 'carry', 'sign', 'connection', 'regular', 'walking', 'putting', 'incredibly', 'awesome', 'fully', 'direction', 'idk', 'nobody', 'deep', 'mentioned', 'currently', 'agree', 'prove', 'abuse', 'marry', 'fitness', 'standard', 'attitude', 'text', 'beat', 'smile', 'discover', 'regularly', 'brother', 'especially', 'ton', 'define', 'laugh', 'fit', 'repeat', 'suppose', 'busy', 'although', 'addicted', 'struggled', 'half', 'barely', 'recommend', 'meaningful', 'hobbies', 'anyway', 'line', 'open', 'emotionally', 'ultimately', 'lonely', 'alcohol', 'kill', 'funny', 'meditate', 'hold', 'doctor', 'awkward', 'multiple', 'pretty', 'unfortunately', 'experienced', 'assume', 'socially', 'means', 'cool', 'version', 'google', 'aspect', 'guess', 'pull', 'seriously', 'straight', 'damn', 'weed', 'related', 'glad', 'lately', 'plenty', 'bunch']\n",
        "\n",
        "Cluster 1:\n",
        "['online', 'recognize', 'inner', 'often', 'conversation', 'productive', 'include', 'growth', 'explain', 'achieve', 'watch', 'perspective', 'character', 'trauma', 'leave', 'grow', 'sense', 'basically', 'happen', 'space', 'whether', 'two', 'lazy', 'present', 'lack', 'anyone', 'hit', 'purpose', 'college', 'act', 'understanding', 'grateful', 'university', 'always', 'set', 'progress', 'toxic', 'buy', 'perfect', 'joy', 'healthy', 'fact', 'name', 'low', 'certain', 'show', 'fat', 'accomplish', 'extremely', 'sure', 'process', 'lose', 'partner', 'pattern', 'capable', 'hand', 'phone', 'attract', 'power', 'success', 'everything', 'journal', 'couple', 'exactly', 'girl', 'exercise', 'man', 'afraid', 'choice', 'back', 'sit', 'opportunity', 'community', 'allow', 'limit', 'research', 'add', 'eat', 'deal', 'lesson', 'face', 'else', 'grade', 'apply', 'struggle', 'level', 'stand', 'bring', 'rather', 'stress', 'motivated', 'friend', 'develop', 'system', 'difficult', 'top', 'guide', 'porn', 'body', 'desire', 'money', 'example', 'worth', 'kind', 'reach', 'response', 'fix', 'accept', 'impact', 'maintain', 'actually', 'speak', 'attempt', 'area', 'study', 'physical', 'become', 'normal', 'tell', 'human', 'read', 'task', 'say', 'group', 'food', 'angry', 'energy', 'method', 'saying', 'quite', 'woman', 'future', 'cause', 'already', 'treat', 'story', 'specific', 'light', 'shit', 'course', 'drug', 'great', 'left', 'lifestyle', 'instead', 'fast', 'eye', 'result', 'create', 'answer', 'parent', 'waste', 'throw', 'decide', 'path', 'reason', 'honest', 'water', 'hate', 'interest', 'source', 'job', 'motivation', 'close', 'school', 'project', 'realize', 'miss', 'sport', 'notice', 'natural', 'family', 'free', 'pain', 'however', 'gym', 'mother', 'figure', 'successful', 'routine', 'perhaps', 'wrong', 'engage', 'late', 'aware', 'practice', 'enough', 'suffer', 'check', 'prepare', 'move', 'mistakes', 'end', 'remember', 'turn', 'trust', 'emotion', 'enjoy', 'provide', 'spend', 'confident', 'real', 'expect', 'party', 'live', 'view', 'front', 'track', 'room', 'usually', 'challenge', 'support', 'approach', 'whatever', 'city', 'able', 'probably', 'full', 'willing', 'personality', 'music', 'media', 'possible', 'social', 'break', 'respect', 'reduce', 'teach', 'drive', 'failure', 'poor', 'long', 'forward', 'thought', 'movie', 'second', 'choose', 'sometimes', 'rest', 'journey', 'receive', 'hurt', 'anger', 'avoid', 'note', 'current', 'expectation', 'addiction', 'activity', 'find', 'due', 'tool', 'finally', 'discipline', 'date', 'together', 'strong', 'consider', 'fear', 'write', 'comfort', 'ask', 'meal', 'see', 'health', 'high', 'internet', 'age', 'language', 'writing', 'confidence', 'order', 'quit', 'depression', 'word', 'simply', 'kid', 'potential', 'cold', 'skill', 'happiness', 'stupid', 'improve', 'attention', 'plan', 'piece', 'fight', 'clear', 'talk', 'degree', 'never', 'video', 'effort', 'type', 'follow', 'scared', 'list', 'succeed', 'die', 'fall', 'single', 'lie', 'fail', 'voice', 'decided', 'schedule', 'class', 'behavior', 'child', 'force', 'build', 'three', 'god', 'gone', 'meditation', 'need', 'important', 'away', 'pursue', 'amount', 'training', 'guy', 'look', 'personal', 'mindset', 'meet', 'part', 'clean', 'benefit', 'eventually', 'fault', 'moment', 'happy', 'case', 'rule', 'made', 'exist', 'constantly', 'call', 'okay', 'solution', 'state', 'coffee', 'literally', 'dad', 'judge', 'ability', 'house', 'bed', 'common', 'key', 'concept', 'mom', 'lead', 'development', 'strength', 'least', 'try', 'heart', 'easy', 'sleep', 'cut', 'comfortable', 'feeling', 'chance', 'worry', 'increase', 'tip', 'understand', 'opinion', 'run', 'come', 'short', 'home', 'interesting', 'career', 'easily', 'company', 'listen', 'begin', 'alone', 'identify', 'value', 'working', 'mental', 'smart', 'nice', 'seek', 'event', 'car', 'early', 'drink', 'either', 'pick', 'play', 'emotional', 'base', 'business', 'game', 'thinking', 'learn', 'overcome', 'knowledgeable', 'drop', 'push', 'muscle', 'information', 'meaning', 'another', 'though', 'simple', 'sex', 'message', 'side', 'environment', 'wear', 'tired', 'large', 'give', 'imagine', 'number', 'first', 'truly', 'mind', 'interested', 'society', 'belief', 'believe', 'require', 'test', 'reality', 'pay', 'term', 'true', 'seem', 'memory', 'weight', 'butt', 'last', 'place', 'individual', 'little', 'complete', 'pass', 'book', 'quality', 'brain', 'decision', 'effect', 'share', 'risk', 'ever', 'keep', 'manage', 'eating', 'anxiety', 'less', 'stop', 'making', 'almost', 'likely', 'big', 'smoke', 'beautiful', 'huge', 'fuck', 'compare', 'living']"
      ],
      "metadata": {
        "id": "WRcgzhKVKVwu"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}