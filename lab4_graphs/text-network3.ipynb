{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25af22c4-e4c1-4104-8e43-237e376b0fd8",
   "metadata": {},
   "source": [
    "This notebooks creates a new network specifically designed for node2vec usage later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759ed55c-4fa6-47c3-add7-4ab66bda7070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/27 17:57:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "import itertools\n",
    "from pyspark.sql.functions import col, when, least, greatest, lit\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"network\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9a7e8b-d5ba-471f-9413-a60a48c024b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../cleaned_moral_scores.csv\", header= True).select('id', 'cleaned_text', 'emo_pos', 'emo_neg', \n",
    "                                                                          'emo_anx','emo_anger','emo_sad', 'moral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5172af0-ce11-4a81-8d8e-63f0c4afea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+\n",
      "|   id|        cleaned_text|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+\n",
      "|hk5r2|i had an appointm...|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|iqimz|i created this si...|   2.56|    0.0|    0.0|      0.0|    0.0| 1.71|\n",
      "|pfzt5|hello everyone  i...|   2.06|    0.0|    0.0|      0.0|    0.0| 0.52|\n",
      "|pk714|i grew up with bo...|   1.71|    1.2|   0.34|      0.0|   0.51| 0.68|\n",
      "|q0q8x|i have to ask whe...|   1.25|   1.61|   0.18|     0.18|    0.9| 0.18|\n",
      "|q412v|nothing but oppor...|   1.05|   3.16|    0.0|      0.0|   3.16|  0.0|\n",
      "|q5mqk|im getting out of...|   3.27|   1.96|   1.31|      0.0|    0.0|  0.0|\n",
      "|q70xe|hey everyone firs...|    0.0|   1.96|    0.0|      0.0|    0.0|  0.0|\n",
      "|q7mrn|facebook is great...|   0.96|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|qcsyp|okay so im 18 yea...|   0.74|   0.74|    0.0|      0.0|    0.0|  0.0|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74f582-3d7e-4cff-a929-a19ed5d61818",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b97dca-8358-4cf4-910b-48f9480fc532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define stopwords\n",
    "english = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \n",
    "    \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"did\", \n",
    "    \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n",
    "    \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "    \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"must\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \n",
    "    \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"some\", \"such\", \n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"will\", \"ll\", \n",
    "    \"re\", \"ve\", \"d\", \"s\", \"m\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "    \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"many\", \"us\", \"ok\", \"hows\", \"ive\", \"ill\", \"im\", \"cant\", \"topics\", \"topic\",\n",
    "    \"discuss\", \"thoughts\", \"yo\", \"thats\", \"whats\", \"lets\", \"nothing\", \"oh\", \"omg\", \n",
    "         \"things\", \"stuff\", \"yall\", \"haha\", \"yes\", \"no\", \"wo\", \"like\", 'good', \n",
    "         'work', 'got', 'going', 'dont', 'really', 'want', 'make', 'think', \n",
    "         'know', 'feel', 'people', 'life', \"getting\", \"lot\" \"great\", \"i\", \"me\", \n",
    "         \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "        \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "        \"they\", \"them\", \"their\", \"theirs\",\"themselves\", \"what\", \"which\", \"who\", \n",
    "        \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "        \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\",\n",
    "        \"might\", \"must\", \"shall\", \"ought\", \"about\", \"above\", \"across\", \"after\", \n",
    "        \"against\", \"along\", \"amid\", \"among\", \"around\", \"as\", \"at\", \"before\", \"behind\",\n",
    "        \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\", \"by\", \n",
    "        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\",\n",
    "        \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"notwithstanding\",\n",
    "        \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\",\n",
    "        \"regarding\", \"round\", \"since\", \"than\", \"through\", \"throughout\", \"till\", \n",
    "        \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\",\n",
    "        \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\", \"cant\", \"cannot\", \n",
    "        \"couldve\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \n",
    "        \"havent\", \"hed\", \"hell\", \"hes\", \"howd\", \"howll\", \"hows\", \"id\", \"ill\", \n",
    "        \"im\", \"ive\", \"isnt\", \"itd\", \"itll\", \"its\", \"lets\", \"mightve\", \"mustve\", \n",
    "        \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldve\", \"shouldnt\", \n",
    "        \"thatll\", \"thats\", \"thered\", \"therell\", \"therere\", \"theres\", \"theyd\", \n",
    "        \"theyll\", \"theyre\", \"theyve\", \"wed\", \"well\", \"were\", \"weve\", \"werent\", \n",
    "        \"whatd\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"whend\", \"whenll\", \n",
    "        \"whens\", \"whered\", \"wherell\", \"wheres\", \"whichd\", \"whichll\", \"whichre\", \n",
    "        \"whichs\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\", \"whyd\", \"whyll\", \n",
    "        \"whys\", \"wont\", \"wouldve\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "        \"f\", \"m\", \"because\", \"go\", \"lot\", \"get\", \"still\", \"way\", \"something\", \"much\",\n",
    "        \"thing\", \"someone\", \"person\", \"anything\", \"goes\", \"ok\", \"so\", \"just\", \"mostly\", \n",
    "        \"put\", \"also\", \"lots\", \"yet\", \"ha\", \"etc\", \"even\", \"one\", \"bye\", \"take\", \"wasnt\"]\n",
    "\n",
    "time = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \n",
    "        \"sunday\", \"morning\", \"noon\", \"afternoon\", \"evening\", \"night\", \"midnight\",\n",
    "        \"dawn\", \"dusk\", \"week\", \"weekend\", \"weekends\",\"weekly\", \"today\", \n",
    "        \"yesterday\", \"tomorrow\", \"yesterdays\", \"todays\", \"mondays\", \"tuesdays\",\n",
    "        \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\", \"day\",\n",
    "        \"everyday\", \"daily\", \"workday\", 'time', 'month', 'year', 'pm', 'am', \"ago\",\n",
    "        \"year\", \"now\"]\n",
    "\n",
    "reddit = [\"welcome\", \"hi\", \"hello\", \"sub\", \"reddit\", \"thanks\", \"thank\", \"maybe\",\n",
    "          \"wo30\", \"mods\", \"mod\", \"moderators\", \"subreddit\", \"btw\", \"aw\", \"aww\", \n",
    "          \"aww\", \"hey\", \"hello\", \"join\", \"joined\", \"post\", \"rselfimprovement\", \"blah\"]\n",
    "\n",
    "topic_specific = [\"self\", \"improvement\", \"change\", \"action\",\n",
    "    'change', 'start', 'goal', 'habit', 'new', 'old', \n",
    "    'care', 'world', 'everyone', 'love', 'u', 'right', 'mean', 'matter',\n",
    "    'best', 'step', 'focus', 'hard', 'small',\n",
    "    'bad', 'help', 'time', 'problem', 'issue', 'advice',\n",
    "    'bit', 'experience', 'different',\n",
    "    'point', 'situation', 'negative', 'control', 'positive',\n",
    "    'use', 'question', 'idea', 'amp', 'medium', 'hour', 'day', 'minute',\n",
    "    'aaaaloot', \"selfimprovement\", \"_\", \"ampxb\"]\n",
    "\n",
    "stopwords = english + time + reddit + topic_specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22f1aa45-fdf9-4280-bf27-cb8448f3672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "     .setInputCol(\"cleaned_text\")\\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols(['document'])\\\n",
    "            .setOutputCol('tokenized')\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized')\n",
    "\n",
    "lemmatizer = LemmatizerModel.load(\"../models/lemma_ewt_en_3.4.3_3.0_1651416655397/\")\\\n",
    "      .setInputCols(\"normalized\")\\\n",
    "      .setOutputCol(\"lemmatized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('words') \\\n",
    "     .setStopWords(stopwords)\n",
    "\n",
    "from sparknlp.base import Finisher\n",
    "\n",
    "finisher = Finisher().setInputCols(['words'])\n",
    "\n",
    "my_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          tokenizer,\n",
    "          normalizer,\n",
    "          lemmatizer,\n",
    "          stopwords_cleaner,\n",
    "          finisher\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb703ec-8d2a-4a77-8468-f35f30baae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/software/spark-3.3.2-el8-x86_64/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+\n",
      "|   id|        cleaned_text|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|      finished_words|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+\n",
      "|hk5r2|i had an appointm...|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|[appointment, den...|\n",
      "|iqimz|i created this si...|   2.56|    0.0|    0.0|      0.0|    0.0| 1.71|[create, site, se...|\n",
      "|pfzt5|hello everyone  i...|   2.06|    0.0|    0.0|      0.0|    0.0| 0.52|[recently, made, ...|\n",
      "|pk714|i grew up with bo...|   1.71|    1.2|   0.34|      0.0|   0.51| 0.68|[grow, body, dysm...|\n",
      "|q0q8x|i have to ask whe...|   1.25|   1.61|   0.18|     0.18|    0.9| 0.18|[content, never, ...|\n",
      "|q412v|nothing but oppor...|   1.05|   3.16|    0.0|      0.0|   3.16|  0.0|[butt, opportunit...|\n",
      "|q5mqk|im getting out of...|   3.27|   1.96|   1.31|      0.0|    0.0|  0.0|[comfort, zone, t...|\n",
      "|q70xe|hey everyone firs...|    0.0|   1.96|    0.0|      0.0|    0.0|  0.0|[first, learn, so...|\n",
      "|q7mrn|facebook is great...|   0.96|    0.0|    0.0|      0.0|    0.0|  0.0|[facebook, great,...|\n",
      "|qcsyp|okay so im 18 yea...|   0.74|   0.74|    0.0|      0.0|    0.0|  0.0|[okay, male, semi...|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipelineModel = my_pipeline.fit(data)\n",
    "processed_data = pipelineModel.transform(data)\n",
    "processed_data.persist()\n",
    "processed_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afcff1a3-f45e-4f58-9fab-307bdc3f3695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, cleaned_text: string, emo_pos: string, emo_neg: string, emo_anx: string, emo_anger: string, emo_sad: string, moral: string, finished_words: array<string>, tf_features: vector, tf_idf_features: vector]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply TF-IDF filtering\n",
    "tfizer = CountVectorizer(inputCol='finished_words', outputCol='tf_features', minDF=0.01, vocabSize=1000)\n",
    "tf_model = tfizer.fit(processed_data)\n",
    "tf_result = tf_model.transform(processed_data)\n",
    "vocabulary = tf_model.vocabulary\n",
    "\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)\n",
    "\n",
    "processed_data.unpersist()\n",
    "tfidf_result.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94440fc3-8f0d-4c52-93de-18bfa4c5fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|   id|        cleaned_text|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|      finished_words|         tf_features|     tf_idf_features|filtered_words_tfidf|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|hk5r2|i had an appointm...|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|[appointment, den...|(803,[13,36,43,76...|(803,[13,36,43,76...|                  []|\n",
      "|iqimz|i created this si...|   2.56|    0.0|    0.0|      0.0|    0.0| 1.71|[create, site, se...|(803,[0,3,57,134,...|(803,[0,3,57,134,...|                  []|\n",
      "|pfzt5|hello everyone  i...|   2.06|    0.0|    0.0|      0.0|    0.0| 0.52|[recently, made, ...|(803,[6,9,11,19,2...|(803,[6,9,11,19,2...|                  []|\n",
      "|pk714|i grew up with bo...|   1.71|    1.2|   0.34|      0.0|   0.51| 0.68|[grow, body, dysm...|(803,[0,2,3,5,6,7...|(803,[0,2,3,5,6,7...|     [wear, clothes]|\n",
      "|q0q8x|i have to ask whe...|   1.25|   1.61|   0.18|     0.18|    0.9| 0.18|[content, never, ...|(803,[0,1,6,7,10,...|(803,[0,1,6,7,10,...|        [water, sad]|\n",
      "|q412v|nothing but oppor...|   1.05|   3.16|    0.0|      0.0|   3.16|  0.0|[butt, opportunit...|(803,[0,3,16,29,3...|(803,[0,3,16,29,3...|       [opportunity]|\n",
      "|q5mqk|im getting out of...|   3.27|   1.96|   1.31|      0.0|    0.0|  0.0|[comfort, zone, t...|(803,[0,1,8,13,21...|(803,[0,1,8,13,21...|                  []|\n",
      "|q70xe|hey everyone firs...|    0.0|   1.96|    0.0|      0.0|    0.0|  0.0|[first, learn, so...|(803,[0,4,9,11,14...|(803,[0,4,9,11,14...|             [break]|\n",
      "|q7mrn|facebook is great...|   0.96|    0.0|    0.0|      0.0|    0.0|  0.0|[facebook, great,...|(803,[0,11,26,28,...|(803,[0,11,26,28,...|                  []|\n",
      "|qcsyp|okay so im 18 yea...|   0.74|   0.74|    0.0|      0.0|    0.0|  0.0|[okay, male, semi...|(803,[8,52,79,81,...|(803,[8,52,79,81,...|                  []|\n",
      "|qu825|well to give ever...|    0.0|   1.57|    0.0|      0.0|   0.79| 0.79|[give, everybody,...|(803,[0,8,11,13,1...|(803,[0,8,11,13,1...|                  []|\n",
      "|qxco0|i hate adderall i...|    0.0|    0.6|    0.0|      0.6|    0.0|  0.0|[hate, adderall, ...|(803,[1,5,7,18,23...|(803,[1,5,7,18,23...|                  []|\n",
      "|r89qc|im not sure if th...|   1.23|   0.62|    0.0|      0.0|    0.0|  0.0|[sure, place, but...|(803,[0,3,24,34,4...|(803,[0,3,24,34,4...|                  []|\n",
      "|ra0bn|to access your to...|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|[access, total, s...|(803,[8,28,85,98,...|(803,[8,28,85,98,...|           [success]|\n",
      "|rbi6h|i beginning to th...|   2.58|   0.52|    0.0|     0.52|    0.0| 0.52|[begin, inferiori...|(803,[0,1,3,4,6,1...|(803,[0,1,3,4,6,1...|                  []|\n",
      "|rd166|ive been working ...|   1.18|    0.0|    0.0|      0.0|    0.0|  0.0|[working, horribl...|(803,[0,5,15,20,2...|(803,[0,5,15,20,2...|                  []|\n",
      "|rrhg8|ive tried every d...|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|[try, ever, butt,...|(803,[0,1,2,7,23,...|(803,[0,1,2,7,23,...|                  []|\n",
      "|rvjcf|context last seme...|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|[context, last, s...|(803,[0,1,2,3,7,1...|(803,[0,1,2,3,7,1...|[read, play, drin...|\n",
      "|s0ruk|lately ive had th...|   1.27|   1.27|   0.64|      0.0|    0.0|  0.0|[lately, urge, ba...|(803,[0,1,10,13,1...|(803,[0,1,10,13,1...|                  []|\n",
      "|sa2de|its at about 1843...|    1.2|   2.41|    0.0|     2.41|    0.0|  0.0|[se, show, surpri...|(803,[9,11,20,23,...|(803,[9,11,20,23,...|                  []|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, cleaned_text: string, emo_pos: string, emo_neg: string, emo_anx: string, emo_anger: string, emo_sad: string, moral: string, finished_words: array<string>, tf_features: vector, tf_idf_features: vector]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to filter words by their TF-IDF score\n",
    "# UDF to map indices to words using the vocabulary\n",
    "def filter_tfidf(features, threshold=10, vocabulary=None):\n",
    "    if features is not None:\n",
    "        # Filter based on TF-IDF score and map indices to actual words\n",
    "        return [vocabulary[features.indices[i]] for i in range(len(features.values)) if features.values[i] >= threshold]\n",
    "    return []\n",
    "\n",
    "# Register the UDF\n",
    "filter_udf = udf(lambda features: filter_tfidf(features, threshold=10, vocabulary=vocabulary), ArrayType(StringType()))\n",
    "\n",
    "# Apply the filtering function\n",
    "df_filtered_tfidf = tfidf_result.withColumn(\"filtered_words_tfidf\", filter_udf(\"tf_idf_features\"))\n",
    "\n",
    "df_filtered_tfidf.show()\n",
    "tfidf_result.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bebc64a9-d01a-4d71-9f61-233f51d860d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def generate_edges(tokens):\n",
    "    return [list(pair) for pair in itertools.combinations(tokens, 2)]\n",
    "\n",
    "generate_edges_udf = udf(generate_edges, ArrayType(ArrayType(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07456dc3-88e6-4bcc-a11c-bedda717d129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|   id|        cleaned_text|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|      finished_words|         tf_features|     tf_idf_features|filtered_words_tfidf|            edges|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|hk5r2|i had an appointm...|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|[appointment, den...|(803,[13,36,43,76...|(803,[13,36,43,76...|                  []|               []|\n",
      "|iqimz|i created this si...|   2.56|    0.0|    0.0|      0.0|    0.0| 1.71|[create, site, se...|(803,[0,3,57,134,...|(803,[0,3,57,134,...|                  []|               []|\n",
      "|pfzt5|hello everyone  i...|   2.06|    0.0|    0.0|      0.0|    0.0| 0.52|[recently, made, ...|(803,[6,9,11,19,2...|(803,[6,9,11,19,2...|                  []|               []|\n",
      "|pk714|i grew up with bo...|   1.71|    1.2|   0.34|      0.0|   0.51| 0.68|[grow, body, dysm...|(803,[0,2,3,5,6,7...|(803,[0,2,3,5,6,7...|     [wear, clothes]|[[wear, clothes]]|\n",
      "|q0q8x|i have to ask whe...|   1.25|   1.61|   0.18|     0.18|    0.9| 0.18|[content, never, ...|(803,[0,1,6,7,10,...|(803,[0,1,6,7,10,...|        [water, sad]|   [[water, sad]]|\n",
      "|q412v|nothing but oppor...|   1.05|   3.16|    0.0|      0.0|   3.16|  0.0|[butt, opportunit...|(803,[0,3,16,29,3...|(803,[0,3,16,29,3...|       [opportunity]|               []|\n",
      "|q5mqk|im getting out of...|   3.27|   1.96|   1.31|      0.0|    0.0|  0.0|[comfort, zone, t...|(803,[0,1,8,13,21...|(803,[0,1,8,13,21...|                  []|               []|\n",
      "|q70xe|hey everyone firs...|    0.0|   1.96|    0.0|      0.0|    0.0|  0.0|[first, learn, so...|(803,[0,4,9,11,14...|(803,[0,4,9,11,14...|             [break]|               []|\n",
      "|q7mrn|facebook is great...|   0.96|    0.0|    0.0|      0.0|    0.0|  0.0|[facebook, great,...|(803,[0,11,26,28,...|(803,[0,11,26,28,...|                  []|               []|\n",
      "|qcsyp|okay so im 18 yea...|   0.74|   0.74|    0.0|      0.0|    0.0|  0.0|[okay, male, semi...|(803,[8,52,79,81,...|(803,[8,52,79,81,...|                  []|               []|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+--------------------+--------------------+--------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_edges = df_filtered_tfidf.withColumn(\"edges\", generate_edges_udf(F.col(\"filtered_words_tfidf\")))\n",
    "df_edges.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84de33c-e828-4a16-bd98-0b9ad23128b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb651e94-17ca-42ba-a308-3c4bc6a2e0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-------+-------+-------+---------+-------+-----+\n",
      "|   id|           edge|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|\n",
      "+-----+---------------+-------+-------+-------+---------+-------+-----+\n",
      "|pk714|[wear, clothes]|   1.71|    1.2|   0.34|      0.0|   0.51| 0.68|\n",
      "|q0q8x|   [water, sad]|   1.25|   1.61|   0.18|     0.18|    0.9| 0.18|\n",
      "|rvjcf|   [read, play]|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|  [read, drink]|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf| [read, amount]|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|   [read, lift]|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|  [play, drink]|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf| [play, amount]|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|   [play, lift]|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|[drink, amount]|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "+-----+---------------+-------+-------+-------+---------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_explode = df_edges.select(\n",
    "    F.col(\"id\"),\n",
    "    F.explode(F.col(\"edges\")).alias(\"edge\"), F.col('emo_pos'),\n",
    "    F.col('emo_neg'), F.col('emo_anx'), F.col('emo_anger'), \n",
    "    F.col('emo_sad'), F.col('moral'))\n",
    "\n",
    "df_explode.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f0c92-acad-4cd5-8e33-37b126114bad",
   "metadata": {},
   "source": [
    "Create edges df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6614b78f-5540-4ebf-b093-035082e07a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = df_explode.select(\n",
    "    F.col(\"edge\")[0].alias(\"node1\"),\n",
    "    F.col(\"edge\")[1].alias(\"node2\"))\n",
    "\n",
    "edges_df = edges_df.withColumn(\"weight\", lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbd948f7-73bd-4b19-b7f9-c136293489e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pairs: ensure node1 is always less than node2, so they can be always on the same order\n",
    "edges_df = edges_df.withColumn(\"node1_norm\", least(col(\"node1\"), col(\"node2\"))) \\\n",
    "             .withColumn(\"node2_norm\", greatest(col(\"node1\"), col(\"node2\"))) \\\n",
    "             .select('node1_norm', 'node2_norm', 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79442c2c-4891-4a28-a4e1-c7ee2e5977a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = edges_df.groupBy(\"node1_norm\", \"node2_norm\").sum(\"weight\") \\\n",
    "                        .withColumnRenamed(\"sum(weight)\", \"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f398cbd5-f47b-42d8-9a89-6b92f963dcef",
   "metadata": {},
   "source": [
    "Create nodes df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c21d634-83bf-4501-a44e-fd2b20ea74c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+-------+-------+---------+-------+-----+\n",
      "|   id|   node|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|\n",
      "+-----+-------+-------+-------+-------+---------+-------+-----+\n",
      "|pk714|   wear|   1.71|    1.2|   0.34|      0.0|   0.51| 0.68|\n",
      "|pk714|clothes|   1.71|    1.2|   0.34|      0.0|   0.51| 0.68|\n",
      "|q0q8x|  water|   1.25|   1.61|   0.18|     0.18|    0.9| 0.18|\n",
      "|q0q8x|    sad|   1.25|   1.61|   0.18|     0.18|    0.9| 0.18|\n",
      "|rvjcf|   read|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|   play|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|   read|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|  drink|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|   read|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf| amount|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|   read|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|   lift|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|   play|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|  drink|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|   play|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf| amount|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|   play|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|   lift|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf|  drink|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "|rvjcf| amount|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|\n",
      "+-----+-------+-------+-------+-------+---------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------+-------+-------+---------+-------+-----+\n",
      "|     id|        node|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|\n",
      "+-------+------------+-------+-------+-------+---------+-------+-----+\n",
      "|1002lda|       grade|   0.82|   2.87|    0.0|      0.0|   2.05| 0.41|\n",
      "|1002wwt|      always|   1.16|   1.01|   0.24|     0.07|    0.1| 0.17|\n",
      "|1002wwt|        hand|   1.16|   1.01|   0.24|     0.07|    0.1| 0.17|\n",
      "|1002wwt|  meditation|   1.16|   1.01|   0.24|     0.07|    0.1| 0.17|\n",
      "|1002wwt|        meet|   1.16|   1.01|   0.24|     0.07|    0.1| 0.17|\n",
      "|1002wwt|      mental|   1.16|   1.01|   0.24|     0.07|    0.1| 0.17|\n",
      "|1002wwt|   recommend|   1.16|   1.01|   0.24|     0.07|    0.1| 0.17|\n",
      "|1002wwt|      slowly|   1.16|   1.01|   0.24|     0.07|    0.1| 0.17|\n",
      "|1002wwt|       sport|   1.16|   1.01|   0.24|     0.07|    0.1| 0.17|\n",
      "|1004pti|        meal|   0.27|   0.27|   0.13|      0.0|   0.13| 0.13|\n",
      "|1004pti|        wake|   0.27|   0.27|   0.13|      0.0|   0.13| 0.13|\n",
      "|1006c2b|        hand|   1.09|   0.11|   0.08|      0.0|    0.0| 0.83|\n",
      "|1006c2b|      honest|   1.09|   0.11|   0.08|      0.0|    0.0| 0.83|\n",
      "|1006c2b|   necessary|   1.09|   0.11|   0.08|      0.0|    0.0| 0.83|\n",
      "|1006c2b|relationship|   1.09|   0.11|   0.08|      0.0|    0.0| 0.83|\n",
      "|100c0hy|      listen|   0.25|   0.37|    0.0|     0.12|    0.0| 0.25|\n",
      "|100c0hy|        room|   0.25|   0.37|    0.0|     0.12|    0.0| 0.25|\n",
      "|100c0hy|        term|   0.25|   0.37|    0.0|     0.12|    0.0| 0.25|\n",
      "|100p3qr|         eye|   2.15|   0.62|   0.62|      0.0|    0.0| 0.62|\n",
      "|100td9d|        book|    0.6|    0.4|    0.0|      0.0|    0.4|  0.0|\n",
      "+-------+------------+-------+-------+-------+---------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nodes_df = df_explode.select(\n",
    "    F.col(\"id\"),\n",
    "    F.explode(F.col(\"edge\")).alias(\"node\"), F.col('emo_pos'),\n",
    "    F.col('emo_neg'), F.col('emo_anx'), F.col('emo_anger'), \n",
    "    F.col('emo_sad'), F.col('moral'))\n",
    "\n",
    "nodes_df.show(20) \n",
    "\n",
    "nodes_df_g = nodes_df.groupBy(\"id\", \"node\").agg(\n",
    "    F.first('emo_pos').alias('emo_pos'),\n",
    "    F.first('emo_neg').alias('emo_neg'),\n",
    "    F.first('emo_anx').alias('emo_anx'),\n",
    "    F.first('emo_anger').alias('emo_anger'),\n",
    "    F.first('emo_sad').alias('emo_sad'),\n",
    "    F.first('moral').alias('moral'))\n",
    "\n",
    "nodes_df_g.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b06cc1-ab84-43fb-8206-6bc551760e4e",
   "metadata": {},
   "source": [
    "Now, aggregate all words and average their emotions and morality scores for all documents in which they appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb4f9a38-1d6a-4d30-bb94-0c292bdb4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nodes = nodes_df_g.groupBy(\"node\").agg(\n",
    "    F.avg('emo_pos').alias('emo_pos'),\n",
    "    F.avg('emo_neg').alias('emo_neg'),\n",
    "    F.avg('emo_anx').alias('emo_anx'),\n",
    "    F.avg('emo_anger').alias('emo_anger'),\n",
    "    F.avg('emo_sad').alias('emo_sad'),\n",
    "    F.avg('moral').alias('moral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d754d5cf-e5a5-4246-bf59-8348840c0fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|        node|           emo_pos|           emo_neg|            emo_anx|          emo_anger|            emo_sad|              moral|\n",
      "+------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|      online|0.8537845303867402|0.9379005524861878|0.25859116022099443|0.18361878453038674|0.16116022099447513|0.27157458563535913|\n",
      "|   recognize|1.0374045801526717| 1.114885496183206| 0.3461068702290076|0.19709923664122134|0.18145038167938932|0.45038167938931295|\n",
      "|       inner|1.1068803418803421|1.1653418803418805|0.30491452991452994|0.19931623931623932|  0.166025641025641|0.46333333333333326|\n",
      "|      travel|              1.21|0.8630107526881721|0.31268817204301075|0.11193548387096774|0.15526881720430105| 0.2249462365591398|\n",
      "|        hope|2.4231147540983606| 1.149344262295082| 0.2624590163934426|0.22426229508196718| 0.2277049180327869|0.48901639344262293|\n",
      "|       often| 1.054423076923077| 1.231153846153846|0.32599999999999996|0.27007692307692305|0.20819230769230768| 0.3855769230769231|\n",
      "|conversation| 1.152925877763329|0.9419765929778933| 0.2921586475942783|0.19579973992197658|0.09630689206762029|0.32803641092327707|\n",
      "|  productive|0.9504166666666666|0.7550757575757576| 0.1810227272727273|0.10333333333333333|  0.125530303030303|0.20481060606060605|\n",
      "|     include| 1.137051282051282|1.1638461538461538| 0.5416666666666667|0.11416666666666665|0.12634615384615386|0.26224358974358974|\n",
      "|      growth|1.0930564784053158|0.7717607973421929|  0.249734219269103|0.08229235880398672|0.09867109634551495|0.33059800664451827|\n",
      "|     explain|0.8139490445859872| 0.972611464968153|0.26248407643312105|0.20535031847133758|0.12847133757961784| 0.3270700636942675|\n",
      "|     achieve|1.0270443349753693| 0.648743842364532|0.19758620689655176|0.08842364532019706|0.09258620689655171| 0.2755665024630542|\n",
      "|       watch|0.9235791366906476| 0.803705035971223|  0.171294964028777|0.12170863309352516|0.14383093525179855|0.24411870503597122|\n",
      "|        slow|0.8211570247933886|1.0583471074380166| 0.3657851239669422|0.19090909090909092| 0.1328099173553719|0.21231404958677688|\n",
      "| perspective|1.1680132450331127|1.0528476821192052|0.25887417218543046|0.21854304635761593|0.11821192052980134|0.37509933774834436|\n",
      "|   character|1.1505027932960894| 0.757877094972067|0.17452513966480449|0.18726256983240225|0.08748603351955307| 0.5388826815642458|\n",
      "|      trauma|0.8172614107883818|2.6555601659751042| 0.4850207468879668| 0.1928630705394191|0.23734439834024898| 0.5181327800829875|\n",
      "|       leave|1.0011363636363635| 1.370909090909091| 0.3967613636363636|0.27454545454545454|0.21238636363636362|0.35869318181818177|\n",
      "|        grow|1.0722185430463576|1.0471523178807947| 0.2786092715231789|0.16367549668874173|0.16794701986754967|0.33347682119205296|\n",
      "|       ready|0.9558064516129032| 1.076967741935484|0.36387096774193545|0.10309677419354839|0.13187096774193552|0.31567741935483873|\n",
      "+------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_nodes.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6fd66-7afb-470c-aec4-4290a9a38bb6",
   "metadata": {},
   "source": [
    "Write to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bacc3721-bf2e-4ef1-9aa9-d19399b9bffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_nodes.write.mode(\"overwrite\").csv(\"nodes_network3\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "475152f3-7076-42a2-82ce-1d1ebb6bc2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "edges_df.write.mode(\"overwrite\").csv(\"edges_network3\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fa598e-4ece-49d0-99a2-1449881d1ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
