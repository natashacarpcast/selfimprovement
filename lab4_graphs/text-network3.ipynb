{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25af22c4-e4c1-4104-8e43-237e376b0fd8",
   "metadata": {},
   "source": [
    "This notebooks creates a new network specifically designed for node2vec usage later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759ed55c-4fa6-47c3-add7-4ab66bda7070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/05 13:50:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sparknlp\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "import itertools\n",
    "from pyspark.sql.functions import col, when, least, greatest, lit\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"network\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9a7e8b-d5ba-471f-9413-a60a48c024b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../data/cleaned_moral_scores.csv\", header= True).select('id', 'cleaned_text', 'emo_pos', 'emo_neg', \n",
    "                                                                          'emo_anx','emo_anger','emo_sad', 'moral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5172af0-ce11-4a81-8d8e-63f0c4afea04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+\n",
      "|   id|        cleaned_text|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+\n",
      "|hk5r2|i had an appointm...|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|iqimz|i created this si...|   2.56|    0.0|    0.0|      0.0|    0.0| 1.71|\n",
      "|pfzt5|hello everyone  i...|   2.06|    0.0|    0.0|      0.0|    0.0| 0.52|\n",
      "|pk714|i grew up with bo...|   1.71|    1.2|   0.34|      0.0|   0.51| 0.68|\n",
      "|q0q8x|i have to ask whe...|   1.25|   1.61|   0.18|     0.18|    0.9| 0.18|\n",
      "|q412v|nothing but oppor...|   1.05|   3.16|    0.0|      0.0|   3.16|  0.0|\n",
      "|q5mqk|im getting out of...|   3.27|   1.96|   1.31|      0.0|    0.0|  0.0|\n",
      "|q70xe|hey everyone firs...|    0.0|   1.96|    0.0|      0.0|    0.0|  0.0|\n",
      "|q7mrn|facebook is great...|   0.96|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|qcsyp|okay so im 18 yea...|   0.74|   0.74|    0.0|      0.0|    0.0|  0.0|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74f582-3d7e-4cff-a929-a19ed5d61818",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4b97dca-8358-4cf4-910b-48f9480fc532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define stopwords\n",
    "english = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \n",
    "    \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"did\", \n",
    "    \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n",
    "    \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "    \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"must\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \n",
    "    \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"some\", \"such\", \n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"will\", \"ll\", \n",
    "    \"re\", \"ve\", \"d\", \"s\", \"m\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "    \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"many\", \"us\", \"ok\", \"hows\", \"ive\", \"ill\", \"im\", \"cant\", \"topics\", \"topic\",\n",
    "    \"discuss\", \"thoughts\", \"yo\", \"thats\", \"whats\", \"lets\", \"nothing\", \"oh\", \"omg\", \n",
    "         \"things\", \"stuff\", \"yall\", \"haha\", \"yes\", \"no\", \"wo\", \"like\", 'good', \n",
    "         'work', 'got', 'going', 'dont', 'really', 'want', 'make', 'think', \n",
    "         'know', 'feel', 'people', 'life', \"getting\", \"lot\" \"great\", \"i\", \"me\", \n",
    "         \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "        \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "        \"they\", \"them\", \"their\", \"theirs\",\"themselves\", \"what\", \"which\", \"who\", \n",
    "        \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "        \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\",\n",
    "        \"might\", \"must\", \"shall\", \"ought\", \"about\", \"above\", \"across\", \"after\", \n",
    "        \"against\", \"along\", \"amid\", \"among\", \"around\", \"as\", \"at\", \"before\", \"behind\",\n",
    "        \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\", \"by\", \n",
    "        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\",\n",
    "        \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"notwithstanding\",\n",
    "        \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\",\n",
    "        \"regarding\", \"round\", \"since\", \"than\", \"through\", \"throughout\", \"till\", \n",
    "        \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\",\n",
    "        \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\", \"cant\", \"cannot\", \n",
    "        \"couldve\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \n",
    "        \"havent\", \"hed\", \"hell\", \"hes\", \"howd\", \"howll\", \"hows\", \"id\", \"ill\", \n",
    "        \"im\", \"ive\", \"isnt\", \"itd\", \"itll\", \"its\", \"lets\", \"mightve\", \"mustve\", \n",
    "        \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldve\", \"shouldnt\", \n",
    "        \"thatll\", \"thats\", \"thered\", \"therell\", \"therere\", \"theres\", \"theyd\", \n",
    "        \"theyll\", \"theyre\", \"theyve\", \"wed\", \"well\", \"were\", \"weve\", \"werent\", \n",
    "        \"whatd\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"whend\", \"whenll\", \n",
    "        \"whens\", \"whered\", \"wherell\", \"wheres\", \"whichd\", \"whichll\", \"whichre\", \n",
    "        \"whichs\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\", \"whyd\", \"whyll\", \n",
    "        \"whys\", \"wont\", \"wouldve\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "        \"f\", \"m\", \"because\", \"go\", \"lot\", \"get\", \"still\", \"way\", \"something\", \"much\",\n",
    "        \"thing\", \"someone\", \"person\", \"anything\", \"goes\", \"ok\", \"so\", \"just\", \"mostly\", \n",
    "        \"put\", \"also\", \"lots\", \"yet\", \"ha\", \"etc\", \"even\", \"one\", \"bye\", \"take\", \"wasnt\"]\n",
    "\n",
    "time = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \n",
    "        \"sunday\", \"morning\", \"noon\", \"afternoon\", \"evening\", \"night\", \"midnight\",\n",
    "        \"dawn\", \"dusk\", \"week\", \"weekend\", \"weekends\",\"weekly\", \"today\", \n",
    "        \"yesterday\", \"tomorrow\", \"yesterdays\", \"todays\", \"mondays\", \"tuesdays\",\n",
    "        \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\", \"day\",\n",
    "        \"everyday\", \"daily\", \"workday\", 'time', 'month', 'year', 'pm', 'am', \"ago\",\n",
    "        \"year\", \"now\"]\n",
    "\n",
    "reddit = [\"welcome\", \"hi\", \"hello\", \"sub\", \"reddit\", \"thanks\", \"thank\", \"maybe\",\n",
    "          \"wo30\", \"mods\", \"mod\", \"moderators\", \"subreddit\", \"btw\", \"aw\", \"aww\", \n",
    "          \"aww\", \"hey\", \"hello\", \"join\", \"joined\", \"post\", \"rselfimprovement\", \"blah\"]\n",
    "\n",
    "topic_specific = [\"self\", \"improvement\", \"change\", \"action\",\n",
    "    'change', 'start', 'goal', 'habit', 'new', 'old', \n",
    "    'care', 'world', 'everyone', 'love', 'u', 'right', 'mean', 'matter',\n",
    "    'best', 'step', 'focus', 'hard', 'small',\n",
    "    'bad', 'help', 'time', 'problem', 'issue', 'advice',\n",
    "    'bit', 'experience', 'different',\n",
    "    'point', 'situation', 'negative', 'control', 'positive',\n",
    "    'use', 'question', 'idea', 'amp', 'medium', 'hour', 'day', 'minute',\n",
    "    'aaaaloot', \"selfimprovement\", \"_\", \"ampxb\"]\n",
    "\n",
    "stopwords = english + time + reddit + topic_specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22f1aa45-fdf9-4280-bf27-cb8448f3672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "     .setInputCol(\"cleaned_text\")\\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols(['document'])\\\n",
    "            .setOutputCol('tokenized')\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized')\n",
    "\n",
    "lemmatizer = LemmatizerModel.load(\"../models/lemma_ewt_en_3.4.3_3.0_1651416655397/\")\\\n",
    "      .setInputCols(\"normalized\")\\\n",
    "      .setOutputCol(\"lemmatized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('words') \\\n",
    "     .setStopWords(stopwords)\n",
    "\n",
    "from sparknlp.base import Finisher\n",
    "\n",
    "finisher = Finisher().setInputCols(['words'])\n",
    "\n",
    "my_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          tokenizer,\n",
    "          normalizer,\n",
    "          lemmatizer,\n",
    "          stopwords_cleaner,\n",
    "          finisher\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb703ec-8d2a-4a77-8468-f35f30baae2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/software/spark-3.3.2-el8-x86_64/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+\n",
      "|   id|        cleaned_text|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|      finished_words|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+\n",
      "|hk5r2|i had an appointm...|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|[appointment, den...|\n",
      "|iqimz|i created this si...|   2.56|    0.0|    0.0|      0.0|    0.0| 1.71|[create, site, se...|\n",
      "|pfzt5|hello everyone  i...|   2.06|    0.0|    0.0|      0.0|    0.0| 0.52|[recently, made, ...|\n",
      "|pk714|i grew up with bo...|   1.71|    1.2|   0.34|      0.0|   0.51| 0.68|[grow, body, dysm...|\n",
      "|q0q8x|i have to ask whe...|   1.25|   1.61|   0.18|     0.18|    0.9| 0.18|[content, never, ...|\n",
      "|q412v|nothing but oppor...|   1.05|   3.16|    0.0|      0.0|   3.16|  0.0|[butt, opportunit...|\n",
      "|q5mqk|im getting out of...|   3.27|   1.96|   1.31|      0.0|    0.0|  0.0|[comfort, zone, t...|\n",
      "|q70xe|hey everyone firs...|    0.0|   1.96|    0.0|      0.0|    0.0|  0.0|[first, learn, so...|\n",
      "|q7mrn|facebook is great...|   0.96|    0.0|    0.0|      0.0|    0.0|  0.0|[facebook, great,...|\n",
      "|qcsyp|okay so im 18 yea...|   0.74|   0.74|    0.0|      0.0|    0.0|  0.0|[okay, male, semi...|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipelineModel = my_pipeline.fit(data)\n",
    "processed_data = pipelineModel.transform(data)\n",
    "processed_data.persist()\n",
    "processed_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afcff1a3-f45e-4f58-9fab-307bdc3f3695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, cleaned_text: string, emo_pos: string, emo_neg: string, emo_anx: string, emo_anger: string, emo_sad: string, moral: string, finished_words: array<string>, tf_features: vector, tf_idf_features: vector]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply TF-IDF filtering\n",
    "tfizer = CountVectorizer(inputCol='finished_words', outputCol='tf_features', minDF=0.01, vocabSize=1000)\n",
    "tf_model = tfizer.fit(processed_data)\n",
    "tf_result = tf_model.transform(processed_data)\n",
    "vocabulary = tf_model.vocabulary\n",
    "\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)\n",
    "\n",
    "processed_data.unpersist()\n",
    "tfidf_result.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94440fc3-8f0d-4c52-93de-18bfa4c5fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:=>               (1 + 9) / 10][Stage 17:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|   id|        cleaned_text|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|      finished_words|         tf_features|     tf_idf_features|filtered_words_tfidf|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|hk5r2|i had an appointm...|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|[appointment, den...|(803,[13,36,43,76...|(803,[13,36,43,76...|[never, happen, s...|\n",
      "|iqimz|i created this si...|   2.56|    0.0|    0.0|      0.0|    0.0| 1.71|[create, site, se...|(803,[0,3,57,134,...|(803,[0,3,57,134,...|[find, hope, futu...|\n",
      "|pfzt5|hello everyone  i...|   2.06|    0.0|    0.0|      0.0|    0.0| 0.52|[recently, made, ...|(803,[6,9,11,19,2...|(803,[6,9,11,19,2...|[look, learn, kee...|\n",
      "|pk714|i grew up with bo...|   1.71|    1.2|   0.34|      0.0|   0.51| 0.68|[grow, body, dysm...|(803,[0,2,3,5,6,7...|(803,[0,2,3,5,6,7...|[need, find, see,...|\n",
      "|q0q8x|i have to ask whe...|   1.25|   1.61|   0.18|     0.18|    0.9| 0.18|[content, never, ...|(803,[0,1,6,7,10,...|(803,[0,1,6,7,10,...|[butt, try, look,...|\n",
      "|q412v|nothing but oppor...|   1.05|   3.16|    0.0|      0.0|   3.16|  0.0|[butt, opportunit...|(803,[0,3,16,29,3...|(803,[0,3,16,29,3...|[find, feeling, m...|\n",
      "|q5mqk|im getting out of...|   3.27|   1.96|   1.31|      0.0|    0.0|  0.0|[comfort, zone, t...|(803,[0,1,8,13,21...|(803,[0,1,8,13,21...|[try, give, never...|\n",
      "|q70xe|hey everyone firs...|    0.0|   1.96|    0.0|      0.0|    0.0|  0.0|[first, learn, so...|(803,[0,4,9,11,14...|(803,[0,4,9,11,14...|[say, learn, keep...|\n",
      "|q7mrn|facebook is great...|   0.96|    0.0|    0.0|      0.0|    0.0|  0.0|[facebook, great,...|(803,[0,11,26,28,...|(803,[0,11,26,28,...|[keep, social, gr...|\n",
      "|qcsyp|okay so im 18 yea...|   0.74|   0.74|    0.0|      0.0|    0.0|  0.0|[okay, male, semi...|(803,[8,52,79,81,...|(803,[8,52,79,81,...|[give, guy, famil...|\n",
      "|qu825|well to give ever...|    0.0|   1.57|    0.0|      0.0|   0.79| 0.79|[give, everybody,...|(803,[0,8,11,13,1...|(803,[0,8,11,13,1...|[give, keep, neve...|\n",
      "|qxco0|i hate adderall i...|    0.0|    0.6|    0.0|      0.6|    0.0|  0.0|[hate, adderall, ...|(803,[1,5,7,18,23...|(803,[1,5,7,18,23...|[try, see, ever, ...|\n",
      "|r89qc|im not sure if th...|   1.23|   0.62|    0.0|      0.0|    0.0|  0.0|[sure, place, but...|(803,[0,3,24,34,4...|(803,[0,3,24,34,4...|[find, live, happ...|\n",
      "|ra0bn|to access your to...|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|[access, total, s...|(803,[8,28,85,98,...|(803,[8,28,85,98,...|[give, great, hig...|\n",
      "|rbi6h|i beginning to th...|   2.58|   0.52|    0.0|     0.52|    0.0| 0.52|[begin, inferiori...|(803,[0,1,3,4,6,1...|(803,[0,1,3,4,6,1...|[butt, try, find,...|\n",
      "|rd166|ive been working ...|   1.18|    0.0|    0.0|      0.0|    0.0|  0.0|[working, horribl...|(803,[0,5,15,20,2...|(803,[0,5,15,20,2...|[see, back, job, ...|\n",
      "|rrhg8|ive tried every d...|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|[try, ever, butt,...|(803,[0,1,2,7,23,...|(803,[0,1,2,7,23,...|[try, need, ever,...|\n",
      "|rvjcf|context last seme...|   2.34|   0.43|    0.0|     0.43|    0.0| 0.21|[context, last, s...|(803,[0,1,2,3,7,1...|(803,[0,1,2,3,7,1...|[try, need, find,...|\n",
      "|s0ruk|lately ive had th...|   1.27|   1.27|   0.64|      0.0|    0.0|  0.0|[lately, urge, ba...|(803,[0,1,10,13,1...|(803,[0,1,10,13,1...|[try, come, never...|\n",
      "|sa2de|its at about 1843...|    1.2|   2.41|    0.0|     2.41|    0.0|  0.0|[se, show, surpri...|(803,[9,11,20,23,...|(803,[9,11,20,23,...|[learn, keep, job...|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, cleaned_text: string, emo_pos: string, emo_neg: string, emo_anx: string, emo_anger: string, emo_sad: string, moral: string, finished_words: array<string>, tf_features: vector, tf_idf_features: vector]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to filter words by their TF-IDF score\n",
    "# UDF to map indices to words using the vocabulary\n",
    "def filter_tfidf(features, threshold=1, vocabulary=None):\n",
    "    if features is not None:\n",
    "        # Filter based on TF-IDF score and map indices to actual words\n",
    "        return [vocabulary[features.indices[i]] for i in range(len(features.values)) if features.values[i] >= threshold]\n",
    "    return []\n",
    "\n",
    "# Register the UDF\n",
    "filter_udf = udf(lambda features: filter_tfidf(features, threshold=1, vocabulary=vocabulary), ArrayType(StringType()))\n",
    "\n",
    "# Apply the filtering function\n",
    "df_filtered_tfidf = tfidf_result.withColumn(\"filtered_words_tfidf\", filter_udf(\"tf_idf_features\"))\n",
    "\n",
    "df_filtered_tfidf.show()\n",
    "tfidf_result.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bebc64a9-d01a-4d71-9f61-233f51d860d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def generate_edges(tokens):\n",
    "    return [list(pair) for pair in itertools.combinations(tokens, 2)]\n",
    "\n",
    "generate_edges_udf = udf(generate_edges, ArrayType(ArrayType(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07456dc3-88e6-4bcc-a11c-bedda717d129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   id|        cleaned_text|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|      finished_words|         tf_features|     tf_idf_features|filtered_words_tfidf|               edges|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|hk5r2|i had an appointm...|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|[appointment, den...|(803,[13,36,43,76...|(803,[13,36,43,76...|[never, happen, s...|[[never, happen],...|\n",
      "|iqimz|i created this si...|   2.56|    0.0|    0.0|      0.0|    0.0| 1.71|[create, site, se...|(803,[0,3,57,134,...|(803,[0,3,57,134,...|[find, hope, futu...|[[find, hope], [f...|\n",
      "|pfzt5|hello everyone  i...|   2.06|    0.0|    0.0|      0.0|    0.0| 0.52|[recently, made, ...|(803,[6,9,11,19,2...|(803,[6,9,11,19,2...|[look, learn, kee...|[[look, learn], [...|\n",
      "|pk714|i grew up with bo...|   1.71|    1.2|   0.34|      0.0|   0.51| 0.68|[grow, body, dysm...|(803,[0,2,3,5,6,7...|(803,[0,2,3,5,6,7...|[need, find, see,...|[[need, find], [n...|\n",
      "|q0q8x|i have to ask whe...|   1.25|   1.61|   0.18|     0.18|    0.9| 0.18|[content, never, ...|(803,[0,1,6,7,10,...|(803,[0,1,6,7,10,...|[butt, try, look,...|[[butt, try], [bu...|\n",
      "|q412v|nothing but oppor...|   1.05|   3.16|    0.0|      0.0|   3.16|  0.0|[butt, opportunit...|(803,[0,3,16,29,3...|(803,[0,3,16,29,3...|[find, feeling, m...|[[find, feeling],...|\n",
      "|q5mqk|im getting out of...|   3.27|   1.96|   1.31|      0.0|    0.0|  0.0|[comfort, zone, t...|(803,[0,1,8,13,21...|(803,[0,1,8,13,21...|[try, give, never...|[[try, give], [tr...|\n",
      "|q70xe|hey everyone firs...|    0.0|   1.96|    0.0|      0.0|    0.0|  0.0|[first, learn, so...|(803,[0,4,9,11,14...|(803,[0,4,9,11,14...|[say, learn, keep...|[[say, learn], [s...|\n",
      "|q7mrn|facebook is great...|   0.96|    0.0|    0.0|      0.0|    0.0|  0.0|[facebook, great,...|(803,[0,11,26,28,...|(803,[0,11,26,28,...|[keep, social, gr...|[[keep, social], ...|\n",
      "|qcsyp|okay so im 18 yea...|   0.74|   0.74|    0.0|      0.0|    0.0|  0.0|[okay, male, semi...|(803,[8,52,79,81,...|(803,[8,52,79,81,...|[give, guy, famil...|[[give, guy], [gi...|\n",
      "+-----+--------------------+-------+-------+-------+---------+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_edges = df_filtered_tfidf.withColumn(\"edges\", generate_edges_udf(F.col(\"filtered_words_tfidf\")))\n",
    "df_edges.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84de33c-e828-4a16-bd98-0b9ad23128b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb651e94-17ca-42ba-a308-3c4bc6a2e0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------+-------+-------+---------+-------+-----+\n",
      "|   id|               edge|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|\n",
      "+-----+-------------------+-------+-------+-------+---------+-------+-----+\n",
      "|hk5r2|    [never, happen]|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|      [never, sure]|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|      [never, last]|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|       [never, two]|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|      [never, call]|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|[never, completely]|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     [never, phone]|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|    [never, forget]|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     [never, smoke]|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     [never, three]|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "+-----+-------------------+-------+-------+-------+---------+-------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_explode = df_edges.select(\n",
    "    F.col(\"id\"),\n",
    "    F.explode(F.col(\"edges\")).alias(\"edge\"), F.col('emo_pos'),\n",
    "    F.col('emo_neg'), F.col('emo_anx'), F.col('emo_anger'), \n",
    "    F.col('emo_sad'), F.col('moral'))\n",
    "\n",
    "df_explode.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f0c92-acad-4cd5-8e33-37b126114bad",
   "metadata": {},
   "source": [
    "Create edges df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6614b78f-5540-4ebf-b093-035082e07a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = df_explode.select(\n",
    "    F.col(\"edge\")[0].alias(\"node1\"),\n",
    "    F.col(\"edge\")[1].alias(\"node2\"))\n",
    "\n",
    "edges_df = edges_df.withColumn(\"weight\", lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbd948f7-73bd-4b19-b7f9-c136293489e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the pairs: ensure node1 is always less than node2, so they can be always on the same order\n",
    "edges_df = edges_df.withColumn(\"node1_norm\", least(col(\"node1\"), col(\"node2\"))) \\\n",
    "             .withColumn(\"node2_norm\", greatest(col(\"node1\"), col(\"node2\"))) \\\n",
    "             .select('node1_norm', 'node2_norm', 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79442c2c-4891-4a28-a4e1-c7ee2e5977a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = edges_df.groupBy(\"node1_norm\", \"node2_norm\").sum(\"weight\") \\\n",
    "                        .withColumnRenamed(\"sum(weight)\", \"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f398cbd5-f47b-42d8-9a89-6b92f963dcef",
   "metadata": {},
   "source": [
    "Create nodes df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c21d634-83bf-4501-a44e-fd2b20ea74c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+-------+-------+---------+-------+-----+\n",
      "|   id|      node|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|\n",
      "+-----+----------+-------+-------+-------+---------+-------+-----+\n",
      "|hk5r2|     never|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|    happen|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     never|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|      sure|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     never|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|      last|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     never|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|       two|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     never|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|      call|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     never|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|completely|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     never|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     phone|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     never|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|    forget|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     never|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     smoke|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     never|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|hk5r2|     three|    0.0|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "+-----+----------+-------+-------+-------+---------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+-------+-------+---------+-------+-----+\n",
      "|     id|      node|emo_pos|emo_neg|emo_anx|emo_anger|emo_sad|moral|\n",
      "+-------+----------+-------+-------+-------+---------+-------+-----+\n",
      "|100005j|everything|   2.86|    0.0|    0.0|      0.0|    0.0|  0.0|\n",
      "|1001497| depressed|   0.53|   4.79|   2.13|     0.53|    1.6|  0.0|\n",
      "|1001497|     first|   0.53|   4.79|   2.13|     0.53|    1.6|  0.0|\n",
      "|1001497|    friend|   0.53|   4.79|   2.13|     0.53|    1.6|  0.0|\n",
      "|1001497|   general|   0.53|   4.79|   2.13|     0.53|    1.6|  0.0|\n",
      "|10018jv|   attract|   1.36|   1.82|    0.0|     1.82|    0.0| 0.45|\n",
      "|10018jv|       see|   1.36|   1.82|    0.0|     1.82|    0.0| 0.45|\n",
      "|1001diz|      give|   0.78|   3.13|   1.56|      0.0|   0.78|  0.0|\n",
      "|1001diz|      rule|   0.78|   3.13|   1.56|      0.0|   0.78|  0.0|\n",
      "|1001mlo|    family|    1.0|   1.34|   0.67|     0.33|    0.0| 0.67|\n",
      "|1001mlo|     money|    1.0|   1.34|   0.67|     0.33|    0.0| 0.67|\n",
      "|1001mlo|     place|    1.0|   1.34|   0.67|     0.33|    0.0| 0.67|\n",
      "|1001mlo|       two|    1.0|   1.34|   0.67|     0.33|    0.0| 0.67|\n",
      "|1001uik|       gym|    0.0|   3.24|   1.21|     0.81|    0.0|  0.4|\n",
      "|1001uik|    muscle|    0.0|   3.24|   1.21|     0.81|    0.0|  0.4|\n",
      "|1001uik|     tired|    0.0|   3.24|   1.21|     0.81|    0.0|  0.4|\n",
      "|1001vtv|  continue|    2.1|    2.1|    0.0|      1.4|    0.0|  0.7|\n",
      "|10021rc|    effect|   1.01|   1.01|    0.0|      0.0|    0.0|  0.0|\n",
      "|10023zp|      come|   1.14|    0.0|    0.0|      0.0|    0.0| 0.57|\n",
      "|10023zp|     crazy|   1.14|    0.0|    0.0|      0.0|    0.0| 0.57|\n",
      "+-------+----------+-------+-------+-------+---------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nodes_df = df_explode.select(\n",
    "    F.col(\"id\"),\n",
    "    F.explode(F.col(\"edge\")).alias(\"node\"), F.col('emo_pos'),\n",
    "    F.col('emo_neg'), F.col('emo_anx'), F.col('emo_anger'), \n",
    "    F.col('emo_sad'), F.col('moral'))\n",
    "\n",
    "nodes_df.show(20) \n",
    "\n",
    "nodes_df_g = nodes_df.groupBy(\"id\", \"node\").agg(\n",
    "    F.first('emo_pos').alias('emo_pos'),\n",
    "    F.first('emo_neg').alias('emo_neg'),\n",
    "    F.first('emo_anx').alias('emo_anx'),\n",
    "    F.first('emo_anger').alias('emo_anger'),\n",
    "    F.first('emo_sad').alias('emo_sad'),\n",
    "    F.first('moral').alias('moral'))\n",
    "\n",
    "nodes_df_g.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b06cc1-ab84-43fb-8206-6bc551760e4e",
   "metadata": {},
   "source": [
    "Now, aggregate all words and average their emotions and morality scores for all documents in which they appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb4f9a38-1d6a-4d30-bb94-0c292bdb4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_nodes = nodes_df_g.groupBy(\"node\").agg(\n",
    "    F.avg('emo_pos').alias('emo_pos'),\n",
    "    F.avg('emo_neg').alias('emo_neg'),\n",
    "    F.avg('emo_anx').alias('emo_anx'),\n",
    "    F.avg('emo_anger').alias('emo_anger'),\n",
    "    F.avg('emo_sad').alias('emo_sad'),\n",
    "    F.avg('moral').alias('moral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d754d5cf-e5a5-4246-bf59-8348840c0fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|        node|           emo_pos|           emo_neg|            emo_anx|          emo_anger|            emo_sad|              moral|\n",
      "+------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|      online|0.9930773797743911|0.8642680915627681| 0.2325318292763375|0.14130945313015372|0.17353123556962863| 0.2742179563295732|\n",
      "|        hope|1.9679303776370307| 1.001623410638601|0.26047764179246635|0.15414400685746943| 0.2085620743844945|0.31705152626315547|\n",
      "|   recognize| 1.148999863369313| 1.108531220112037| 0.2857630823883044|0.21788222434758844| 0.1751523432162864| 0.4084150840278727|\n",
      "|      travel|1.2334344172451592|0.7930800146145416|0.22865911582024115|0.10651625867738401|0.17240591888929485|  0.233425283156741|\n",
      "|       inner|1.3315992585102798|1.0986198179979778|0.31553252443545665|0.17856083586113922|0.17701381867205937|0.40146275699359635|\n",
      "|       often|1.1084409291720103| 1.111802052159042| 0.3051147213909078|0.20062954253954682|0.19360944848225742| 0.3343929029499785|\n",
      "|  productive|1.0598410010349042|0.9140003763289111| 0.2164408693197855|0.13702700159939787|0.18140747012889266|0.27433342741556116|\n",
      "|conversation| 1.166337912890968|0.9520105232388193|0.28700847705349314|0.19012101724641917|0.13111604793919907| 0.3236351943876059|\n",
      "|     include|1.0839625260235948|0.9175303608605136|0.24829024982650932|0.15650503122831366|0.16437283136710618|0.31126561415683546|\n",
      "|       watch|1.1188408922195905|0.9117519177065292|0.22115698681466298|0.13717628760295517|0.16941213899395524| 0.2779730637350208|\n",
      "|     achieve|1.2117560636820184|0.7858980660326961|0.21653648894112626|0.12349983972646648|0.14536328667592693|0.28204615877764716|\n",
      "|      growth|1.2319115815691155|0.8968343711083439|0.25234620174346206|0.12298630136986304|0.14663760896637612|0.32516064757160656|\n",
      "|     explain|0.9724896714538658|1.0478723194963604| 0.2499754082234901|  0.220282313594334|0.17087940192799528|0.36650895140664963|\n",
      "|        slow|1.0695438640339923|0.9232141964508874| 0.2893726568357911|0.14497625593601599| 0.1415958510372407|0.24681579605098722|\n",
      "| perspective|1.2162104470115522|0.9404746358613761| 0.2341160220994475|0.16941319270048552|0.16740331491712707|  0.365004185501423|\n",
      "|   character|1.2394899650156506|0.9139679617013442| 0.2228374148407292|0.17957834652918434|0.14243601546676488|0.46958755293684395|\n",
      "|        grow|1.1875579739625712|1.0286530377000267|0.26667548142120956|  0.178085503119067| 0.1733465554651478| 0.3570029834553838|\n",
      "|      trauma|1.0287998767524262|2.2451255584655674| 0.3883207518101988|0.20658912340163305| 0.2669419195809582| 0.4092173779078724|\n",
      "|       leave|1.0203442683717914| 1.116791307571124|  0.296076359929571| 0.1949096469279955| 0.2003970901677324|0.33857427485867864|\n",
      "|       ready|1.0731666484298061|0.9362610788926576| 0.2622398511872196| 0.1354852828537039| 0.1726906663748769|0.29703030966188854|\n",
      "+------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_nodes.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6fd66-7afb-470c-aec4-4290a9a38bb6",
   "metadata": {},
   "source": [
    "Write to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e281823d-d1de-46d3-9ec5-73cf2be86bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_nodes.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"nodes4_node2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8fa598e-4ece-49d0-99a2-1449881d1ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "edges_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"edges4_node2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86dd9f-91b5-44bc-a641-adb80b66482b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
