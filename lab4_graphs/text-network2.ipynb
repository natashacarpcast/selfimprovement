{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a40b24b-306f-4024-85fd-4dede7c9e8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/25 20:56:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from graphframes import *\n",
    "import sparknlp\n",
    "from sparknlp.annotator import Tokenizer, PerceptronModel\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "import itertools\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"network\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c728af7d-9a85-4e6b-ac93-deabc1515641",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../1moral_data.csv\", header= True).select([\"id\", \"cleaned_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17cf322f-480c-479f-b5d4-8a8cf55d6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \n",
    "    \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"did\", \n",
    "    \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n",
    "    \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "    \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"must\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \n",
    "    \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"some\", \"such\", \n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"will\", \"ll\", \n",
    "    \"re\", \"ve\", \"d\", \"s\", \"m\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "    \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"many\", \"us\", \"ok\", \"hows\", \"ive\", \"ill\", \"im\", \"cant\", \"topics\", \"topic\",\n",
    "    \"discuss\", \"thoughts\", \"yo\", \"thats\", \"whats\", \"lets\", \"nothing\", \"oh\", \"omg\", \n",
    "         \"things\", \"stuff\", \"yall\", \"haha\", \"yes\", \"no\", \"wo\", \"like\", 'good', \n",
    "         'work', 'got', 'going', 'dont', 'really', 'want', 'make', 'think', \n",
    "         'know', 'feel', 'people', 'life', \"getting\", \"lot\" \"great\", \"i\", \"me\", \n",
    "         \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "        \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "        \"they\", \"them\", \"their\", \"theirs\",\"themselves\", \"what\", \"which\", \"who\", \n",
    "        \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "        \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\",\n",
    "        \"might\", \"must\", \"shall\", \"ought\", \"about\", \"above\", \"across\", \"after\", \n",
    "        \"against\", \"along\", \"amid\", \"among\", \"around\", \"as\", \"at\", \"before\", \"behind\",\n",
    "        \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\", \"by\", \n",
    "        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\",\n",
    "        \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"notwithstanding\",\n",
    "        \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\",\n",
    "        \"regarding\", \"round\", \"since\", \"than\", \"through\", \"throughout\", \"till\", \n",
    "        \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\",\n",
    "        \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\", \"cant\", \"cannot\", \n",
    "        \"couldve\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \n",
    "        \"havent\", \"hed\", \"hell\", \"hes\", \"howd\", \"howll\", \"hows\", \"id\", \"ill\", \n",
    "        \"im\", \"ive\", \"isnt\", \"itd\", \"itll\", \"its\", \"lets\", \"mightve\", \"mustve\", \n",
    "        \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldve\", \"shouldnt\", \n",
    "        \"thatll\", \"thats\", \"thered\", \"therell\", \"therere\", \"theres\", \"theyd\", \n",
    "        \"theyll\", \"theyre\", \"theyve\", \"wed\", \"well\", \"were\", \"weve\", \"werent\", \n",
    "        \"whatd\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"whend\", \"whenll\", \n",
    "        \"whens\", \"whered\", \"wherell\", \"wheres\", \"whichd\", \"whichll\", \"whichre\", \n",
    "        \"whichs\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\", \"whyd\", \"whyll\", \n",
    "        \"whys\", \"wont\", \"wouldve\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "        \"f\", \"m\", \"because\", \"go\", \"lot\", \"get\", \"still\", \"way\", \"something\", \"much\",\n",
    "        \"thing\", \"someone\", \"person\", \"anything\", \"goes\", \"ok\", \"so\", \"just\", \"mostly\", \n",
    "        \"put\", \"also\", \"lots\", \"yet\", \"ha\", \"etc\", \"even\", \"one\", \"bye\", \"take\", \"wasnt\"]\n",
    "\n",
    "time = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \n",
    "        \"sunday\", \"morning\", \"noon\", \"afternoon\", \"evening\", \"night\", \"midnight\",\n",
    "        \"dawn\", \"dusk\", \"week\", \"weekend\", \"weekends\",\"weekly\", \"today\", \n",
    "        \"yesterday\", \"tomorrow\", \"yesterdays\", \"todays\", \"mondays\", \"tuesdays\",\n",
    "        \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\", \"day\",\n",
    "        \"everyday\", \"daily\", \"workday\", 'time', 'month', 'year', 'pm', 'am', \"ago\",\n",
    "        \"year\", \"now\"]\n",
    "\n",
    "reddit = [\"welcome\", \"hi\", \"hello\", \"sub\", \"reddit\", \"thanks\", \"thank\", \"maybe\",\n",
    "          \"wo30\", \"mods\", \"mod\", \"moderators\", \"subreddit\", \"btw\", \"aw\", \"aww\", \n",
    "          \"aww\", \"hey\", \"hello\", \"join\", \"joined\", \"post\", \"rselfimprovement\", \"blah\"]\n",
    "\n",
    "topic_specific = [\"self\", \"improvement\", \"change\", \"action\",\n",
    "    'change', 'start', 'goal', 'habit', 'new', 'old', \n",
    "    'care', 'world', 'everyone', 'love', 'u', 'right', 'mean', 'matter',\n",
    "    'best', 'step', 'focus', 'hard', 'small',\n",
    "    'bad', 'help', 'time', 'problem', 'issue', 'advice',\n",
    "    'bit', 'experience', 'different',\n",
    "    'point', 'situation', 'negative', 'control', 'positive',\n",
    "    'use', 'question', 'idea', 'amp', 'medium', 'hour', 'day', 'minute',\n",
    "    'aaaaloot', \"selfimprovement\", \"_\"]\n",
    "\n",
    "stopwords = english + time + reddit + topic_specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d34f2080-e493-49fb-ba81-338f768943c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "     .setInputCol(\"cleaned_text\")\\\n",
    "     .setOutputCol('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be005bb-366c-410e-a11e-310d33b73250",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols(['document'])\\\n",
    "            .setOutputCol('tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac3915ad-cf3c-433d-9b61-894d94c77e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd6d31ab-d314-4b0e-8164-ed4cf9d779bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('cleaned') \\\n",
    "     .setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3185d58-e3db-484b-86f0-99cdb64bf4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = LemmatizerModel.load(\"../models/lemma_ewt_en_3.4.3_3.0_1651416655397/\")\\\n",
    "      .setInputCols(\"cleaned\")\\\n",
    "      .setOutputCol(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51267cb6-f9fd-47e8-871d-5600ef8151de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher\n",
    "\n",
    "finisher = Finisher().setInputCols(['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a21456d8-3741-46f4-8373-9a678fc749fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          tokenizer,\n",
    "          normalizer,\n",
    "          stopwords_cleaner,\n",
    "          lemmatizer,\n",
    "          finisher\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9020de41-7c5d-4ca6-bb02-144cb8770924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/software/spark-3.3.2-el8-x86_64/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|   id|        cleaned_text|      finished_words|\n",
      "+-----+--------------------+--------------------+\n",
      "|iqimz|i created this si...|[create, site, se...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipelineModel = my_pipeline.fit(data)\n",
    "processed_data = pipelineModel.transform(data)\n",
    "processed_data.persist()\n",
    "processed_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41038ffc-baee-4f81-a739-a37727bd8723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, cleaned_text: string, finished_words: array<string>, tf_features: vector, tf_idf_features: vector]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Apply TF-IDF filtering\n",
    "tfizer = CountVectorizer(inputCol='finished_words', outputCol='tf_features', minDF=0.01, vocabSize=1000)\n",
    "tf_model = tfizer.fit(processed_data)\n",
    "tf_result = tf_model.transform(processed_data)\n",
    "vocabulary = tf_model.vocabulary\n",
    "\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)\n",
    "\n",
    "processed_data.unpersist()\n",
    "tfidf_result.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80951065-503f-4b7f-95b5-c0e6ff90567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- finished_words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tf_features: vector (nullable = true)\n",
      " |-- tf_idf_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be0bbd-577d-44a7-acd5-47a893f7b63e",
   "metadata": {},
   "source": [
    "Asked ChatGPT \"how can I filter tokenized words from TF IDF?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "153fc1c1-6577-4a76-925a-d42dba5b01e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to filter words by their TF-IDF score\n",
    "# UDF to map indices to words using the vocabulary\n",
    "def filter_tfidf(features, threshold=25, vocabulary=None):\n",
    "    if features is not None:\n",
    "        # Filter based on TF-IDF score and map indices to actual words\n",
    "        return [vocabulary[features.indices[i]] for i in range(len(features.values)) if features.values[i] >= threshold]\n",
    "    return []\n",
    "\n",
    "# Register the UDF\n",
    "filter_udf = udf(lambda features: filter_tfidf(features, threshold=25, vocabulary=vocabulary), ArrayType(StringType()))\n",
    "\n",
    "# Apply the filtering function\n",
    "df_filtered_tfidf = tfidf_result.withColumn(\"filtered_words_tfidf\", filter_udf(\"tf_idf_features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04ef505d-3fcb-48f2-be03-26060f1b01ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   id|        cleaned_text|      finished_words|         tf_features|     tf_idf_features|filtered_words_tfidf|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|iqimz|i created this si...|[create, site, se...|(1000,[4,49,75,15...|(1000,[4,49,75,15...|                  []|\n",
      "|pfzt5|hello everyone  i...|[recently, take, ...|(1000,[7,11,19,23...|(1000,[7,11,19,23...|                  []|\n",
      "|pk714|i grew up with bo...|[grow, body, dysm...|(1000,[3,4,5,6,7,...|(1000,[3,4,5,6,7,...|                  []|\n",
      "|qu825|well to give ever...|[give, everybody,...|(1000,[1,8,10,16,...|(1000,[1,8,10,16,...|                  []|\n",
      "|rbi6h|i beginning to th...|[begin, inferiori...|(1000,[0,2,4,6,7,...|(1000,[0,2,4,6,7,...|                  []|\n",
      "|tpsp4|this might come a...|[come, rant, disj...|(1000,[0,1,2,3,7,...|(1000,[0,1,2,3,7,...|                  []|\n",
      "|u8ury|hey all i hope yo...|[hope, lazy, rout...|(1000,[3,36,69,73...|(1000,[3,36,69,73...|                  []|\n",
      "|ufxst|i have a huge pro...|[huge, sure, over...|(1000,[1,2,5,10,1...|(1000,[1,2,5,10,1...|                  []|\n",
      "|umfq3|24 useless colleg...|[useless, college...|(1000,[3,5,6,11,2...|(1000,[3,5,6,11,2...|                  []|\n",
      "|uov5x|so i am starting ...|[start, journey, ...|(1000,[3,5,7,9,11...|(1000,[3,5,7,9,11...|                  []|\n",
      "|uqxay|now to make a lon...|[long, story, sho...|(1000,[0,3,7,11,1...|(1000,[0,3,7,11,1...|                  []|\n",
      "|v3xws|im a sophmore in ...|[sophmore, high, ...|(1000,[0,1,3,5,7,...|(1000,[0,1,3,5,7,...|                  []|\n",
      "|v7x38|your persistence ...|[persistence, mea...|(1000,[14,39,169,...|(1000,[14,39,169,...|                  []|\n",
      "|vbh04|for quite awhile ...|[quite, awhile, a...|(1000,[2,4,8,13,4...|(1000,[2,4,8,13,4...|                  []|\n",
      "|vew4z|hi guys i really ...|[guy, want, say, ...|(1000,[0,2,3,5,6,...|(1000,[0,2,3,5,6,...|           [college]|\n",
      "|vt9pd|i want to make fr...|[_, easy, keep, r...|(1000,[0,1,3,7,10...|(1000,[0,1,3,7,10...|                  []|\n",
      "|vt7xo|i want to make fr...|[_, easy, keep, r...|(1000,[0,1,3,7,10...|(1000,[0,1,3,7,10...|                  []|\n",
      "|vrtwp|hey guys this is ...|[guy, first, time...|(1000,[0,1,3,5,10...|(1000,[0,1,3,5,10...|                  []|\n",
      "|vuwba|pardon me if i am...|[pardon, follow, ...|(1000,[0,3,4,5,6,...|(1000,[0,3,4,5,6,...|                  []|\n",
      "|vvoxg|work out every da...|[ever, already, p...|(1000,[0,1,2,5,8,...|(1000,[0,1,2,5,8,...|                  []|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_filtered_tfidf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31714a57-04e3-44e2-a9a4-ff95bec2ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_result.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14d51d-c86f-496d-819a-e8e81f0d6739",
   "metadata": {},
   "source": [
    "Create network - Asked chatgpt how to create all possible pair combinations between a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9aa27cd-ae0b-417b-a90b-ea211559dbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def generate_edges(tokens):\n",
    "    return [list(pair) for pair in itertools.combinations(tokens, 2)]\n",
    "\n",
    "generate_edges_udf = udf(generate_edges, ArrayType(ArrayType(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6c3e770-99d1-4261-9e8b-05c244ace1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges = df_filtered_tfidf.withColumn(\"edges\", generate_edges_udf(F.col(\"filtered_words_tfidf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69f14453-f3ba-49a4-a6eb-d920c98ab638",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flat_edges = df_edges.select(\n",
    "    F.col(\"id\"),\n",
    "    F.explode(F.col(\"edges\")).alias(\"edge\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4846b876-1579-4583-83e7-c5f4d6473f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|    id|   src|   dst|\n",
      "+------+------+------+\n",
      "|17ctnf|   kid|   fun|\n",
      "|1lnr36| cause| brain|\n",
      "|1lnr36| cause|stress|\n",
      "|1lnr36| cause| smile|\n",
      "|1lnr36| cause| field|\n",
      "|1lnr36| brain|stress|\n",
      "|1lnr36| brain| smile|\n",
      "|1lnr36| brain| field|\n",
      "|1lnr36|stress| smile|\n",
      "|1lnr36|stress| field|\n",
      "+------+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "edges_df = df_flat_edges.select(\n",
    "    F.col(\"id\"),\n",
    "    F.col(\"edge\")[0].alias(\"src\"),\n",
    "    F.col(\"edge\")[1].alias(\"dst\")\n",
    ")\n",
    "\n",
    "edges_df = edges_df\n",
    "edges_df.write.mode(\"overwrite\").csv(\"edges_network2\")\n",
    "edges_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be551ec1-459c-4415-a766-3132b3ad2e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vertices_df = edges_df.select(F.col(\"src\").alias(\"id\")).union(edges_df.select(F.col(\"dst\").alias(\"id\"))).distinct()\n",
    "vertices_df.write.mode(\"overwrite\").csv(\"nodes_network2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c73f4f9-6706-47fd-8478-87345ab1b3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==================================================>     (18 + 2) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|          id|\n",
      "+------------+\n",
      "|       inner|\n",
      "|      online|\n",
      "|conversation|\n",
      "|       often|\n",
      "|  productive|\n",
      "|     achieve|\n",
      "|       watch|\n",
      "|     explain|\n",
      "|   character|\n",
      "|        grow|\n",
      "+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vertices_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2596a811-6901-4424-b408-e52ec0a938f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphFrame(v:[id: string], e:[src: string, dst: string ... 1 more field])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = GraphFrame(vertices_df, edges_df)\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2079c385-7d63-49d2-8840-a516ded03803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n",
      "[Stage 20:=============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|           id|inDegree|\n",
      "+-------------+--------+\n",
      "|   individual|     117|\n",
      "|       belief|     109|\n",
      "|         rule|      75|\n",
      "|knowledgeable|      70|\n",
      "|  information|      69|\n",
      "|   successful|      64|\n",
      "|        ampxb|      62|\n",
      "|     question|      61|\n",
      "|      ability|      59|\n",
      "|      success|      57|\n",
      "|    happiness|      56|\n",
      "|      succeed|      53|\n",
      "|          dad|      53|\n",
      "|       unique|      53|\n",
      "|       pursue|      52|\n",
      "|    emotional|      52|\n",
      "|       desire|      51|\n",
      "|         task|      50|\n",
      "|     business|      47|\n",
      "|    motivated|      47|\n",
      "+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "network.inDegrees.orderBy(F.col(\"inDegree\"), ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b731fc5-1174-4a67-b48b-b436782a7c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|           id|          pagerank|\n",
      "+-------------+------------------+\n",
      "|    regularly|15.460319733225699|\n",
      "|      extreme|12.547538500509107|\n",
      "|       member|11.849140802317338|\n",
      "|       unique| 11.59687772077006|\n",
      "|    determine| 10.31618598885699|\n",
      "|         fire| 9.538834403651848|\n",
      "|        death|7.7503217283148365|\n",
      "|        quote| 7.216798789675523|\n",
      "|        extra| 6.739809718415769|\n",
      "|  development| 6.608371365117951|\n",
      "|   particular| 6.100894315069684|\n",
      "|   individual| 5.865006476139233|\n",
      "|    conscious|5.6630243440763675|\n",
      "|     kindness| 5.506858584973658|\n",
      "|        image| 5.488105883173017|\n",
      "|      address| 5.360595061267332|\n",
      "|     internal| 5.210499253867584|\n",
      "|      abusive| 5.187160527245134|\n",
      "|  communicate| 4.980519222798485|\n",
      "|          rid| 4.614553437967763|\n",
      "|      succeed| 4.538124397698883|\n",
      "|       sister| 4.061027125671454|\n",
      "|       reward|  4.05178771762602|\n",
      "|    behaviour|4.0247676114382225|\n",
      "|       boring|3.9977672453342947|\n",
      "|      concept|3.8569344187330152|\n",
      "|       commit|3.8470547189391615|\n",
      "|communication| 3.804556883231325|\n",
      "|         door|3.6995483672254723|\n",
      "|        loser|3.6918899421237725|\n",
      "|        large| 3.430935569465542|\n",
      "|    motivated|3.3556270929337564|\n",
      "|       delete| 3.346765184326562|\n",
      "|        inner| 3.308864203492659|\n",
      "|         rude|3.2973326430323464|\n",
      "|   perception| 3.263971833526872|\n",
      "|     schedule| 3.242062284608932|\n",
      "|      outcome|3.2134311076930864|\n",
      "|         adhd|3.1482816116592467|\n",
      "|         harm| 3.084749798696288|\n",
      "|         meal|3.0140412472565368|\n",
      "|           bc| 2.981041602416798|\n",
      "|      culture| 2.980355993722621|\n",
      "|      workout| 2.980338305402124|\n",
      "|        reply|2.9772156547276443|\n",
      "|     laziness|2.9529244522514255|\n",
      "|          fat| 2.937393759219076|\n",
      "|         apps| 2.838823284227787|\n",
      "|       impact| 2.835302402080489|\n",
      "|      pattern|2.8189786385129207|\n",
      "+-------------+------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = network.pageRank(resetProbability=0.01, maxIter=20)\n",
    "results.vertices.select(\"id\", \"pagerank\").orderBy(F.col(\"pagerank\"), ascending=False).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ac4fc-c4e4-4c53-bcca-f54c060afca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
