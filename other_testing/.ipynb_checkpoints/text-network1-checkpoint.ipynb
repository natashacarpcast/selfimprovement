{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1977c0-f9fe-42ea-8638-3876ae92b876",
   "metadata": {},
   "source": [
    "Asked ChatGPT how to convert bigrams into a network and it showed me the workflow which I corrected for errors and adapted to my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15971f5-6ba3-41f0-a9d1-e30f4df9cda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/24 20:34:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "Warning::Spark Session already created, some configs may not take.\n",
      "24/11/24 20:34:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from graphframes import *\n",
    "import sparknlp\n",
    "from sparknlp.annotator import Tokenizer, PerceptronModel\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"network\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark_nlp = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b98d5b-7611-453c-b0d3-ff06bc630977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|    id|        cleaned_text|\n",
      "+------+--------------------+\n",
      "|163nsc|i discovered a fe...|\n",
      "|1wwiwh|every morning i h...|\n",
      "|204aqo|it seems as thoug...|\n",
      "|5ircyx|hey hey  im back ...|\n",
      "|6oybi8|m20 today a girl ...|\n",
      "|a2z51v|im i just fucking...|\n",
      "|bihqtw|hey guys so some ...|\n",
      "|ejp9qr|i stumbled upon t...|\n",
      "|elnp1n|hello everyone iv...|\n",
      "|ff73wp|in moments where ...|\n",
      "+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"../1moral_data.csv\", header= True).select([\"id\", \"cleaned_text\"])\n",
    "data = data.sample(0.001)\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc25d006-9b9e-4f2d-b5e7-1468315de212",
   "metadata": {},
   "source": [
    "Preprocess and get tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "339744d3-b3e8-4d10-862c-a6a24ed00f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \n",
    "    \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"did\", \n",
    "    \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n",
    "    \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "    \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"must\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \n",
    "    \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"some\", \"such\", \n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"will\", \"ll\", \n",
    "    \"re\", \"ve\", \"d\", \"s\", \"m\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "    \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"many\", \"us\", \"ok\", \"hows\", \"ive\", \"ill\", \"im\", \"cant\", \"topics\", \"topic\",\n",
    "    \"discuss\", \"thoughts\", \"yo\", \"thats\", \"whats\", \"lets\", \"nothing\", \"oh\", \"omg\", \n",
    "         \"things\", \"stuff\", \"yall\", \"haha\", \"yes\", \"no\", \"wo\", \"like\", 'good', \n",
    "         'work', 'got', 'going', 'dont', 'really', 'want', 'make', 'think', \n",
    "         'know', 'feel', 'people', 'life', \"getting\", \"lot\" \"great\", \"i\", \"me\", \n",
    "         \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "        \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "        \"they\", \"them\", \"their\", \"theirs\",\"themselves\", \"what\", \"which\", \"who\", \n",
    "        \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "        \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\",\n",
    "        \"might\", \"must\", \"shall\", \"ought\", \"about\", \"above\", \"across\", \"after\", \n",
    "        \"against\", \"along\", \"amid\", \"among\", \"around\", \"as\", \"at\", \"before\", \"behind\",\n",
    "        \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\", \"by\", \n",
    "        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\",\n",
    "        \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"notwithstanding\",\n",
    "        \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\",\n",
    "        \"regarding\", \"round\", \"since\", \"than\", \"through\", \"throughout\", \"till\", \n",
    "        \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\",\n",
    "        \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\", \"cant\", \"cannot\", \n",
    "        \"couldve\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \n",
    "        \"havent\", \"hed\", \"hell\", \"hes\", \"howd\", \"howll\", \"hows\", \"id\", \"ill\", \n",
    "        \"im\", \"ive\", \"isnt\", \"itd\", \"itll\", \"its\", \"lets\", \"mightve\", \"mustve\", \n",
    "        \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldve\", \"shouldnt\", \n",
    "        \"thatll\", \"thats\", \"thered\", \"therell\", \"therere\", \"theres\", \"theyd\", \n",
    "        \"theyll\", \"theyre\", \"theyve\", \"wed\", \"well\", \"were\", \"weve\", \"werent\", \n",
    "        \"whatd\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"whend\", \"whenll\", \n",
    "        \"whens\", \"whered\", \"wherell\", \"wheres\", \"whichd\", \"whichll\", \"whichre\", \n",
    "        \"whichs\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\", \"whyd\", \"whyll\", \n",
    "        \"whys\", \"wont\", \"wouldve\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "        \"f\", \"m\", \"because\", \"go\", \"lot\", \"get\", \"still\", \"way\", \"something\", \"much\",\n",
    "        \"thing\", \"someone\", \"person\", \"anything\", \"goes\", \"ok\", \"so\", \"just\", \"mostly\", \n",
    "        \"put\", \"also\", \"lots\", \"yet\", \"ha\", \"etc\", \"even\", \"one\", \"bye\", \"take\"]\n",
    "\n",
    "time = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \n",
    "        \"sunday\", \"morning\", \"noon\", \"afternoon\", \"evening\", \"night\", \"midnight\",\n",
    "        \"dawn\", \"dusk\", \"week\", \"weekend\", \"weekends\",\"weekly\", \"today\", \n",
    "        \"yesterday\", \"tomorrow\", \"yesterdays\", \"todays\", \"mondays\", \"tuesdays\",\n",
    "        \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\", \"day\",\n",
    "        \"everyday\", \"daily\", \"workday\", 'time', 'month', 'year', 'pm', 'am', \"ago\",\n",
    "        \"year\", \"now\"]\n",
    "\n",
    "reddit = [\"welcome\", \"hi\", \"hello\", \"sub\", \"reddit\", \"thanks\", \"thank\", \"maybe\",\n",
    "          \"wo30\", \"mods\", \"mod\", \"moderators\", \"subreddit\", \"btw\", \"aw\", \"aww\", \n",
    "          \"aww\", \"hey\", \"hello\", \"join\", \"joined\", \"post\", \"rselfimprovement\", \"blah\"]\n",
    "\n",
    "topic_specific = [\"self\", \"improvement\", \"change\", \"action\",\n",
    "    'change', 'start', 'goal', 'habit', 'new', 'old', \n",
    "    'care', 'world', 'everyone', 'love', 'u', 'right', 'mean', 'matter',\n",
    "    'best', 'step', 'focus', 'hard', 'small',\n",
    "    'bad', 'help', 'time', 'problem', 'issue', 'advice',\n",
    "    'bit', 'experience', 'different',\n",
    "    'point', 'situation', 'negative', 'control', 'positive',\n",
    "    'use', 'question', 'idea', 'amp', 'medium', 'hour', 'day', 'minute',\n",
    "    'aaaaloot', \"selfimprovement\", \"_\"]\n",
    "\n",
    "stopwords = english + time + reddit + topic_specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83126f4e-9686-4e62-a9cc-45536169c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "     .setInputCol(\"cleaned_text\")\\\n",
    "     .setOutputCol('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e241496-7e88-4d42-b563-3368fce129a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols(['document'])\\\n",
    "            .setOutputCol('tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02645fca-5d88-4f53-8276-22837f01cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b73cf75-5450-4a82-b5f3-009867d04c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher\n",
    "\n",
    "finisher = Finisher().setInputCols(['normalized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48059742-500e-446c-bf75-b37840cf3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          tokenizer,\n",
    "          normalizer,\n",
    "          finisher\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16765900-49e1-489f-b062-5842ddad2092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/software/spark-3.3.2-el8-x86_64/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|    id|        cleaned_text| finished_normalized|\n",
      "+------+--------------------+--------------------+\n",
      "|163nsc|i discovered a fe...|[i, discovered, a...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = my_pipeline.fit(data)\n",
    "processed_data = pipelineModel.transform(data)\n",
    "processed_data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5e58c-4c4d-495d-935a-5acaea4fc340",
   "metadata": {},
   "source": [
    "Create bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "537a06f2-50f6-436d-b6dd-61282396cee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                             bigrams|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[[low, confidence], [hardly, better], [youtube, video], [unlocked, several], [several, habits], [...|\n",
      "|                [[drive, home], [home, depending], [traffic, days], [decent, chunk], [car, driving]]|\n",
      "|[[worst, years], [recall, anyways], [mom, always], [always, told], [pushed, away], [constant, pur...|\n",
      "|[[another, invitation], [entrench, habits], [nearing, participants], [using, slack], [dedicated, ...|\n",
      "|[[girl, accused], [totally, changed], [pretty, pathetic], [nerdy, crap], [light, blue], [blue, sa...|\n",
      "|[[fucking, lazy], [always, see], [brainless, nice], [nice, guy], [shit, memory], [watch, tone], [...|\n",
      "|[[context, first], [nerdy, kid], [always, kind], [held, back], [dream, school], [minority, card],...|\n",
      "|[[graduate, college], [emotional, trauma], [abuse, due], [stop, making], [making, excuses], [bigg...|\n",
      "|[[hoping, posting], [absolute, mess], [mess, multiple], [multiple, mental], [mental, health], [he...|\n",
      "|                       [[quickly, pulled], [animal, rescue], [animal, suffering], [desperate, need]]|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "def generate_bigrams(tokens):\n",
    "    return [(tokens[i], tokens[i+1]) for i in range(len(tokens) - 1) \\\n",
    "           if tokens[i] not in stopwords and tokens[i+1] not in stopwords]\n",
    "                    \n",
    "bigram_udf = F.udf(generate_bigrams, T.ArrayType(T.ArrayType(T.StringType())))\n",
    "bigrams_df = processed_data.withColumn(\"bigrams\", bigram_udf(F.col(\"finished_normalized\")))\n",
    "\n",
    "bigrams_df.select('bigrams').show(10, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c401947-52e3-4440-94e5-624e6aae3a01",
   "metadata": {},
   "source": [
    "Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b57a00b1-2cb2-4160-96bf-c4a77db1b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = bigrams_df.select(\"id\", F.explode(F.col(\"bigrams\")).alias(\"bigram\"))\n",
    "edges_df = edges_df.select(\n",
    "    F.col(\"bigram\")[0].alias(\"src\"),\n",
    "    F.col(\"bigram\")[1].alias(\"dst\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f77a59d-0c66-4d7e-b48e-0933281e081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|      src|       dst|\n",
      "+---------+----------+\n",
      "|      low|confidence|\n",
      "|   hardly|    better|\n",
      "|  youtube|     video|\n",
      "| unlocked|   several|\n",
      "|  several|    habits|\n",
      "|    goals|   whether|\n",
      "|     fold|   laundry|\n",
      "|    arent|     happy|\n",
      "|   anyone|      else|\n",
      "|     else|     happy|\n",
      "|   direct|       set|\n",
      "|      set|     goals|\n",
      "| granular|    always|\n",
      "|   always|       ask|\n",
      "|  success|       ask|\n",
      "|  anyones| happiness|\n",
      "|    happy|    trying|\n",
      "|  impress|    others|\n",
      "|   always|      fail|\n",
      "|defending|    others|\n",
      "+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce81a3-cd1a-446c-908e-0232d8ce8716",
   "metadata": {},
   "source": [
    "Create vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc8d0725-a28f-478b-8fa5-6fe2cc725913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vertices_df = edges_df.select(F.col(\"src\").alias(\"id\")).union(edges_df.select(F.col(\"dst\").alias(\"id\"))).distinct()\n",
    "vertices_df.write.mode(\"overwrite\").csv(\"vertices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "842692e5-e511-4b98-87b6-6eae7ac1d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|                id|\n",
      "+------------------+\n",
      "|rgoodbyedepression|\n",
      "|             often|\n",
      "|            taking|\n",
      "|         partially|\n",
      "|           achieve|\n",
      "|             watch|\n",
      "|           traffic|\n",
      "|          graduate|\n",
      "|         basically|\n",
      "|            highly|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vertices_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a19ef05d-51c8-4bcc-a715-68ae2c2c96f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|      src|     dst|\n",
      "+---------+--------+\n",
      "|  desires|   owned|\n",
      "|  workout|   often|\n",
      "|    dream|  school|\n",
      "| unlocked| several|\n",
      "|     fail| forgive|\n",
      "|     thus|creating|\n",
      "|  traffic|    days|\n",
      "|brainless|    nice|\n",
      "|  context|   first|\n",
      "| absolute|    mess|\n",
      "+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "edges_df = edges_df.distinct()\n",
    "edges_df.write.mode(\"overwrite\").csv(\"edges\")\n",
    "edges_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db80b540-acc7-450a-bc14-7005679f606f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/dataframe.py:148: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphFrame(v:[id: string], e:[src: string, dst: string])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = GraphFrame(vertices_df, edges_df)\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23dde3a5-ef0c-4226-9abb-145f13d4d861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|        id|inDegree|\n",
      "+----------+--------+\n",
      "|     every|      20|\n",
      "|    social|      13|\n",
      "|      back|      11|\n",
      "|    school|      10|\n",
      "|    others|      10|\n",
      "|everything|       9|\n",
      "|       job|       9|\n",
      "|      part|       9|\n",
      "|   friends|       8|\n",
      "|    better|       8|\n",
      "|     wrong|       7|\n",
      "|       try|       7|\n",
      "|     years|       7|\n",
      "|    enough|       7|\n",
      "|     truth|       7|\n",
      "|     first|       7|\n",
      "|     hours|       7|\n",
      "|       ask|       7|\n",
      "|      stop|       7|\n",
      "|     human|       6|\n",
      "+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network.inDegrees.orderBy(F.col(\"inDegree\"), ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f005237-a95c-4527-b9de-595301d62a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|           id|          pagerank|\n",
      "+-------------+------------------+\n",
      "|        femmy| 36.50755241322142|\n",
      "| distractions| 36.50755241322142|\n",
      "|        every|15.570693741953018|\n",
      "|         part|11.166338748531777|\n",
      "|       school|   9.2506214000977|\n",
      "|      mindset| 8.822736565721419|\n",
      "|         stop| 8.197516504777868|\n",
      "|       social|7.7860415519264485|\n",
      "|      friends| 6.917299732502962|\n",
      "|       almost| 6.791626158077601|\n",
      "|        truth| 6.712792304029796|\n",
      "|         back| 6.631590769467356|\n",
      "|        first|  6.50212656297195|\n",
      "|       months| 6.346217234274218|\n",
      "|     although| 6.203777764284255|\n",
      "|   everything| 5.781925694292085|\n",
      "|        toxic| 5.780099076070397|\n",
      "|         away| 5.582484758795605|\n",
      "|        fixed| 5.477750409187343|\n",
      "|       eating|  5.41253139166865|\n",
      "|      luckily| 5.221437246518929|\n",
      "|         joke| 5.141714904134607|\n",
      "|         else| 5.060567061161978|\n",
      "|         made| 4.996619243753398|\n",
      "|       issues|4.9951133546254765|\n",
      "|       family| 4.977421754569161|\n",
      "|       making| 4.967726790563315|\n",
      "|          try| 4.761689938196581|\n",
      "|relationships| 4.646024890844656|\n",
      "|       others| 4.626737617285052|\n",
      "|      project| 4.592006151635084|\n",
      "|         ever| 4.519261586640523|\n",
      "|     together|  4.48705107639485|\n",
      "|       habits| 4.477858828944028|\n",
      "|       better| 4.387672393256272|\n",
      "|       growth| 4.366697219610022|\n",
      "|    sometimes| 4.338520606578008|\n",
      "|       untill| 4.330429947489365|\n",
      "|        tasks|4.3213744280276165|\n",
      "|        women| 4.294087806266827|\n",
      "|      educate| 4.271714897204568|\n",
      "|        hours|4.1863279281688675|\n",
      "|      perhaps|  4.09610511993954|\n",
      "|          set| 4.095870714064827|\n",
      "|       humans|4.0041102848689105|\n",
      "|           vs|  4.00342048822103|\n",
      "|         food| 3.962437215569006|\n",
      "|         look|  3.87530676759167|\n",
      "|        stick|3.8550841887297165|\n",
      "|      complex|3.8266478675054816|\n",
      "+-------------+------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = network.pageRank(resetProbability=0.01, maxIter=20)\n",
    "results.vertices.select(\"id\", \"pagerank\").orderBy(F.col(\"pagerank\"), ascending=False).show(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
