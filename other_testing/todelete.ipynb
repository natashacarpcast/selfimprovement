{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b6aa00-eeb1-46d2-9970-0cc7142d605d",
   "metadata": {},
   "source": [
    "Used tutorial https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP/blob/master/Topic_Modelling_with_PySpark_and_Spark_NLP.ipynb for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80dfe811-6044-456b-9b67-1dd90f647207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "24/11/20 20:15:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import Tokenizer, PerceptronModel\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from sparknlp.annotator import NGramGenerator\n",
    "from sparknlp.base import Finisher\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18ccc18-b119-472f-af9c-19a44eeea279",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../data_topicmodel.csv\", header= True).select([\"id\", \"cleaned_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d856c9-673a-4d0b-a03f-732af6820a65",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7911044e-82b5-4034-a43b-16e1306cc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "     .setInputCol(\"cleaned_text\")\\\n",
    "     .setOutputCol('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b99eb0-aaee-49e6-accd-8382b9778873",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols(['document'])\\\n",
    "            .setOutputCol('tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e08506b3-9c85-4b12-915c-179501a1fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7785596-e18b-44b5-a57a-602da0c6c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \n",
    "    \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"did\", \n",
    "    \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n",
    "    \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "    \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"must\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \n",
    "    \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"some\", \"such\", \n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"will\", \"ll\", \n",
    "    \"re\", \"ve\", \"d\", \"s\", \"m\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "    \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"many\", \"us\", \"ok\", \"hows\", \"ive\", \"ill\", \"im\", \"cant\", \"topics\", \"topic\",\n",
    "    \"discuss\", \"thoughts\", \"yo\", \"thats\", \"whats\", \"lets\", \"nothing\", \"oh\", \"omg\", \n",
    "         \"things\", \"stuff\", \"yall\", \"haha\", \"yes\", \"no\", \"wo\", \"like\", 'good', \n",
    "         'work', 'got', 'going', 'dont', 'really', 'want', 'make', 'think', \n",
    "         'know', 'feel', 'people', 'life', \"getting\", \"lot\" \"great\", \"i\", \"me\", \n",
    "         \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "        \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "        \"they\", \"them\", \"their\", \"theirs\",\"themselves\", \"what\", \"which\", \"who\", \n",
    "        \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "        \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\",\n",
    "        \"might\", \"must\", \"shall\", \"ought\", \"about\", \"above\", \"across\", \"after\", \n",
    "        \"against\", \"along\", \"amid\", \"among\", \"around\", \"as\", \"at\", \"before\", \"behind\",\n",
    "        \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\", \"by\", \n",
    "        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\",\n",
    "        \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"notwithstanding\",\n",
    "        \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\",\n",
    "        \"regarding\", \"round\", \"since\", \"than\", \"through\", \"throughout\", \"till\", \n",
    "        \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\",\n",
    "        \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\", \"cant\", \"cannot\", \n",
    "        \"couldve\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \n",
    "        \"havent\", \"hed\", \"hell\", \"hes\", \"howd\", \"howll\", \"hows\", \"id\", \"ill\", \n",
    "        \"im\", \"ive\", \"isnt\", \"itd\", \"itll\", \"its\", \"lets\", \"mightve\", \"mustve\", \n",
    "        \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldve\", \"shouldnt\", \n",
    "        \"thatll\", \"thats\", \"thered\", \"therell\", \"therere\", \"theres\", \"theyd\", \n",
    "        \"theyll\", \"theyre\", \"theyve\", \"wed\", \"well\", \"were\", \"weve\", \"werent\", \n",
    "        \"whatd\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"whend\", \"whenll\", \n",
    "        \"whens\", \"whered\", \"wherell\", \"wheres\", \"whichd\", \"whichll\", \"whichre\", \n",
    "        \"whichs\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\", \"whyd\", \"whyll\", \n",
    "        \"whys\", \"wont\", \"wouldve\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "        \"f\", \"m\", \"because\", \"go\", \"lot\", \"get\", \"still\", \"way\", \"something\", \"much\",\n",
    "        \"thing\", \"someone\", \"person\", \"anything\", \"goes\", \"ok\", \"so\", \"just\", \"mostly\", \n",
    "        \"put\", \"also\", \"lots\", \"yet\", \"ha\", \"etc\"]\n",
    "\n",
    "time = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \n",
    "        \"sunday\", \"morning\", \"noon\", \"afternoon\", \"evening\", \"night\", \"midnight\",\n",
    "        \"dawn\", \"dusk\", \"week\", \"weekend\", \"weekends\",\"weekly\", \"today\", \n",
    "        \"yesterday\", \"tomorrow\", \"yesterdays\", \"todays\", \"mondays\", \"tuesdays\",\n",
    "        \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\", \"day\",\n",
    "        \"everyday\", \"daily\", \"workday\", 'time', 'month', 'year', 'pm', 'am', \"ago\",\n",
    "        \"year\"]\n",
    "\n",
    "reddit = [\"welcome\", \"hi\", \"hello\", \"sub\", \"reddit\", \"thanks\", \"thank\", \"maybe\",\n",
    "          \"wo30\", \"mods\", \"mod\", \"moderators\", \"subreddit\", \"btw\", \"aw\", \"aww\", \n",
    "          \"aww\", \"hey\", \"hello\", \"join\", \"joined\", \"post\", \"rselfimprovement\"]\n",
    "\n",
    "topic_specific = [\"self\", \"improvement\", \"change\", \"action\",\n",
    "    'change', 'start', 'goal', 'habit', 'new', 'old', \n",
    "    'care', 'world', 'everyone', 'love', 'u', 'right', 'mean', 'matter',\n",
    "    'best', 'step', 'focus', 'hard', 'small',\n",
    "    'bad', 'help', 'time', 'problem', 'issue', 'advice',\n",
    "    'bit', 'experience', 'different',\n",
    "    'point', 'situation', 'negative', 'control', 'positive',\n",
    "    'use', 'question', 'idea', 'amp', 'medium', 'hour', 'day', 'minute',\n",
    "    'aaaaloot', \"selfimprovement\", \"_\"]\n",
    "\n",
    "stopwords = english + time + reddit + topic_specific\n",
    "\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95e33a63-8c34-4a81-8fa1-157c86492364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sparknlp.annotator import NGramGenerator\n",
    "\n",
    "#ngrammer = NGramGenerator() \\\n",
    " #   .setInputCols(['normalized']) \\\n",
    "  #  .setOutputCol('ngrams') \\\n",
    "   # .setN(3) \\\n",
    "    #.setEnableCumulative(True) \\\n",
    "    #.setDelimiter('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5843616-767e-448f-8943-006848263e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pos = PerceptronModel.load(\"/project/macs40123/spark-jars/pos_anc_en_3.0.0_3.0_1614962126490/\")\\\n",
    "      .setInputCols(\"document\", \"unigrams\")\\\n",
    "      .setOutputCol(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20199976-1358-47e2-8685-a8e54cc73200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher\n",
    "\n",
    "finisher = Finisher().setInputCols(['unigrams', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a9dbb2-1f65-4c5b-826e-feedefbf682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          tokenizer,\n",
    "          normalizer,\n",
    "          stopwords_cleaner,\n",
    "          #ngrammer,\n",
    "          pos,\n",
    "          finisher\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "680ea3cb-e5cf-4de6-aae3-c8e4ac8a3258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/software/spark-3.3.2-el8-x86_64/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|   id|        cleaned_text|   finished_unigrams|        finished_pos|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|hk5r2|i had an appointm...|[appointment, den...|[NN, NN, JJ, CD, ...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = my_pipeline.fit(data)\n",
    "processed_data = pipelineModel.transform(data)\n",
    "processed_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "198344d1-7dcc-4258-b5a1-4bba2466aac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'cleaned_text', 'finished_unigrams', 'finished_pos']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7598af2e-4e59-4115-877d-95bf5fe8552e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                   finished_unigrams|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[appointment, dentist, last, one, reminder, email, two, reminder, phone, calls, marked, google, c...|\n",
      "|[created, site, several, months, finished, decided, release, beta, form, rather, simply, stagnate...|\n",
      "|[recently, took, made, changes, overall, look, hope, pleasing, enough, vision, place, share, info...|\n",
      "|[grew, body, dysmorphia, eating, disorders, social, anixety, depression, early, years, remember, ...|\n",
      "|[ask, content, never, process, stop, made, speak, months, hopeless, yearold, drunk, regular, frie...|\n",
      "|[opportunity, exists, feeling, opportunities, recharge, motivation, build, selfconfidence, affirm...|\n",
      "|[comfort, zone, friends, ton, probably, hit, bar, club, never, comfortable, environment, alcohol,...|\n",
      "|[first, posting, learn, social, terrible, talk, talking, brand, usually, keep, quiet, head, say, ...|\n",
      "|[facebook, great, sharing, ideas, keeping, touch, sit, two, hours, bedtime, making, stupid, jokes...|\n",
      "|[okay, years, male, semiasian, stand, shy, americans, weigh, kg, standard, eat, heaps, eat, colle...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_data.select('finished_unigrams').show(10, truncate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c9dbe0a-f678-4a30-b0be-52677577fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|   id|        cleaned_text|   finished_unigrams|        finished_pos|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|hk5r2|i had an appointm...|[appointment, den...|[NN, NN, JJ, CD, ...|\n",
      "|iqimz|i created this si...|[created, site, s...|[VBN, NN, JJ, NNS...|\n",
      "|pfzt5|hello everyone  i...|[recently, took, ...|[RB, VBD, VBN, NN...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4834ef54-290e-406a-a357-68c12c100c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, cleaned_text: string, finished_unigrams: array<string>, finished_pos: array<string>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a88ada38-b7f7-43d9-a593-93b7ed787514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unigrams(finished_unigrams, finished_pos):\n",
    "    '''Filters individual words based on their POS tag'''\n",
    "    return [word for word, pos in zip(finished_unigrams, finished_pos)\n",
    "            if pos in ['JJ', 'NN', 'NNS', 'NNPS']]\n",
    "\n",
    "udf_filter_unigrams = F.udf(filter_unigrams, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4ebd6c8-ba26-47bf-9db8-f761f1a0e921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                            filtered_unigrams_by_pos|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[appointment, dentist, last, reminder, email, reminder, phone, calls, google, calendar, glance, t...|\n",
      "|[site, several, months, release, beta, form, forgive, useful, community, form, direction, future,...|\n",
      "|[changes, hope, vision, place, share, information, mindset, prosperity, mindset, money, purpose, ...|\n",
      "|[body, dysmorphia, disorders, social, anixety, depression, early, years, fat, weight, case, healt...|\n",
      "|[content, process, stop, speak, months, hopeless, yearold, drunk, regular, friends, wreck, apartm...|\n",
      "|[opportunity, feeling, opportunities, motivation, build, selfconfidence, opportunity, existswhen,...|\n",
      "|[comfort, zone, friends, ton, bar, club, comfortable, environment, alcohol, noise, crowds, edge, ...|\n",
      "|[first, learn, social, terrible, talk, brand, quiet, head, stupid, terrible, trait, break, happen...|\n",
      "|[facebook, great, sharing, ideas, touch, hours, stupid, jokes, amount, amount, social, interactio...|\n",
      "|[okay, years, male, semiasian, stand, shy, americans, kg, standard, eat, heaps, college, friends,...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_data = processed_data.withColumn('filtered_unigrams_by_pos', udf_filter_unigrams(\n",
    "                                                   F.col('finished_unigrams'),\n",
    "                                                   F.col('finished_pos')))\n",
    "\n",
    "processed_data.select(\"filtered_unigrams_by_pos\").show(10, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "817d0b5a-ff0d-4f00-bfb7-2fc8a993e388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'cleaned_text',\n",
       " 'finished_unigrams',\n",
       " 'finished_pos',\n",
       " 'filtered_unigrams_by_pos']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53a75ec1-9139-4fac-8b38-0295dbb18082",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "241ee1e3-18af-4c54-95d9-d717e5cde253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that POS was done, lemmatization makes more sense at this point\n",
    "\n",
    "#Merge tokens as just one string to be able to take it as a document in the new Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "tokens_as_string = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_data = processed_data.withColumn('joined_tokens', tokens_as_string(F.col('filtered_unigrams_by_pos')))\n",
    "\n",
    "last_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('joined_tokens') \\\n",
    "     .setOutputCol('joined_document')\n",
    "\n",
    "last_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['joined_document']) \\\n",
    "     .setOutputCol('tokenized')\n",
    "     \n",
    "lemmatizer = LemmatizerModel.load(\"../models/lemma_ewt_en_3.4.3_3.0_1651416655397/\")\\\n",
    "      .setInputCols(\"tokenized\")\\\n",
    "      .setOutputCol(\"lemmatized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('final') \\\n",
    "     .setStopWords(stopwords)\n",
    "\n",
    "last_finisher = Finisher() \\\n",
    "     .setInputCols(['final']) \\\n",
    "\n",
    "last_pipeline = Pipeline() \\\n",
    "     .setStages([last_documentAssembler,                  \n",
    "                 last_tokenizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 last_finisher])\n",
    "\n",
    "final_data = last_pipeline.fit(processed_data).transform(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a1a0d7a-f91f-4ba5-89d4-68a5f2aaf873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-----------------+------------+------------------------+-------------+--------------+\n",
      "|   id|cleaned_text|finished_unigrams|finished_pos|filtered_unigrams_by_pos|joined_tokens|finished_final|\n",
      "+-----+------------+-----------------+------------+------------------------+-------------+--------------+\n",
      "|hk5r2|  i had a...|       [appoin...|  [NN, NN...|              [appoin...|   appoint...|    [appoin...|\n",
      "|iqimz|  i creat...|       [create...|  [VBN, N...|              [site, ...|   site se...|    [site, ...|\n",
      "|pfzt5|  hello e...|       [recent...|  [RB, VB...|              [change...|   changes...|    [hope, ...|\n",
      "|pk714|  i grew ...|       [grew, ...|  [VBD, N...|              [body, ...|   body dy...|    [body, ...|\n",
      "|q0q8x|  i have ...|       [ask, c...|  [VB, NN...|              [conten...|   content...|    [conten...|\n",
      "|q412v|  nothing...|       [opport...|  [NN, VB...|              [opport...|   opportu...|    [opport...|\n",
      "|q5mqk|  im gett...|       [comfor...|  [NN, NN...|              [comfor...|   comfort...|    [comfor...|\n",
      "|q70xe|  hey eve...|       [first,...|  [JJ, VB...|              [first,...|   first l...|    [first,...|\n",
      "|q7mrn|  faceboo...|       [facebo...|  [NN, JJ...|              [facebo...|   faceboo...|    [facebo...|\n",
      "|qcsyp|  okay so...|       [okay, ...|  [JJ, NN...|              [okay, ...|   okay ye...|    [okay, ...|\n",
      "+-----+------------+-----------------+------------+------------------------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show(10, truncate=10)\n",
    "                                                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7deb738d-484f-45f8-b6a4-12430b06111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                      finished_final|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[appointment, dentist, last, reminder, email, reminder, phone, call, google, calendar, glance, ph...|\n",
      "|[site, several, release, beta, forgive, useful, community, direction, future, development, fix, f...|\n",
      "|[hope, vision, place, share, information, mindset, prosperity, mindset, money, purpose, attitude,...|\n",
      "|[body, dysmorphia, disorders, social, anixety, depression, early, fat, weight, case, healthy, ave...|\n",
      "|[content, process, stop, speak, hopeless, yearold, drunk, regular, wreck, apartment, romance, int...|\n",
      "|[opportunity, feeling, opportunity, motivation, build, selfconfidence, opportunity, existswhen, c...|\n",
      "|[comfort, zone, ton, bar, club, comfortable, environment, alcohol, noise, crowd, edge, defensive,...|\n",
      "|[first, learn, social, terrible, talk, brand, quiet, head, stupid, terrible, trait, break, happen...|\n",
      "|[facebook, great, share, touch, stupid, joke, amount, amount, social, interaction, funny, joke, w...|\n",
      "|[okay, male, semiasian, stand, shy, American, kg, standard, eat, heaps, college, family, general,...|\n",
      "|[everybody, story, worthwhile, state, last, overweight, loser, house, due, isolation, boom, bust,...|\n",
      "|[adderall, quick, fix, diagnose, adhd, lack, motivation, willpower, selfrespect, plan, medication...|\n",
      "|[sure, place, computer, game, phase, play, free, besides, university, fill, void, boredom, happy,...|\n",
      "|[access, total, success, power, goalfocused, attention, intention, direct, innerpower, manifest, ...|\n",
      "|[inferiority, complex, critical, conduct, fact, great, rightmoral, decision, social, situationsco...|\n",
      "|[horrible, place, bullies, long, bosses, death, threat, sexual, harassment, refuse, promote, revi...|\n",
      "|                                                     [take, everything, stop, guess, realistic, end]|\n",
      "|[context, last, semester, credits, grad, school, play, warcraft, minimal, amount, rest, game, net...|\n",
      "|[strength, background, dance, emotional, spectrum, fear, failure, hardwork, surebut, feeling, fee...|\n",
      "|[show, awesome, scene, true, none, anyone, mental, illness, breath, mint, commercials, sandra, bu...|\n",
      "|[title, whole, kind, parent, great, build, relationship, kind, sick, argument, supportive, girlfr...|\n",
      "|                [computer, build, study, university, class, easy, complex, study, job, field, happy]|\n",
      "|[happy, confident, outgoing, guy, girl, model, reach, challenge, buck, confident, ambitious, suic...|\n",
      "|[overthink, easy, example, fear, loss, tend, carry, shoulders, mood, everything, part, single, fu...|\n",
      "|[rant, independant, guy, chilled, hold, conversation, school, dependant, solid, group, partys, ec...|\n",
      "|[story, short, possilble, sluggish, college, summer, improve, browse, subreddits, input, plan, fa...|\n",
      "|[take, lazy, routine, anyone, spice, open, suggestion, awesome, book, movie, music, website, open...|\n",
      "|[skin, great, light, acne, young, adult, facial, blemishes, perfect, skin, sucker, perfect, skin,...|\n",
      "|[first, sure, place, lottery, couple, parent, divorce, big, deal, college, senior, high, school, ...|\n",
      "|[huge, sure, overcome, sure, enemy, prided, intelligence, identifier, strong, attractive, charmin...|\n",
      "|       [simple, program, block, computer, access, internet, period, gt, gt, internet, access, block]|\n",
      "|[useless, college, degree, office, job, lesson, lesson, cubical, laugh, xx, bleaker, future, guy,...|\n",
      "|[journey, young, compile, list, importance, difficulty, link, important, material, avoid, text, e...|\n",
      "|[unusual, example, cut, lawn, mom, able, cut, cut, decision, favor, big, chore, effortless, restr...|\n",
      "|[story, short, live, offer, job, opportunitiesthe, main, reason, distance, size, town, major, fac...|\n",
      "|[bank, account, credits, account, balance, bank, deletes, part, balance, draw, cent, course, bank...|\n",
      "|[short, patience, snappy, repeat, instruction, type, behavior, common, hurt, example, click, vide...|\n",
      "|[high, school, size, industry, town, strong, interest, math, science, strong, suit, sort, handson...|\n",
      "|[opinion, whiteboard, calendar, planner, stay, future, whiteboard, specific, task, check, box, em...|\n",
      "|[persistence, measure, belief, ability, succeed, brian, tracy, tracy, human, resolutely, spite, d...|\n",
      "|[male, male, role, model, role, model, learn, hunt, fish, trap, shelter, modern, civilization, do...|\n",
      "|[able, interest, happen, evils, possess, interest, interested, instinct, answer, lame, ass, reaso...|\n",
      "|[guy, external, motivation, girlfriend, inevitable, fake, breakups, great, relationship, unhealth...|\n",
      "|[taste, travel, tourist, able, communicate, skill, place, confidence, martial, pay, lizardpoint, ...|\n",
      "|[anyone, carry, brief, conversation, rapport, joke, acquaintances, home, plan, home, able, social...|\n",
      "|[keep, record, friendship, unacceptable, friend, tagalong, friend, everybody, sort, ignores, talk...|\n",
      "|[keep, record, friendship, unacceptable, friend, tagalong, friend, everybody, sort, ignores, talk...|\n",
      "|[guy, first, timefirst, little, strange, harsh, reminder, downfall, crowd, try, prevents, strong,...|\n",
      "|[pardon, correct, procedure, cross, postsi, cross, rselfhelp, response, several, response, previo...|\n",
      "|[strict, schedule, able, consecutive, full, mile, achievement, strength, training, weight, limit,...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.select('finished_final').show(50, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d8812d7-fd39-46ab-889d-ed86ceba27e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'cleaned_text',\n",
       " 'finished_unigrams',\n",
       " 'finished_pos',\n",
       " 'filtered_unigrams_by_pos',\n",
       " 'joined_tokens',\n",
       " 'finished_final']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4313f77f-cb6f-427b-a034-a123f752abc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, cleaned_text: string, finished_unigrams: array<string>, finished_pos: array<string>, filtered_unigrams_by_pos: array<string>, joined_tokens: string, finished_final: array<string>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.unpersist()\n",
    "final_data.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db230604-f5c6-447a-9b01-57976afeab68",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fd16dff-01c4-43ff-ab72-f6824f2d8a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=====>                                                   (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/20 20:20:46 WARN BlockManager: Putting block rdd_30_9 failed due to exception org.apache.spark.SparkException: Failed to execute user defined function (HasSimpleAnnotate$$Lambda$3156/0x0000000801a2eb08: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>).\n",
      "24/11/20 20:20:46 WARN BlockManager: Block rdd_30_9 could not be removed as it was not found on disk or in memory\n",
      "24/11/20 20:20:46 WARN BlockManager: Putting block rdd_63_9 failed due to exception org.apache.spark.SparkException: Failed to execute user defined function (HasSimpleAnnotate$$Lambda$3156/0x0000000801a2eb08: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>).\n",
      "24/11/20 20:20:46 WARN BlockManager: Block rdd_63_9 could not be removed as it was not found on disk or in memory\n",
      "24/11/20 20:20:46 ERROR Executor: Exception in task 9.0 in stage 13.0 (TID 115)\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (HasSimpleAnnotate$$Lambda$3156/0x0000000801a2eb08: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.ObjectInputStream$HandleTable.grow(ObjectInputStream.java:4119)\n",
      "\tat java.base/java.io.ObjectInputStream$HandleTable.assign(ObjectInputStream.java:3925)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2220)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1712)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:519)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:477)\n",
      "\tat scala.collection.immutable.HashMap$SerializationProxy.$anonfun$readObject$1(HashMap.scala:1048)\n",
      "\tat scala.collection.immutable.HashMap$SerializationProxy.readObject(HashMap.scala:1046)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1226)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2401)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2235)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1712)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2540)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2434)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2235)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1712)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:519)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:477)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1669)\n",
      "\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:958)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:236)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$$Lambda$2699/0x00000008018fe110.apply(Unknown Source)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "24/11/20 20:20:46 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 9.0 in stage 13.0 (TID 115),5,main]\n",
      "org.apache.spark.SparkException: Failed to execute user defined function (HasSimpleAnnotate$$Lambda$3156/0x0000000801a2eb08: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.ObjectInputStream$HandleTable.grow(ObjectInputStream.java:4119)\n",
      "\tat java.base/java.io.ObjectInputStream$HandleTable.assign(ObjectInputStream.java:3925)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2220)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1712)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:519)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:477)\n",
      "\tat scala.collection.immutable.HashMap$SerializationProxy.$anonfun$readObject$1(HashMap.scala:1048)\n",
      "\tat scala.collection.immutable.HashMap$SerializationProxy.readObject(HashMap.scala:1046)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1226)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2401)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2235)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1712)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2540)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2434)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2235)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1712)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:519)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:477)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1669)\n",
      "\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:958)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:236)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$$Lambda$2699/0x00000008018fe110.apply(Unknown Source)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "24/11/20 20:20:46 WARN TaskSetManager: Lost task 9.0 in stage 13.0 (TID 115) (midway3-0136.rcc.local executor driver): org.apache.spark.SparkException: Failed to execute user defined function (HasSimpleAnnotate$$Lambda$3156/0x0000000801a2eb08: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:190)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:87)\n",
      "\tat org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.next(InMemoryRelation.scala:79)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:378)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1523)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1450)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1514)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1337)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.io.ObjectInputStream$HandleTable.grow(ObjectInputStream.java:4119)\n",
      "\tat java.base/java.io.ObjectInputStream$HandleTable.assign(ObjectInputStream.java:3925)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2220)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1712)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:519)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:477)\n",
      "\tat scala.collection.immutable.HashMap$SerializationProxy.$anonfun$readObject$1(HashMap.scala:1048)\n",
      "\tat scala.collection.immutable.HashMap$SerializationProxy.readObject(HashMap.scala:1046)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1226)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2401)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2235)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1712)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2540)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2434)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2235)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1712)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:519)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:477)\n",
      "\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:87)\n",
      "\tat org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.maybeCacheDiskValuesInMemory(BlockManager.scala:1669)\n",
      "\tat org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:958)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:236)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$$Lambda$2699/0x00000008018fe110.apply(Unknown Source)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\n",
      "24/11/20 20:20:46 ERROR TaskSetManager: Task 9 in stage 13.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 47850)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/software/spark-3.3.2-el8-x86_64/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/software/spark-3.3.2-el8-x86_64/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/software/spark-3.3.2-el8-x86_64/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/software/spark-3.3.2-el8-x86_64/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/software/spark-3.3.2-el8-x86_64/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/software/spark-3.3.2-el8-x86_64/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/software/spark-3.3.2-el8-x86_64/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/software/python-anaconda-2022.05-el8-x86_64/lib/python3.9/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/software/python-anaconda-2022.05-el8-x86_64/lib/python3.9/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/software/python-anaconda-2022.05-el8-x86_64/lib/python3.9/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/software/python-anaconda-2022.05-el8-x86_64/lib/python3.9/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/software/spark-3.3.2-el8-x86_64/python/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/software/spark-3.3.2-el8-x86_64/python/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/software/spark-3.3.2-el8-x86_64/python/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/software/spark-3.3.2-el8-x86_64/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "py4j does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Apply TF-IDF filtering\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tfizer \u001b[38;5;241m=\u001b[39m CountVectorizer(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinished_final\u001b[39m\u001b[38;5;124m'\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_features\u001b[39m\u001b[38;5;124m'\u001b[39m, minDF\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, maxDF\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.80\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m tf_model \u001b[38;5;241m=\u001b[39m \u001b[43mtfizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m tf_result \u001b[38;5;241m=\u001b[39m tf_model\u001b[38;5;241m.\u001b[39mtransform(final_data)\n\u001b[1;32m      6\u001b[0m idfizer \u001b[38;5;241m=\u001b[39m IDF(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_features\u001b[39m\u001b[38;5;124m'\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_idf_features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/utils.py:192\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 192\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/utils.py:184\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    178\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnknownException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstackTrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacktrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/utils.py:58\u001b[0m, in \u001b[0;36mCapturedException.__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackTrace \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     54\u001b[0m     stackTrace\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (SparkContext\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(origin))\n\u001b[1;32m     57\u001b[0m )\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin\u001b[38;5;241m.\u001b[39mgetCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m convert_exception(origin\u001b[38;5;241m.\u001b[39mgetCause())\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/utils.py:153\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    150\u001b[0m jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_jvm\n\u001b[1;32m    151\u001b[0m gw \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_instance_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.sql.catalyst.parser.ParseException\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ParseException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Order matters. ParseException inherits AnalysisException.\u001b[39;00m\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy4j\u001b[49m\u001b[38;5;241m.\u001b[39mreflection\u001b[38;5;241m.\u001b[39mTypeUtil\u001b[38;5;241m.\u001b[39misInstanceOf(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1722\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1719\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1720\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: py4j does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "#Apply TF-IDF filtering\n",
    "tfizer = CountVectorizer(inputCol='finished_final', outputCol='tf_features', minDF=0.01, maxDF=0.80)\n",
    "tf_model = tfizer.fit(final_data)\n",
    "tf_result = tf_model.transform(final_data)\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21437e2e-eb3e-4138-96c0-e14c164ea335",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ace0a6-7188-4040-bd25-311942d5c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.unpersist()\n",
    "tfidf_result.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84caf9a8-3db1-47b0-867f-2b53bc791b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=15, maxIter=50, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', seed=2503)\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9fae7f-a3cf-41c3-98b6-f9fecb709a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4214784-d38e-49f4-ac19-59b0c40d4969",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_words = 15\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18b857-5d49-4da2-af3b-07e641c49775",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.logPerplexity(tfidf_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186833c-1d1c-473d-b404-cdc241b2590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.logLikelihood(tfidf_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
