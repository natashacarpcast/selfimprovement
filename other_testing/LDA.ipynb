{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b6aa00-eeb1-46d2-9970-0cc7142d605d",
   "metadata": {},
   "source": [
    "Used tutorial https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP/blob/master/Topic_Modelling_with_PySpark_and_Spark_NLP.ipynb for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80dfe811-6044-456b-9b67-1dd90f647207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "24/11/10 16:42:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import Tokenizer, PerceptronModel\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18ccc18-b119-472f-af9c-19a44eeea279",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data = spark.read.csv(\"../data_topicmodel.csv\", header= True).select([\"id\", \"cleaned_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e156a53b-e98c-488a-bd55-4d9028bad129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   id|        cleaned_text|\n",
      "+-----+--------------------+\n",
      "|hk5r2|i had an appointm...|\n",
      "|iqimz|i created this si...|\n",
      "|pfzt5|hello everyone  i...|\n",
      "|pk714|i grew up with bo...|\n",
      "|q0q8x|i have to ask whe...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Remove sample\n",
    "og_data.show(5)\n",
    "sample_data = og_data.sample(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d856c9-673a-4d0b-a03f-732af6820a65",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7911044e-82b5-4034-a43b-16e1306cc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "     .setInputCol(\"cleaned_text\")\\\n",
    "     .setOutputCol('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b99eb0-aaee-49e6-accd-8382b9778873",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols(['document'])\\\n",
    "            .setOutputCol('tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e08506b3-9c85-4b12-915c-179501a1fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7785596-e18b-44b5-a57a-602da0c6c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \n",
    "    \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"did\", \n",
    "    \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n",
    "    \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "    \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"must\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \n",
    "    \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"some\", \"such\", \n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"will\", \"ll\", \n",
    "    \"re\", \"ve\", \"d\", \"s\", \"m\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "    \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"many\", \"us\", \"ok\", \"hows\", \"ive\", \"ill\", \"im\", \"cant\", \"topics\", \"topic\",\n",
    "    \"discuss\", \"thoughts\", \"yo\", \"thats\", \"whats\", \"lets\", \"nothing\", \"oh\", \"omg\", \n",
    "         \"things\", \"stuff\", \"yall\", \"haha\", \"yes\", \"no\", \"wo\", \"like\", 'good', \n",
    "         'work', 'got', 'going', 'dont', 'really', 'want', 'make', 'think', \n",
    "         'know', 'feel', 'people', 'life', \"getting\", \"lot\" \"great\", \"i\", \"me\", \n",
    "         \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "        \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "        \"they\", \"them\", \"their\", \"theirs\",\"themselves\", \"what\", \"which\", \"who\", \n",
    "        \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "        \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\",\n",
    "        \"might\", \"must\", \"shall\", \"ought\", \"about\", \"above\", \"across\", \"after\", \n",
    "        \"against\", \"along\", \"amid\", \"among\", \"around\", \"as\", \"at\", \"before\", \"behind\",\n",
    "        \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\", \"by\", \n",
    "        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\",\n",
    "        \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"notwithstanding\",\n",
    "        \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\",\n",
    "        \"regarding\", \"round\", \"since\", \"than\", \"through\", \"throughout\", \"till\", \n",
    "        \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\",\n",
    "        \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\", \"cant\", \"cannot\", \n",
    "        \"couldve\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \n",
    "        \"havent\", \"hed\", \"hell\", \"hes\", \"howd\", \"howll\", \"hows\", \"id\", \"ill\", \n",
    "        \"im\", \"ive\", \"isnt\", \"itd\", \"itll\", \"its\", \"lets\", \"mightve\", \"mustve\", \n",
    "        \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldve\", \"shouldnt\", \n",
    "        \"thatll\", \"thats\", \"thered\", \"therell\", \"therere\", \"theres\", \"theyd\", \n",
    "        \"theyll\", \"theyre\", \"theyve\", \"wed\", \"well\", \"were\", \"weve\", \"werent\", \n",
    "        \"whatd\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"whend\", \"whenll\", \n",
    "        \"whens\", \"whered\", \"wherell\", \"wheres\", \"whichd\", \"whichll\", \"whichre\", \n",
    "        \"whichs\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\", \"whyd\", \"whyll\", \n",
    "        \"whys\", \"wont\", \"wouldve\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "        \"f\", \"m\", \"because\", \"go\", \"lot\", \"get\", \"still\", \"way\", \"something\", \"much\",\n",
    "        \"thing\", \"someone\", \"person\", \"anything\", \"goes\", \"ok\", \"so\", \"just\", \"mostly\", \n",
    "        \"put\", \"also\", \"lots\", \"yet\"]\n",
    "\n",
    "time = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \n",
    "        \"sunday\", \"morning\", \"noon\", \"afternoon\", \"evening\", \"night\", \"midnight\",\n",
    "        \"dawn\", \"dusk\", \"week\", \"weekend\", \"weekends\",\"weekly\", \"today\", \n",
    "        \"yesterday\", \"tomorrow\", \"yesterdays\", \"todays\", \"mondays\", \"tuesdays\",\n",
    "        \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\", \"day\",\n",
    "        \"everyday\", \"daily\", \"workday\", 'time', 'month', 'year', 'pm', 'am', \"ago\",\n",
    "        \"year\"]\n",
    "\n",
    "reddit = [\"welcome\", \"hi\", \"hello\", \"sub\", \"reddit\", \"thanks\", \"thank\", \"maybe\",\n",
    "          \"wo30\", \"mods\", \"mod\", \"moderators\", \"subreddit\", \"btw\", \"aw\", \"aww\", \n",
    "          \"aww\", \"hey\", \"hello\", \"join\", \"joined\", \"post\", \"rselfimprovement\"]\n",
    "\n",
    "topic_specific = [\"self\", \"improvement\", \"change\", \"action\",\n",
    "    'change', 'start', 'goal', 'habit', 'new', 'old', \n",
    "    'care', 'world', 'everyone', 'love', 'u', 'right', 'mean', 'matter',\n",
    "    'best', 'step', 'focus', 'hard', 'small',\n",
    "    'bad', 'help', 'time', 'problem', 'issue', 'advice',\n",
    "    'bit', 'experience', 'different',\n",
    "    'point', 'situation', 'negative', 'control', 'positive',\n",
    "    'use', 'question', 'idea', 'amp', 'medium', 'hour', 'day', 'minute',\n",
    "    'aaaaloot']\n",
    "\n",
    "stopwords = english + time + reddit + topic_specific\n",
    "\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e33a63-8c34-4a81-8fa1-157c86492364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.annotator import NGramGenerator\n",
    "\n",
    "ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5843616-767e-448f-8943-006848263e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pos = PerceptronModel.load(\"/project/macs40123/spark-jars/pos_anc_en_3.0.0_3.0_1614962126490/\")\\\n",
    "      .setInputCols(\"document\", \"normalized\")\\\n",
    "      .setOutputCol(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20199976-1358-47e2-8685-a8e54cc73200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher\n",
    "\n",
    "finisher = Finisher().setInputCols(['unigrams', 'ngrams', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50a9dbb2-1f65-4c5b-826e-feedefbf682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          tokenizer,\n",
    "          normalizer,\n",
    "          stopwords_cleaner,\n",
    "          ngrammer,\n",
    "          pos,\n",
    "          finisher\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "680ea3cb-e5cf-4de6-aae3-c8e4ac8a3258",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/software/spark-3.3.2-el8-x86_64/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    id|        cleaned_text|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|25fn2l|the biggest obsta...|[biggest, obstacl...|[the, biggest, ob...|[DT, JJS, NN, IN,...|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = my_pipeline.fit(sample_data)\n",
    "processed_data = pipelineModel.transform(sample_data)\n",
    "processed_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "198344d1-7dcc-4258-b5a1-4bba2466aac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'cleaned_text', 'finished_unigrams', 'finished_ngrams', 'finished_pos']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87fb8dd9-18a7-453a-969a-6000f42d6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.select('document').show(5, truncate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3772846d-a5b0-4164-a7f3-adb696119d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.select('tokenized').show(1, truncate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "393e8e4f-fbdc-4537-88d9-2e928217322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.select('normalized').show(1, truncate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53c04957-b85f-4f0b-8d02-6667a4e45cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.select('lemmatized').show(1, truncate = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7598af2e-4e59-4115-877d-95bf5fe8552e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                   finished_unigrams|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[biggest, obstacle, selfimprovement, undeniably, computer, spend, literally, free, using, hobbies...|\n",
      "|[guys, semester, coming, close, worried, fall, routine, sleeping, working, break, comes, hate, be...|\n",
      "|[become, akin, mindstate, mindstate, become, unable, even, acknowledge, ability, improve, related...|\n",
      "|[always, cynical, perspective, humanity, perhaps, feeling, lately, anxiety, convincing, somehow, ...|\n",
      "|[male, suffered, depression, years, now, saying, never, turned, easy, always, ended, deal, proble...|\n",
      "|[title, kind, says, slightly, emotional, logical, problems, heart, tends, example, taking, even, ...|\n",
      "|[shake, angerhurtfrustration, letting, former, friend, years, manipulate, blame, misery, list, sa...|\n",
      "|[always, shit, social, interactions, gangs, brings, times, brother, even, gives, shit, lack, sex,...|\n",
      "|[pretty, talking, hispanics, race, generally, quiet, guy, pretty, talking, knw, issues, else, hea...|\n",
      "|[real, pulling, away, pc, find, staying, early, watching, pointless, youtube, videos, ruins, prod...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_data.select('finished_unigrams').show(10, truncate = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a3ca69-28c3-4747-86d5-ff4e3ab15af8",
   "metadata": {},
   "source": [
    "Create pos-tags n-grams that correspond to words n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14d85977-59fb-4c78-913c-4a5aa465389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge POS tags as just one string to be able to take it as a document in the Spark NLP Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "pos_as_string = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_data = processed_data.withColumn('finished_pos', pos_as_string(F.col('finished_pos')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c9dbe0a-f678-4a30-b0be-52677577fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    id|        cleaned_text|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|25fn2l|the biggest obsta...|[biggest, obstacl...|[the, biggest, ob...|DT JJS NN IN PRP$...|\n",
      "|2m2wpm|hey guys this sem...|[guys, semester, ...|[hey, guys, this,...|UH NNS DT NN VBZ ...|\n",
      "|32etc8|some become akin ...|[become, akin, mi...|[some, become, ak...|DT VBP NN TO DT N...|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2526f892-a934-4101-a8f1-044479b473fd",
   "metadata": {},
   "source": [
    "New pipeline for pos-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e13efe3f-256a-4261-93c2-d19b2c735785",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('finished_pos') \\\n",
    "     .setOutputCol('pos_document')\n",
    "\n",
    "pos_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['pos_document']) \\\n",
    "     .setOutputCol('pos')\n",
    "     \n",
    "    \n",
    "pos_ngrammer = NGramGenerator() \\\n",
    "    .setInputCols(['pos']) \\\n",
    "    .setOutputCol('pos_ngrams') \\\n",
    "    .setN(3) \\\n",
    "    .setEnableCumulative(True) \\\n",
    "    .setDelimiter('_')\n",
    "\n",
    "pos_finisher = Finisher() \\\n",
    "     .setInputCols(['pos', 'pos_ngrams']) \\\n",
    "\n",
    "pos_pipeline = Pipeline() \\\n",
    "     .setStages([pos_documentAssembler,                  \n",
    "                 pos_tokenizer,\n",
    "                 pos_ngrammer,  \n",
    "                 pos_finisher])\n",
    "\n",
    "processed_data = pos_pipeline.fit(processed_data).transform(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b5f3238-a48b-4632-bb35-c5c796163816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'cleaned_text',\n",
       " 'finished_unigrams',\n",
       " 'finished_ngrams',\n",
       " 'finished_pos',\n",
       " 'finished_pos_ngrams']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dcb5892-a1a6-486a-9845-faa5f8b3780a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    id|        cleaned_text|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|25fn2l|the biggest obsta...|[biggest, obstacl...|[the, biggest, ob...|[DT, JJS, NN, IN,...|[DT, JJS, NN, IN,...|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a88ada38-b7f7-43d9-a593-93b7ed787514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unigrams(finished_unigrams, finished_pos):\n",
    "    '''Filters individual words based on their POS tag'''\n",
    "    return [word for word, pos in zip(finished_unigrams, finished_pos)\n",
    "            if pos in ['JJ', 'NN', 'NNS', 'NNPS']]\n",
    "\n",
    "udf_filter_unigrams = F.udf(filter_unigrams, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4ebd6c8-ba26-47bf-9db8-f761f1a0e921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------------+\n",
      "|    id|        cleaned_text|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|filtered_unigrams_by_pos|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------------+\n",
      "|25fn2l|the biggest obsta...|[biggest, obstacl...|[the, biggest, ob...|[DT, JJS, NN, IN,...|[DT, JJS, NN, IN,...|    [selfimprovement,...|\n",
      "|2m2wpm|hey guys this sem...|[guys, semester, ...|[hey, guys, this,...|[UH, NNS, DT, NN,...|[UH, NNS, DT, NN,...|    [semester, close,...|\n",
      "|32etc8|some become akin ...|[become, akin, mi...|[some, become, ak...|[DT, VBP, NN, TO,...|[DT, VBP, NN, TO,...|    [mindstate, unabl...|\n",
      "|3tcov4|well ive always h...|[always, cynical,...|[well, ive, alway...|[RB, JJ, RB, VBD,...|[RB, JJ, RB, VBD,...|    [cynical, feeling...|\n",
      "|4rjrkm|im a 20 year old ...|[male, suffered, ...|[im, a, year, old...|[NN, DT, NN, JJ, ...|[NN, DT, NN, JJ, ...|    [male, depression...|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_data = processed_data.withColumn('filtered_unigrams_by_pos', udf_filter_unigrams(\n",
    "                                                   F.col('finished_unigrams'),\n",
    "                                                   F.col('finished_pos')))\n",
    "\n",
    "processed_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1327c68b-1ba7-432f-96a5-e83a4ce36933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos_ngrams(finished_ngrams, finished_pos_tags):\n",
    "    return [word for word, pos in zip(finished_ngrams, finished_pos_tags) \n",
    "            if (len(pos.split('_')) == 2 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
    "            or (len(pos.split('_')) == 3 and \\\n",
    "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
    "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
    "    \n",
    "udf_filter_pos_ngrams = F.udf(filter_pos_ngrams, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32c8fd55-67eb-420a-b66e-0be30b932898",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = processed_data.withColumn('filtered_ngrams_by_pos',\n",
    "                       udf_filter_pos_ngrams(F.col('finished_ngrams'),\n",
    "                                             F.col('finished_pos_ngrams')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cee6080-5f1b-43b9-a7c3-d1f8573c8f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------------+----------------------+\n",
      "|    id|        cleaned_text|   finished_unigrams|     finished_ngrams|        finished_pos| finished_pos_ngrams|filtered_unigrams_by_pos|filtered_ngrams_by_pos|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------------+----------------------+\n",
      "|25fn2l|the biggest obsta...|[biggest, obstacl...|[the, biggest, ob...|[DT, JJS, NN, IN,...|[DT, JJS, NN, IN,...|    [selfimprovement,...|  [free_time, tv_se...|\n",
      "|2m2wpm|hey guys this sem...|[guys, semester, ...|[hey, guys, this,...|[UH, NNS, DT, NN,...|[UH, NNS, DT, NN,...|    [semester, close,...|  [ill_fall, become...|\n",
      "|32etc8|some become akin ...|[become, akin, mi...|[some, become, ak...|[DT, VBP, NN, TO,...|[DT, VBP, NN, TO,...|    [mindstate, unabl...|  [become_akin, bec...|\n",
      "|3tcov4|well ive always h...|[always, cynical,...|[well, ive, alway...|[RB, JJ, RB, VBD,...|[RB, JJ, RB, VBD,...|    [cynical, feeling...|  [cynical_perspect...|\n",
      "|4rjrkm|im a 20 year old ...|[male, suffered, ...|[im, a, year, old...|[NN, DT, NN, JJ, ...|[NN, DT, NN, JJ, ...|    [male, depression...|  [year_old, old_ma...|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0479f4ba-a6a8-4278-a1d7-0d212602da71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "|    id|                                                                              filtered_ngrams_by_pos|\n",
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "|25fn2l|[free_time, tv_series, complete_change, entire_lifestyle, lifestyle_ive, internet_hobby, hobby_th...|\n",
      "|2m2wpm|             [ill_fall, become_stagnant, stay_productive, productive_thanks, stay_productive_thanks]|\n",
      "|32etc8|[become_akin, become_unable, own_self, self_ability, chakra_zen, zen_state, be_wrong, please_dire...|\n",
      "|3tcov4|[cynical_perspective, theres_nothing, nothing_new, last_few, few_months, general_thats, have_frie...|\n",
      "|4rjrkm|[year_old, old_male, male_ive, past_years, happy_kid, kid_yrs, yrs_old, naive_soul, high_school, ...|\n",
      "|8it6z4|                                             [title_kind, im_someone, someone_whos, im_someone_whos]|\n",
      "|8sbqyi|[cant_shake, former_friend, years_manipulate, cant_list, things_shes, bad_memories, head_im, ther...|\n",
      "|95mojy|[get_shit, social_interactions, interactions_everyone, everyone_gangs, things_didnt, didnt_work, ...|\n",
      "|9rzk74|[other_hispanics, own_race, race_im, quiet_guy, person_im, start_conversations, dont_approach, ap...|\n",
      "|9v3adf|[real_problem, early_morning, pointless_youtube, youtube_videos, next_day, good_piece, certain_ti...|\n",
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_data.select('id','filtered_ngrams_by_pos').show(10, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7690a15-09ef-4158-b492-9bd5c5e8eaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "|    id|                                                                            filtered_unigrams_by_pos|\n",
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "|25fn2l|[selfimprovement, spend, free, soccer, tv, series, literally, giving, tried, hobbies, replace, in...|\n",
      "|2m2wpm|                                            [semester, close, working, comes, stagnant, break, stay]|\n",
      "|32etc8|                                            [mindstate, unable, related, akin, state, wrong, please]|\n",
      "|3tcov4|[cynical, feeling, convincing, somehow, lately, slowly, developed, mild, say, friends, random, de...|\n",
      "|4rjrkm|[male, depression, years, now, saying, easy, deal, problems, friends, started, never, friendsi, m...|\n",
      "|8it6z4|              [kind, says, heart, tends, example, anymore, memories, telling, slowly, slowly, heart]|\n",
      "|8sbqyi|[angerhurtfrustration, letting, years, crazy, illogical, memories, whirring, seeing, little, taki...|\n",
      "|95mojy|[interactions, times, brother, even, gives, always, shit, college, job, shit, never, jokes, deny,...|\n",
      "|9rzk74|      [pretty, hispanics, guy, pretty, knw, issues, else, often, speak, speak, rarely, girls, never]|\n",
      "|9v3adf|                                  [pc, find, ruins, neglect, reminds, turn, even, forces, pc, ticks]|\n",
      "+------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_data.select('id','filtered_unigrams_by_pos').show(10, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "817d0b5a-ff0d-4f00-bfb7-2fc8a993e388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'cleaned_text',\n",
       " 'finished_unigrams',\n",
       " 'finished_ngrams',\n",
       " 'finished_pos',\n",
       " 'finished_pos_ngrams',\n",
       " 'filtered_unigrams_by_pos',\n",
       " 'filtered_ngrams_by_pos']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "241ee1e3-18af-4c54-95d9-d717e5cde253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that POS was done, lemmatization makes more sense at this point\n",
    "\n",
    "#Merge tokens as just one string to be able to take it as a document in the new Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "tokens_as_string = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_data = processed_data.withColumn('joined_tokens', tokens_as_string(F.col('filtered_unigrams_by_pos')))\n",
    "\n",
    "last_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('joined_tokens') \\\n",
    "     .setOutputCol('joined_document')\n",
    "\n",
    "last_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['joined_document']) \\\n",
    "     .setOutputCol('tokenized')\n",
    "     \n",
    "lemmatizer = LemmatizerModel.load(\"../models/lemma_ewt_en_3.4.3_3.0_1651416655397/\")\\\n",
    "      .setInputCols(\"tokenized\")\\\n",
    "      .setOutputCol(\"lemmatized\")\n",
    "\n",
    "#Delete these tokens that remained from the lemmatizer model and topic's n grams\n",
    "last_stopwords = [\"_\", \"self_improvement\"]\n",
    "\n",
    "last_stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('cleaned_unigrams') \\\n",
    "     .setStopWords(last_stopwords)\n",
    "\n",
    "last_finisher = Finisher() \\\n",
    "     .setInputCols(['cleaned_unigrams']) \\\n",
    "\n",
    "last_pipeline = Pipeline() \\\n",
    "     .setStages([last_documentAssembler,                  \n",
    "                 last_tokenizer,\n",
    "                 lemmatizer,\n",
    "                 last_stopwords_cleaner,\n",
    "                 last_finisher])\n",
    "\n",
    "final_data = last_pipeline.fit(processed_data).transform(processed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d9d0f1-ee07-4f52-829d-f1f01c44d96e",
   "metadata": {},
   "source": [
    "Create one column merging unigrams and ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a1a0d7a-f91f-4ba5-89d4-68a5f2aaf873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "final_data = final_data.withColumn('final', concat(F.col('finished_cleaned_unigrams'), \\\n",
    "                                                   F.col('filtered_ngrams_by_pos')))\\\n",
    "                                                   .select('id','cleaned_text','final')\n",
    "                                                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7deb738d-484f-45f8-b6a4-12430b06111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                               final|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[selfimprovement, spend, free, soccer, tv, series, literally, give, try, hobbies, replace, intern...|\n",
      "|[semester, close, working, come, stagnant, break, stay, ill_fall, become_stagnant, stay_productiv...|\n",
      "|[mindstate, unable, related, akin, state, wrong, please, become_akin, become_unable, own_self, se...|\n",
      "|[cynical, feeling, convince, somehow, lately, slowly, develop, mild, say, random, develop, experi...|\n",
      "|[male, depression, year, now, saying, easy, deal, problem, start, never, friendsi, mind, loud, en...|\n",
      "|[kind, say, heart, tend, example, anymore, memory, tell, slowly, slowly, heart, title_kind, im_so...|\n",
      "|[angerhurtfrustration, let, year, crazy, illogical, memory, whirring, see, little, take, long, ca...|\n",
      "|[interactions, time, brother, even, give, always, shit, college, job, shit, never, joke, deny, ta...|\n",
      "|[pretty, hispanics, guy, pretty, knw, issue, else, often, speak, speak, rarely, girl, never, othe...|\n",
      "|[pc, find, ruin, neglect, remind, turn, even, force, pc, tick, real_problem, early_morning, point...|\n",
      "|[day, month, detail, constantly, weigh, low, selfesteem, confidence, now, deal, moment, selfloath...|\n",
      "|   [strong, belief, selfflagellation, hand, difficulty, hi_everyone, strong_belief, have_difficulty]|\n",
      "|[hardships, feeling, emotion, annoyance, regret, guilt, happen, worried, saying, never, insightfu...|\n",
      "|[little, background, see, girl, zoo, cinema, hand, hold, tell, start, now, pit, despair, become, ...|\n",
      "|[grade, peers, study, making, study, everytime, away, try, weird, skill, make, improve, be_brilli...|\n",
      "|[working, research, project, please, share, follow, mental, big, challenge, university_student, d...|\n",
      "|[use, full, hour, late, game, weight, make, slow, full_time, time_job, work_full, full_time, work...|\n",
      "|[first, sorry, long, need, guy, month, try, become, version, need, body, thingd, tidying, somethi...|\n",
      "|[hear, parent, toxic, relationship, toxic, attempt, take, etc, span, made, mother, abusive, start...|\n",
      "|[lurking, feeling, last, first, job, approaching, main, interest, always, school, go, however, al...|\n",
      "|[ye, scared, now, watch, horror, paranoia, think, fear, darkness, ye_im, childhood_im, get_rid, w...|\n",
      "|                                   [find, now, either, place, chore, current_situation, rest_chores]|\n",
      "|[stick, school, make, look, pay, minimum, indeed, college, resume, idea, appreciate, grocery_stor...|\n",
      "|                              [meet, stupid, seem, word, other_people, stupid_excuses, words_thanks]|\n",
      "|[due, need, apply, mundane, early, obviously, deal, load, unable, lost, meditation, stop, start, ...|\n",
      "|[planning, month, time, jogging, time, stay, start, list, almost, particularly, goal, improve, lo...|\n",
      "|[set, readers, seem, almost, always, write, goal, certain, amount, goal, do, require, read, affir...|\n",
      "|                                                            [either, support, cant_login, feel_good]|\n",
      "|[everything, minor, surroundings, notice, cue, remind, reminder, big, youtube, even, youtube, les...|\n",
      "|[action, evolution, closelyknit, family, often, complication, start, health, moreover, contribute...|\n",
      "|[big, now, relationship, whenever, arises, tell, way, sometimes, realize, recognizing, let_fear, ...|\n",
      "|[sense, face, owl, stamina, move, certs, exercise, turn, tv, watches, tv, year, productive, snooz...|\n",
      "|[include, easily, unnecessary, video, already, deadline, say, flaws, left, depression, even, flaw...|\n",
      "|[anymore, okay, small, hate, shifting, family, accept, extremely, regret, worthy, enough, family,...|\n",
      "|[gym, working, passion, back, college, job, month, covid, notice, reason, honestly, want, walk, l...|\n",
      "|[made, challenge, comfort, good, version, cause, win, winner, example, flip, group, winner, up, c...|\n",
      "|[answer, dilemma, try, specific, goal, aim, weaknesses, exactly, good, actually, invest, become, ...|\n",
      "|[sociology, video, come, give, read, excuse, break, cause, emotion, problem, own_advice, knowledg...|\n",
      "|[military, college, blame, low, fixations, none, matter, waste, door, society, one, care, uturn, ...|\n",
      "|[remember, bein, thinkin, drive, kind, one, mind, gettin, wayout, example, option, fear, problem,...|\n",
      "|[able, end, idea, say, working, one, overthinking, able, way, example, meditating, find, life_ill...|\n",
      "|[willing, read, uncertainty, now, year, avoid, success, good, avoid, stupid, graduate, succeed, c...|\n",
      "|[school, soul, alone, sit, alone, talk, guy, never, high_schools, soul_im, science_labs, labs_im,...|\n",
      "|[stress, one, tend, irrational, peace, subconsciously, rocky_childhood, find_peace, irrational_th...|\n",
      "|[left, try, day, productivity, goal, media, term, productivity_work, work_gym, gym_side, side_hus...|\n",
      "|[cricket, associated, deep, bring, chirp, nocturnal, relax, deep, sleep, nature, youtube, start_p...|\n",
      "|[basically, shoein, recommended, college, teach, graduate, one, teacher, recommendation, want, ac...|\n",
      "|[basically, rant, motivation, basically, probably, shit, stick, route, year, saying, good, move, ...|\n",
      "|[last, college, start, college, Greek, hit, kind, withdraw, focus, although, settling, social, an...|\n",
      "|[glass, forget, coca, taste, sometimes, show, low, blood, sugar, juice, light, workout, differenc...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_data.select('final').show(50, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d8812d7-fd39-46ab-889d-ed86ceba27e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'cleaned_text', 'final']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db230604-f5c6-447a-9b01-57976afeab68",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fd16dff-01c4-43ff-ab72-f6824f2d8a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Apply TF-IDF filtering\n",
    "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features', minDF=0.01, maxDF=0.70)\n",
    "tf_model = tfizer.fit(final_data)\n",
    "tf_result = tf_model.transform(final_data)\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21437e2e-eb3e-4138-96c0-e14c164ea335",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84caf9a8-3db1-47b0-867f-2b53bc791b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/10 16:43:27 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/11/10 16:43:27 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LocalLDAModel: uid=LDA_a87aaf592677, k=10, numFeatures=351"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 10\n",
    "max_iter = 100\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features', seed=2503)\n",
    "lda_model = lda.fit(tfidf_result)\n",
    "lda_model.setSeed(2503)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c9fae7f-a3cf-41c3-98b6-f9fecb709a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4214784-d38e-49f4-ac19-59b0c40d4969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                topicWords|\n",
      "+-----+------------------------------------------------------------------------------------------+\n",
      "|    0|[other_people, tell, exercise, skill, now, few_months, seem, little, deep, see, always,...|\n",
      "|    1|[social_media, sleep, show, college, enjoy, video, social, sure, check, reason, cause, ...|\n",
      "|    2|[long, great, book, day, need, read, create, try, whatever, honest, hope, truth, sorry,...|\n",
      "|    3|[sometimes, come, making, alone, though, instead, struggle, talk, be_honest, give, scho...|\n",
      "|    4|[live, part, other, habit, choice, healthy, never, good, find, opinion, feeling, spend,...|\n",
      "|    5|[job, one, mind, eat, ever, definitely, avoid, issue, easy, take, mental_health, anymor...|\n",
      "|    6|[video_games, self_improvement, happen, back, turn, okay, plan, eventually, hour, game,...|\n",
      "|    7|[leave, answer, anxiety, different_things, much_time, fuck, study, force, good_luck, ho...|\n",
      "|    8|[etc, else, take, first, woman, say, try, take_care, future, need, even, term, saying, ...|\n",
      "|    9|[drink, make_sure, call, almost, few_years, first_step, goal, side, progress, love, con...|\n",
      "+-----+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 15\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e34f3-6f1f-45db-a828-4e7453032713",
   "metadata": {},
   "source": [
    "# REMEMBER TO CHANGE DATASET TO COMPLETE ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18b857-5d49-4da2-af3b-07e641c49775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
