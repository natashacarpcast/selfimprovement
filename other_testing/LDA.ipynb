{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b6aa00-eeb1-46d2-9970-0cc7142d605d",
   "metadata": {},
   "source": [
    "Used tutorial https://github.com/maobedkova/TopicModelling_PySpark_SparkNLP/blob/master/Topic_Modelling_with_PySpark_and_Spark_NLP.ipynb for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80dfe811-6044-456b-9b67-1dd90f647207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "24/11/18 20:48:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import Tokenizer, PerceptronModel\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c18ccc18-b119-472f-af9c-19a44eeea279",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data = spark.read.csv(\"../data_topicmodel.csv\", header= True).select([\"id\", \"cleaned_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e156a53b-e98c-488a-bd55-4d9028bad129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|   id|        cleaned_text|\n",
      "+-----+--------------------+\n",
      "|hk5r2|i had an appointm...|\n",
      "|iqimz|i created this si...|\n",
      "|pfzt5|hello everyone  i...|\n",
      "|pk714|i grew up with bo...|\n",
      "|q0q8x|i have to ask whe...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Remove sample\n",
    "og_data.show(5)\n",
    "sample_data = og_data.sample(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d856c9-673a-4d0b-a03f-732af6820a65",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7911044e-82b5-4034-a43b-16e1306cc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "     .setInputCol(\"cleaned_text\")\\\n",
    "     .setOutputCol('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12b99eb0-aaee-49e6-accd-8382b9778873",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols(['document'])\\\n",
    "            .setOutputCol('tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e08506b3-9c85-4b12-915c-179501a1fe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7785596-e18b-44b5-a57a-602da0c6c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \n",
    "    \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"did\", \n",
    "    \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n",
    "    \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "    \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"must\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \n",
    "    \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"some\", \"such\", \n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"will\", \"ll\", \n",
    "    \"re\", \"ve\", \"d\", \"s\", \"m\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "    \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"many\", \"us\", \"ok\", \"hows\", \"ive\", \"ill\", \"im\", \"cant\", \"topics\", \"topic\",\n",
    "    \"discuss\", \"thoughts\", \"yo\", \"thats\", \"whats\", \"lets\", \"nothing\", \"oh\", \"omg\", \n",
    "         \"things\", \"stuff\", \"yall\", \"haha\", \"yes\", \"no\", \"wo\", \"like\", 'good', \n",
    "         'work', 'got', 'going', 'dont', 'really', 'want', 'make', 'think', \n",
    "         'know', 'feel', 'people', 'life', \"getting\", \"lot\" \"great\", \"i\", \"me\", \n",
    "         \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "        \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "        \"they\", \"them\", \"their\", \"theirs\",\"themselves\", \"what\", \"which\", \"who\", \n",
    "        \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "        \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\",\n",
    "        \"might\", \"must\", \"shall\", \"ought\", \"about\", \"above\", \"across\", \"after\", \n",
    "        \"against\", \"along\", \"amid\", \"among\", \"around\", \"as\", \"at\", \"before\", \"behind\",\n",
    "        \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\", \"by\", \n",
    "        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\",\n",
    "        \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"notwithstanding\",\n",
    "        \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\",\n",
    "        \"regarding\", \"round\", \"since\", \"than\", \"through\", \"throughout\", \"till\", \n",
    "        \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\",\n",
    "        \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\", \"cant\", \"cannot\", \n",
    "        \"couldve\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \n",
    "        \"havent\", \"hed\", \"hell\", \"hes\", \"howd\", \"howll\", \"hows\", \"id\", \"ill\", \n",
    "        \"im\", \"ive\", \"isnt\", \"itd\", \"itll\", \"its\", \"lets\", \"mightve\", \"mustve\", \n",
    "        \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldve\", \"shouldnt\", \n",
    "        \"thatll\", \"thats\", \"thered\", \"therell\", \"therere\", \"theres\", \"theyd\", \n",
    "        \"theyll\", \"theyre\", \"theyve\", \"wed\", \"well\", \"were\", \"weve\", \"werent\", \n",
    "        \"whatd\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"whend\", \"whenll\", \n",
    "        \"whens\", \"whered\", \"wherell\", \"wheres\", \"whichd\", \"whichll\", \"whichre\", \n",
    "        \"whichs\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\", \"whyd\", \"whyll\", \n",
    "        \"whys\", \"wont\", \"wouldve\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "        \"f\", \"m\", \"because\", \"go\", \"lot\", \"get\", \"still\", \"way\", \"something\", \"much\",\n",
    "        \"thing\", \"someone\", \"person\", \"anything\", \"goes\", \"ok\", \"so\", \"just\", \"mostly\", \n",
    "        \"put\", \"also\", \"lots\", \"yet\", \"ha\", \"etc\"]\n",
    "\n",
    "time = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \n",
    "        \"sunday\", \"morning\", \"noon\", \"afternoon\", \"evening\", \"night\", \"midnight\",\n",
    "        \"dawn\", \"dusk\", \"week\", \"weekend\", \"weekends\",\"weekly\", \"today\", \n",
    "        \"yesterday\", \"tomorrow\", \"yesterdays\", \"todays\", \"mondays\", \"tuesdays\",\n",
    "        \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\", \"day\",\n",
    "        \"everyday\", \"daily\", \"workday\", 'time', 'month', 'year', 'pm', 'am', \"ago\",\n",
    "        \"year\"]\n",
    "\n",
    "reddit = [\"welcome\", \"hi\", \"hello\", \"sub\", \"reddit\", \"thanks\", \"thank\", \"maybe\",\n",
    "          \"wo30\", \"mods\", \"mod\", \"moderators\", \"subreddit\", \"btw\", \"aw\", \"aww\", \n",
    "          \"aww\", \"hey\", \"hello\", \"join\", \"joined\", \"post\", \"rselfimprovement\"]\n",
    "\n",
    "topic_specific = [\"self\", \"improvement\", \"change\", \"action\",\n",
    "    'change', 'start', 'goal', 'habit', 'new', 'old', \n",
    "    'care', 'world', 'everyone', 'love', 'u', 'right', 'mean', 'matter',\n",
    "    'best', 'step', 'focus', 'hard', 'small',\n",
    "    'bad', 'help', 'time', 'problem', 'issue', 'advice',\n",
    "    'bit', 'experience', 'different',\n",
    "    'point', 'situation', 'negative', 'control', 'positive',\n",
    "    'use', 'question', 'idea', 'amp', 'medium', 'hour', 'day', 'minute',\n",
    "    'aaaaloot', \"selfimprovement\", \"_\"]\n",
    "\n",
    "stopwords = english + time + reddit + topic_specific\n",
    "\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95e33a63-8c34-4a81-8fa1-157c86492364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sparknlp.annotator import NGramGenerator\n",
    "\n",
    "#ngrammer = NGramGenerator() \\\n",
    " #   .setInputCols(['normalized']) \\\n",
    "  #  .setOutputCol('ngrams') \\\n",
    "   # .setN(3) \\\n",
    "    #.setEnableCumulative(True) \\\n",
    "    #.setDelimiter('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5843616-767e-448f-8943-006848263e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pos = PerceptronModel.load(\"/project/macs40123/spark-jars/pos_anc_en_3.0.0_3.0_1614962126490/\")\\\n",
    "      .setInputCols(\"document\", \"unigrams\")\\\n",
    "      .setOutputCol(\"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20199976-1358-47e2-8685-a8e54cc73200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher\n",
    "\n",
    "finisher = Finisher().setInputCols(['unigrams', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50a9dbb2-1f65-4c5b-826e-feedefbf682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          tokenizer,\n",
    "          normalizer,\n",
    "          stopwords_cleaner,\n",
    "          #ngrammer,\n",
    "          pos,\n",
    "          finisher\n",
    "      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "680ea3cb-e5cf-4de6-aae3-c8e4ac8a3258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+\n",
      "|    id|        cleaned_text|   finished_unigrams|        finished_pos|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "|1wyudh|i love technology...|[technology, cert...|[NN, JJ, NN, JJ, ...|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = my_pipeline.fit(sample_data)\n",
    "processed_data = pipelineModel.transform(sample_data)\n",
    "processed_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "198344d1-7dcc-4258-b5a1-4bba2466aac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'cleaned_text', 'finished_unigrams', 'finished_pos']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7598af2e-4e59-4115-877d-95bf5fe8552e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                   finished_unigrams|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[technology, certain, extent, okay, macbook, pro, gaming, pc, xbox, iphone, staples, selling, tec...|\n",
      "|[giving, away, hours, coaching, three, done, times, always, welcoming, process, move, pardon, shr...|\n",
      "|[chronically, fantasize, times, least, problematic, creates, great, disparity, disappointment, cu...|\n",
      "|[fucking, pissed, conduct, hs, interested, classes, given, weeks, school, left, school, pissing, ...|\n",
      "|[last, found, ex, found, boyfriend, one, hardest, weeks, together, years, child, together, slowly...|\n",
      "|[title, pretty, explanatory, apologize, throwaway, sure, protocol, deleted, personally, social, p...|\n",
      "|[recently, kids, made, crude, economic, version, behavioral, marble, jar, often, seen, classrooms...|\n",
      "|        [wake, early, every, single, early, last, messed, sleeping, schedule, days, waking, achieve]|\n",
      "|[recovering, neet, familiar, term, ask, comments, decided, working, online, started, freelance, t...|\n",
      "|[constantly, reminding, young, whole, ahead, retirement, community, recently, woman, told, others...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_data.select('finished_unigrams').show(10, truncate = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c9dbe0a-f678-4a30-b0be-52677577fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+\n",
      "|    id|        cleaned_text|   finished_unigrams|        finished_pos|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "|1wyudh|i love technology...|[technology, cert...|[NN, JJ, NN, JJ, ...|\n",
      "|3apk27|hi reddit   im gi...|[giving, away, ho...|[VBG, RB, NNS, VB...|\n",
      "|6t1o0t|i chronically fan...|[chronically, fan...|[RB, JJ, NNS, JJS...|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a88ada38-b7f7-43d9-a593-93b7ed787514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unigrams(finished_unigrams, finished_pos):\n",
    "    '''Filters individual words based on their POS tag'''\n",
    "    return [word for word, pos in zip(finished_unigrams, finished_pos)\n",
    "            if pos in ['JJ', 'NN', 'NNS', 'NNPS']]\n",
    "\n",
    "udf_filter_unigrams = F.udf(filter_unigrams, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4ebd6c8-ba26-47bf-9db8-f761f1a0e921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                            filtered_unigrams_by_pos|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[technology, certain, extent, okay, macbook, pc, xbox, iphone, staples, tech, fun, hand, hand, ti...|\n",
      "|[hours, times, process, move, pardon, offer, previous, offers, promise, twohour, session, powerfu...|\n",
      "|[fantasize, times, problematic, creates, great, disparity, disappointment, current, state, depres...|\n",
      "|[pissed, conduct, hs, interested, classes, weeks, school, school, tf, school, home, phone, watch,...|\n",
      "|[last, ex, weeks, years, child, realization, entire, years, asocial, slob, relationship, read, ti...|\n",
      "|[title, explanatory, throwaway, sure, protocol, social, pressures, powerful, forces, proud, ackno...|\n",
      "|[kids, crude, economic, version, behavioral, marble, jar, classrooms, behavior, marble, optional,...|\n",
      "|                                 [wake, early, single, early, last, messed, schedule, days, achieve]|\n",
      "|[neet, familiar, term, comments, online, freelance, translation, career, credentials, lose, proje...|\n",
      "|[young, whole, retirement, community, woman, others, age, number, mind, midst, water, aerobics, c...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_data = processed_data.withColumn('filtered_unigrams_by_pos', udf_filter_unigrams(\n",
    "                                                   F.col('finished_unigrams'),\n",
    "                                                   F.col('finished_pos')))\n",
    "\n",
    "processed_data.select(\"filtered_unigrams_by_pos\").show(10, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "817d0b5a-ff0d-4f00-bfb7-2fc8a993e388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'cleaned_text',\n",
       " 'finished_unigrams',\n",
       " 'finished_pos',\n",
       " 'filtered_unigrams_by_pos']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "241ee1e3-18af-4c54-95d9-d717e5cde253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that POS was done, lemmatization makes more sense at this point\n",
    "\n",
    "#Merge tokens as just one string to be able to take it as a document in the new Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "tokens_as_string = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_data = processed_data.withColumn('joined_tokens', tokens_as_string(F.col('filtered_unigrams_by_pos')))\n",
    "\n",
    "last_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('joined_tokens') \\\n",
    "     .setOutputCol('joined_document')\n",
    "\n",
    "last_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['joined_document']) \\\n",
    "     .setOutputCol('tokenized')\n",
    "     \n",
    "lemmatizer = LemmatizerModel.load(\"../models/lemma_ewt_en_3.4.3_3.0_1651416655397/\")\\\n",
    "      .setInputCols(\"tokenized\")\\\n",
    "      .setOutputCol(\"lemmatized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('final') \\\n",
    "     .setStopWords(stopwords)\n",
    "\n",
    "last_finisher = Finisher() \\\n",
    "     .setInputCols(['final']) \\\n",
    "\n",
    "last_pipeline = Pipeline() \\\n",
    "     .setStages([last_documentAssembler,                  \n",
    "                 last_tokenizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 last_finisher])\n",
    "\n",
    "final_data = last_pipeline.fit(processed_data).transform(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a1a0d7a-f91f-4ba5-89d4-68a5f2aaf873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------------+------------+------------------------+-------------+--------------+\n",
      "|    id|cleaned_text|finished_unigrams|finished_pos|filtered_unigrams_by_pos|joined_tokens|finished_final|\n",
      "+------+------------+-----------------+------------+------------------------+-------------+--------------+\n",
      "|1wyudh|  i love ...|       [techno...|  [NN, JJ...|              [techno...|   technol...|    [techno...|\n",
      "|3apk27|  hi redd...|       [giving...|  [VBG, R...|              [hours,...|   hours t...|    [proces...|\n",
      "|6t1o0t|  i chron...|       [chroni...|  [RB, JJ...|              [fantas...|   fantasi...|    [fantas...|\n",
      "|8eikjz|  hey rse...|       [fuckin...|  [VBG, J...|              [pissed...|   pissed ...|    [pissed...|\n",
      "|8j4p29|  last we...|       [last, ...|  [JJ, VB...|              [last, ...|   last ex...|    [last, ...|\n",
      "|96w9bd|  title i...|       [title,...|  [NN, RB...|              [title,...|   title e...|    [title,...|\n",
      "|9gwrhq|  recentl...|       [recent...|  [RB, NN...|              [kids, ...|   kids cr...|    [kid, c...|\n",
      "|a9009z|  my goal...|       [wake, ...|  [NN, JJ...|              [wake, ...|   wake ea...|    [wake, ...|\n",
      "|ad50hu|  hi ther...|       [recove...|  [VBG, N...|              [neet, ...|   neet fa...|    [neet, ...|\n",
      "|ap8bw5|  constan...|       [consta...|  [RB, VB...|              [young,...|   young w...|    [young,...|\n",
      "+------+------------+-----------------+------------+------------------------+-------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show(10, truncate=10)\n",
    "                                                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7deb738d-484f-45f8-b6a4-12430b06111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                      finished_final|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[technology, certain, extent, okay, macbook, pc, xbox, iphone, staple, tech, fun, hand, hand, cat...|\n",
      "|[process, move, pardon, offer, previous, offer, promise, twohour, session, powerful, transformati...|\n",
      "|[fantasize, problematic, create, great, disparity, disappointment, current, state, depressive, li...|\n",
      "|[pissed, conduct, hs, interested, class, school, school, tf, school, home, phone, watch, youtube,...|\n",
      "|[last, ex, child, realization, entire, asocial, slob, relationship, read, pain, entire, view, soc...|\n",
      "|[title, explanatory, throwaway, sure, protocol, social, pressure, powerful, force, proud, acknowl...|\n",
      "|[kid, crude, economic, version, behavioral, marble, jar, classrooms, behavior, marble, optional, ...|\n",
      "|                                         [wake, early, single, early, last, mess, schedule, achieve]|\n",
      "|[neet, familiar, term, comment, online, freelance, translation, career, credentials, lose, projec...|\n",
      "|[young, whole, retirement, community, woman, age, number, mind, midst, water, aerobics, class, re...|\n",
      "|[perfect, attractive, athletic, intelligent, college, student, family, peers, home, class, recall...|\n",
      "|[mild, cerebral, palsy, premature, lucky, case, severe, move, myriad, term, esteem, bullied, big,...|\n",
      "|[practical, tactical, deep, meaningful, connection, family, andor, partner, millennials, importan...|\n",
      "|[relationship, estranged, mental, health, childhood, little, fault, distance, voice, opinion, min...|\n",
      "|[realisation, husband, adores, colleague, wonderful, contact, okay, job, load, realise, expectati...|\n",
      "|[fix, creativity, spatial, memory, associations, curiosities, main, broaden, thematic, horizon, c...|\n",
      "|[guy, backstory, suffer, depression, feeling, inadequacy, unworthy, friendship, cause, due, menta...|\n",
      "|[routines, stress, newish, job, recent, college, grad, feeling, end, sleep, need, need, wake, bed...|\n",
      "|[improve, amazing, solve, strangers, internet, entire, paragraph, non, easy, period, tough, outco...|\n",
      "|             [interesting, apathy, school, degree, stable, job, future, family, worth, family, free]|\n",
      "|[likely, wrong, sure, fact, turn, sick, stomach, atm, content, sad, fear, late, able, kid, age, p...|\n",
      "|[little, story, mom, young, pregnant, single, young, mother, little, take, grandmother, father, s...|\n",
      "|[figure, marathon, figure, mile, huge, manageable, task, failure, large, simple, clich, break, la...|\n",
      "|[interesting, simple, room, food, room, brain, thoughtwant, initial, task, complete, initial, tas...|\n",
      "|[guy, mine, discord, place, harmful, big, deal, digital, technology, skyrocket, kid, technology, ...|\n",
      "|[pandemic, job, seem, productive, webinars, tos, trick, learnings, career, hold, client, industry...|\n",
      "|[disclaimer, long, sorry, typo, mold, version, sense, recent, college, grad, certain, gpa, colleg...|\n",
      "|[mindset, benefit, feeling, mind, high, level, anxiety, hopelessness, unmotivation, bright, side,...|\n",
      "|[big, implement, essential, consistent, manner, gym, deep, bed, big, wake, success, part, success...|\n",
      "|[last, job, processing, wrong, part, fault, part, fault, fault, changeable, part, changeable, bra...|\n",
      "|[guy, cycle, school, porn, youtube, game, result, classwork, look, answer, online, brain, long, c...|\n",
      "|[male, quarantine, homework, project, endeavors, hackathons, class, stem, summer, activity, super...|\n",
      "|[disorders, complete, failure, redeem, read, previous, threads, struggled, binge, disorder, confi...|\n",
      "|[school, progression, test, feedback, grade, gis, funcareer, youtube, tutorials, important, tutor...|\n",
      "|[place, guy, backstory, medical, student, turkey, iran, turkish, father, persian, mother, mother,...|\n",
      "|[deal, chronic, anxiety, entire, adult, cope, complete, task, gargantuan, take, reassurance, fell...|\n",
      "|[kind, realisation, metro, god, soul, identity, live, brain, body, adapt, environment, dirty, pla...|\n",
      "|[friend, tarot, reader, book, last, strong, resistance, instinct, honor, intuition, respectful, s...|\n",
      "|[reality, energy, decision, dark, assumption, future, complacent, failure, try, acceptance, reali...|\n",
      "|[absolute, truth, paradigm, part, imperfect, show, scars, existence, eat, junk, food, exercise, i...|\n",
      "|[emotional, pain, primary, reason, therapy, force, illusion, mile, reality, turn, process, abilit...|\n",
      "|[enough, eat, anyone, tip, appetitenot, extreme, course, body, little, body, usual, amount, hungr...|\n",
      "|[rut, everything, try, constant, loops, give, school, try, pressure, part, productive, try, tiny,...|\n",
      "|[sorry, try, deal, slump, progress, final, high, school, academics, hectic, exams, system, mental...|\n",
      "|[complicated, relationship, family, mother, child, family, member, contact, parent, maintain, con...|\n",
      "|[rare, mental, state, lucidity, productivity, adderall, mg, caffeine, suicidal, tip, depression, ...|\n",
      "|[main, laungage, difficult, anyone, smoke, weed, sister, mom, schizo, weed, little, strange, idk,...|\n",
      "|[okay, fap, video, game, ramble, pls, mind, adhd, basic, video, run, watch, complex, job, ssm, so...|\n",
      "|                                                     [tip, okay, breakup, stay, somebody, dependent]|\n",
      "|[themsevles, anxiety, certain, event, weed, substance, distracts, anxiety, substance, tolerance, ...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.select('finished_final').show(50, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d8812d7-fd39-46ab-889d-ed86ceba27e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'cleaned_text',\n",
       " 'finished_unigrams',\n",
       " 'finished_pos',\n",
       " 'filtered_unigrams_by_pos',\n",
       " 'joined_tokens',\n",
       " 'finished_final']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db230604-f5c6-447a-9b01-57976afeab68",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5fd16dff-01c4-43ff-ab72-f6824f2d8a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Apply TF-IDF filtering\n",
    "tfizer = CountVectorizer(inputCol='finished_final', outputCol='tf_features', minDF=0.01, maxDF=0.80)\n",
    "tf_model = tfizer.fit(final_data)\n",
    "tf_result = tf_model.transform(final_data)\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21437e2e-eb3e-4138-96c0-e14c164ea335",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "84caf9a8-3db1-47b0-867f-2b53bc791b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_topics = 5\n",
    "max_iter = 100\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features', seed=2503)\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5c9fae7f-a3cf-41c3-98b6-f9fecb709a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e4214784-d38e-49f4-ac19-59b0c40d4969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------------------+\n",
      "|topic|                                                                                topicWords|\n",
      "+-----+------------------------------------------------------------------------------------------+\n",
      "|    0|[mom, expert, drink, weight, mother, early, cold, long, tired, task, energy, word, talk...|\n",
      "|    1|[purpose, part, reason, journey, discipline, sex, social, line, adhd, sort, fear, simil...|\n",
      "|    2|[job, family, friend, ego, confidence, age, career, kid, depression, school, mental, ph...|\n",
      "|    3|[woman, result, conversation, body, man, give, school, true, sport, environment, test, ...|\n",
      "|    4|[book, anxiety, feeling, pain, certain, last, future, mind, read, look, free, relations...|\n",
      "+-----+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 15\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e34f3-6f1f-45db-a828-4e7453032713",
   "metadata": {},
   "source": [
    "# REMEMBER TO CHANGE DATASET TO COMPLETE ONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb18b857-5d49-4da2-af3b-07e641c49775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.073087460300905"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.logPerplexity(tfidf_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8186833c-1d1c-473d-b404-cdc241b2590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-175183.90217685243"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.logLikelihood(tfidf_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3979f4ae-b675-48ca-944b-af997c7745ed",
   "metadata": {},
   "source": [
    "Try with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74641e0d-3065-438d-8d13-fc1b3e86314f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {Param(parent='LDA_827c4141dbad', name='seed', doc='random seed.'): 2503, Param(parent='LDA_827c4141dbad', name='k', doc='The number of topics (clusters) to infer. Must be > 1.'): 5, Param(parent='LDA_827c4141dbad', name='maxIter', doc='max number of iterations (>= 0).'): 10}, Log Likelihood: -388229.90755375766, Perplexity: 6.7414869595950835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {Param(parent='LDA_827c4141dbad', name='seed', doc='random seed.'): 2503, Param(parent='LDA_827c4141dbad', name='k', doc='The number of topics (clusters) to infer. Must be > 1.'): 5, Param(parent='LDA_827c4141dbad', name='maxIter', doc='max number of iterations (>= 0).'): 20}, Log Likelihood: -382982.50760265486, Perplexity: 6.650367554176167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {Param(parent='LDA_827c4141dbad', name='seed', doc='random seed.'): 2503, Param(parent='LDA_827c4141dbad', name='k', doc='The number of topics (clusters) to infer. Must be > 1.'): 8, Param(parent='LDA_827c4141dbad', name='maxIter', doc='max number of iterations (>= 0).'): 10}, Log Likelihood: -391892.4418861631, Perplexity: 6.8050856854030926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 566:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {Param(parent='LDA_827c4141dbad', name='seed', doc='random seed.'): 2503, Param(parent='LDA_827c4141dbad', name='k', doc='The number of topics (clusters) to infer. Must be > 1.'): 8, Param(parent='LDA_827c4141dbad', name='maxIter', doc='max number of iterations (>= 0).'): 20}, Log Likelihood: -385150.12457071117, Perplexity: 6.6880075227597935\n",
      "Best parameters for perplexity\n",
      "{Param(parent='LDA_827c4141dbad', name='seed', doc='random seed.'): 2503, Param(parent='LDA_827c4141dbad', name='k', doc='The number of topics (clusters) to infer. Must be > 1.'): 5, Param(parent='LDA_827c4141dbad', name='maxIter', doc='max number of iterations (>= 0).'): 20}\n",
      "6.650367554176167\n",
      "------------------------------------------\n",
      "Best parameters for log likelihood\n",
      "{Param(parent='LDA_827c4141dbad', name='seed', doc='random seed.'): 2503, Param(parent='LDA_827c4141dbad', name='k', doc='The number of topics (clusters) to infer. Must be > 1.'): 5, Param(parent='LDA_827c4141dbad', name='maxIter', doc='max number of iterations (>= 0).'): 20}\n",
      "-382982.50760265486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lda.seed, [2503]) \\\n",
    "    .addGrid(lda.k, [5, 8]) \\\n",
    "    .addGrid(lda.maxIter, [10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    log_likelihood = model.logLikelihood(data)\n",
    "    perplexity = model.logPerplexity(data)\n",
    "    return log_likelihood, perplexity\n",
    "\n",
    "best_perplexity = 100\n",
    "best_log_likelihood = -99999999999\n",
    "best_param_map_ll = None\n",
    "best_param_map_pxty = None\n",
    "\n",
    "for param_map in paramGrid:\n",
    "    model = lda.copy(param_map).fit(tfidf_result)\n",
    "    log_likelihood, perplexity = evaluate_model(model, tfidf_result)\n",
    "    print(f\"Params: {param_map}, Log Likelihood: {log_likelihood}, Perplexity: {perplexity}\")\n",
    "    if perplexity < best_perplexity: \n",
    "        best_perplexity = perplexity\n",
    "        best_param_map_pxty = param_map\n",
    "    if log_likelihood > best_log_likelihood: \n",
    "        best_log_likelihood = log_likelihood\n",
    "        best_param_map_ll = param_map\n",
    "\n",
    "print(\"Best parameters for perplexity\")\n",
    "print(best_param_map_pxty)\n",
    "print(best_perplexity)\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Best parameters for log likelihood\")\n",
    "print(best_param_map_ll)\n",
    "print(best_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c4312-12d7-4cc9-860c-fea2cbb6fc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
