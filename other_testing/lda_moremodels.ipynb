{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cab5220-7c97-4a2e-8721-fe427bf0f309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "24/11/25 21:12:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/software/spark-3.3.2-el8-x86_64/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/25 21:20:02 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n",
      "24/11/25 21:20:02 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/11/25 21:20:02 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import Tokenizer, PerceptronModel\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from sparknlp.annotator import NGramGenerator\n",
    "from sparknlp.base import Finisher\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "data = spark.read.csv(\"../data_topicmodel.csv\", header= True).select([\"id\", \"cleaned_text\"])\n",
    "\n",
    "#Preprocessing\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "     .setInputCol(\"cleaned_text\")\\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols(['document'])\\\n",
    "            .setOutputCol('tokenized')\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') \n",
    "\n",
    "english = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \n",
    "    \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"did\", \n",
    "    \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n",
    "    \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "    \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"must\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \n",
    "    \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"some\", \"such\", \n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"will\", \"ll\", \n",
    "    \"re\", \"ve\", \"d\", \"s\", \"m\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "    \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"many\", \"us\", \"ok\", \"hows\", \"ive\", \"ill\", \"im\", \"cant\", \"topics\", \"topic\",\n",
    "    \"discuss\", \"thoughts\", \"yo\", \"thats\", \"whats\", \"lets\", \"nothing\", \"oh\", \"omg\", \n",
    "         \"things\", \"stuff\", \"yall\", \"haha\", \"yes\", \"no\", \"wo\", \"like\", 'good', \n",
    "         'work', 'got', 'going', 'dont', 'really', 'want', 'make', 'think', \n",
    "         'know', 'feel', 'people', 'life', \"getting\", \"lot\" \"great\", \"i\", \"me\", \n",
    "         \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "        \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "        \"they\", \"them\", \"their\", \"theirs\",\"themselves\", \"what\", \"which\", \"who\", \n",
    "        \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "        \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\",\n",
    "        \"might\", \"must\", \"shall\", \"ought\", \"about\", \"above\", \"across\", \"after\", \n",
    "        \"against\", \"along\", \"amid\", \"among\", \"around\", \"as\", \"at\", \"before\", \"behind\",\n",
    "        \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\", \"by\", \n",
    "        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\",\n",
    "        \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"notwithstanding\",\n",
    "        \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\",\n",
    "        \"regarding\", \"round\", \"since\", \"than\", \"through\", \"throughout\", \"till\", \n",
    "        \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\",\n",
    "        \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\", \"cant\", \"cannot\", \n",
    "        \"couldve\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \n",
    "        \"havent\", \"hed\", \"hell\", \"hes\", \"howd\", \"howll\", \"hows\", \"id\", \"ill\", \n",
    "        \"im\", \"ive\", \"isnt\", \"itd\", \"itll\", \"its\", \"lets\", \"mightve\", \"mustve\", \n",
    "        \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldve\", \"shouldnt\", \n",
    "        \"thatll\", \"thats\", \"thered\", \"therell\", \"therere\", \"theres\", \"theyd\", \n",
    "        \"theyll\", \"theyre\", \"theyve\", \"wed\", \"well\", \"were\", \"weve\", \"werent\", \n",
    "        \"whatd\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"whend\", \"whenll\", \n",
    "        \"whens\", \"whered\", \"wherell\", \"wheres\", \"whichd\", \"whichll\", \"whichre\", \n",
    "        \"whichs\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\", \"whyd\", \"whyll\", \n",
    "        \"whys\", \"wont\", \"wouldve\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "        \"f\", \"m\", \"because\", \"go\", \"lot\", \"get\", \"still\", \"way\", \"something\", \"much\",\n",
    "        \"thing\", \"someone\", \"person\", \"anything\", \"goes\", \"ok\", \"so\", \"just\", \"mostly\", \n",
    "        \"put\", \"also\", \"lots\", \"yet\", \"ha\", \"etc\"]\n",
    "\n",
    "time = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \n",
    "        \"sunday\", \"morning\", \"noon\", \"afternoon\", \"evening\", \"night\", \"midnight\",\n",
    "        \"dawn\", \"dusk\", \"week\", \"weekend\", \"weekends\",\"weekly\", \"today\", \n",
    "        \"yesterday\", \"tomorrow\", \"yesterdays\", \"todays\", \"mondays\", \"tuesdays\",\n",
    "        \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\", \"day\",\n",
    "        \"everyday\", \"daily\", \"workday\", 'time', 'month', 'year', 'pm', 'am', \"ago\",\n",
    "        \"year\"]\n",
    "\n",
    "reddit = [\"welcome\", \"hi\", \"hello\", \"sub\", \"reddit\", \"thanks\", \"thank\", \"maybe\",\n",
    "          \"wo30\", \"mods\", \"mod\", \"moderators\", \"subreddit\", \"btw\", \"aw\", \"aww\", \n",
    "          \"aww\", \"hey\", \"hello\", \"join\", \"joined\", \"post\", \"rselfimprovement\"]\n",
    "\n",
    "topic_specific = [\"self\", \"improvement\", \"change\", \"action\",\n",
    "    'change', 'start', 'goal', 'habit', 'new', 'old', \n",
    "    'care', 'world', 'everyone', 'love', 'u', 'right', 'mean', 'matter',\n",
    "    'best', 'step', 'focus', 'hard', 'small',\n",
    "    'bad', 'help', 'time', 'problem', 'issue', 'advice',\n",
    "    'bit', 'experience', 'different',\n",
    "    'point', 'situation', 'negative', 'control', 'positive',\n",
    "    'use', 'question', 'idea', 'amp', 'medium', 'hour', 'day', 'minute',\n",
    "    'aaaaloot', \"selfimprovement\", \"_\"]\n",
    "\n",
    "stopwords = english + time + reddit + topic_specific\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(stopwords)\n",
    "\n",
    "pos = PerceptronModel.load(\"/project/macs40123/spark-jars/pos_anc_en_3.0.0_3.0_1614962126490/\")\\\n",
    "      .setInputCols(\"document\", \"unigrams\")\\\n",
    "      .setOutputCol(\"pos\")\n",
    "\n",
    "finisher = Finisher().setInputCols(['unigrams', 'pos'])\n",
    "\n",
    "my_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          tokenizer,\n",
    "          normalizer,\n",
    "          stopwords_cleaner,\n",
    "          #ngrammer,\n",
    "          pos,\n",
    "          finisher\n",
    "      ])\n",
    "\n",
    "pipelineModel = my_pipeline.fit(data)\n",
    "processed_data = pipelineModel.transform(data)\n",
    "processed_data.persist()\n",
    "\n",
    "#Filter by POS\n",
    "def filter_unigrams(finished_unigrams, finished_pos):\n",
    "    '''Filters individual words based on their POS tag'''\n",
    "    return [word for word, pos in zip(finished_unigrams, finished_pos)\n",
    "            if pos in ['JJ', 'NN', 'NNS', 'NNPS']]\n",
    "\n",
    "udf_filter_unigrams = F.udf(filter_unigrams, T.ArrayType(T.StringType()))\n",
    "\n",
    "processed_data = processed_data.withColumn('filtered_unigrams_by_pos', udf_filter_unigrams(\n",
    "                                                   F.col('finished_unigrams'),\n",
    "                                                   F.col('finished_pos')))\n",
    "\n",
    "#Now that POS was done, lemmatization makes more sense at this point\n",
    "\n",
    "#Merge tokens as just one string to be able to take it as a document in the new Pipeline\n",
    "tokens_as_string = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_data = processed_data.withColumn('joined_tokens', tokens_as_string(F.col('filtered_unigrams_by_pos')))\n",
    "\n",
    "last_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('joined_tokens') \\\n",
    "     .setOutputCol('joined_document')\n",
    "\n",
    "last_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['joined_document']) \\\n",
    "     .setOutputCol('tokenized')\n",
    "     \n",
    "lemmatizer = LemmatizerModel.load(\"../models/lemma_ewt_en_3.4.3_3.0_1651416655397/\")\\\n",
    "      .setInputCols(\"tokenized\")\\\n",
    "      .setOutputCol(\"lemmatized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('final') \\\n",
    "     .setStopWords(stopwords)\n",
    "\n",
    "last_finisher = Finisher() \\\n",
    "     .setInputCols(['final']) \\\n",
    "\n",
    "last_pipeline = Pipeline() \\\n",
    "     .setStages([last_documentAssembler,                  \n",
    "                 last_tokenizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 last_finisher])\n",
    "\n",
    "final_data = last_pipeline.fit(processed_data).transform(processed_data)\n",
    "\n",
    "processed_data.unpersist()\n",
    "final_data.persist()\n",
    "\n",
    "## Vectorization\n",
    "#Apply TF-IDF filtering\n",
    "tfizer = CountVectorizer(inputCol='finished_final', outputCol='tf_features', minDF=0.01, maxDF=0.80, vocabSize= 2000)\n",
    "tf_model = tfizer.fit(final_data)\n",
    "tf_result = tf_model.transform(final_data)\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)\n",
    "\n",
    "final_data.unpersist()\n",
    "tfidf_result.persist()\n",
    "\n",
    "## LDA\n",
    "lda = LDA(k=15, maxIter=50, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', seed=2503)\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ebfb836-5472-4074-8347-d0ff646c8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data):\n",
    "    log_likelihood = model.logLikelihood(data)\n",
    "    perplexity = model.logPerplexity(data)\n",
    "    return log_likelihood, perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b2571c-b54c-4d36-85cd-6f6c785e970b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-174302354.053725, 5.834824923979042)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(lda_model, tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59fdc37a-6d9d-4e10-9bcc-06fe27e303ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "       \n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ca6b15-7193-42b7-afe2-a0c380148266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                                        |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[brain, emotion, food, stress, tired, drug, effect, anger, lazy, feeling, memory, alcohol, addiction, emotional, healthy]                         |\n",
      "|1    |[anxiety, story, okay, family, friend, depression, voice, child, feeling, support, boundaries, fix, opinion, hair, heart]                         |\n",
      "|2    |[job, school, college, parent, class, kid, home, money, high, study, mom, degree, house, dad, university]                                         |\n",
      "|3    |[fear, choice, comment, pain, mindset, perfect, project, journey, growth, version, failure, challenge, journal, pressure, nobody]                 |\n",
      "|4    |[book, read, sleep, bed, wake, early, car, schedule, light, pick, drive, late, routine, risk, fall]                                               |\n",
      "|5    |[phone, meditation, task, practice, list, app, present, mind, fight, room, attention, environment, apps, break, busy]                             |\n",
      "|6    |[value, decision, happiness, money, path, purpose, happy, success, worth, career, direction, dream, successful, esteem, company]                  |\n",
      "|7    |[woman, man, sex, porn, gym, weight, cold, body, fat, skin, male, workout, average, attractive, bro]                                              |\n",
      "|8    |[word, behavior, mistakes, aware, desire, view, discipline, motivation, learn, language, knowledgeable, process, character, method, understanding]|\n",
      "|9    |[girl, guy, conversation, confidence, group, talk, nice, date, meet, confident, friend, relationship, eye, contact, interested]                   |\n",
      "|10   |[game, social, video, media, youtube, hobbies, smoke, hobby, content, fun, play, comfort, sport, music, zone]                                     |\n",
      "|11   |[health, mental, ampxb, physical, solution, doctor, exercise, body, system, push, muscle, training, weight, diet, professional]                   |\n",
      "|12   |[therapy, drink, partner, therapist, water, energy, relationship, op, walk, ready, meal, trauma, coffee, grateful, healthy]                       |\n",
      "|13   |[fuck, sad, shit, respect, response, regret, cycle, idk, several, actual, expectation, deserve, test, personality, stupid]                        |\n",
      "|14   |[belief, god, truth, responsibility, sound, information, joy, improve, human, yeah, wife, true, reality, edit, internet]                          |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 15\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d4c87b-466e-4cb8-a523-24ed912feb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/25 21:21:31 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## LDA\n",
    "lda = LDA(k=10, maxIter=50, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', seed=2503)\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98a1193f-020d-42ef-a12b-a458ae46a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-176176196.4664392, 5.897552375209811)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(lda_model, tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3527c0b4-7449-40c2-8926-43e123213eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                            |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[brain, health, food, mental, energy, stress, emotion, tired, motivation, healthy, smoke, drug, drink, addiction, effect]             |\n",
      "|1    |[anxiety, game, depression, therapy, story, family, therapist, voice, feeling, support, fix, friend, okay, video, boundaries]         |\n",
      "|2    |[job, school, college, parent, money, class, kid, home, high, house, mom, degree, family, dad, study]                                 |\n",
      "|3    |[fear, comment, pain, choice, op, project, mindset, perfect, shit, failure, challenge, responsibility, nobody, piece, music]          |\n",
      "|4    |[book, read, bed, sleep, skill, wake, list, video, hobby, early, schedule, routine, game, youtube, pick]                              |\n",
      "|5    |[media, social, phone, meditation, task, app, practice, attention, present, fight, mind, apps, instagram, account, busy]              |\n",
      "|6    |[value, happiness, decision, path, happy, success, purpose, worth, comfort, dream, relationship, successful, direction, future, money]|\n",
      "|7    |[woman, man, gym, weight, body, porn, sex, water, cold, muscle, workout, fat, meal, diet, skin]                                       |\n",
      "|8    |[word, ampxb, behavior, mistakes, belief, aware, view, response, desire, process, mind, trauma, understanding, feeling, emotion]      |\n",
      "|9    |[girl, guy, relationship, group, conversation, talk, date, confidence, friend, nice, meet, partner, social, confident, interest]      |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 15\n",
    "\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18546dcc-13c2-4a6e-a6a8-fc5fcdfb254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/25 21:22:42 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## LDA\n",
    "lda3 = LDA(k=10, maxIter=50, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', topicConcentration= 0.1, seed=2503)\n",
    "lda_model3 = lda3.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "129c6c73-28f0-44b9-b722-427908284e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-176176196.46643913, 5.897552375209811)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(lda_model3, tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1572aed9-4e50-42ee-b76f-20c7515426fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                            |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[brain, health, food, mental, energy, stress, emotion, tired, motivation, healthy, smoke, drug, drink, addiction, effect]             |\n",
      "|1    |[anxiety, game, depression, therapy, story, family, therapist, voice, feeling, support, fix, friend, okay, video, boundaries]         |\n",
      "|2    |[job, school, college, parent, money, class, kid, home, high, house, mom, degree, family, dad, study]                                 |\n",
      "|3    |[fear, comment, pain, choice, op, project, mindset, perfect, shit, failure, challenge, responsibility, nobody, piece, music]          |\n",
      "|4    |[book, read, bed, sleep, skill, wake, list, video, hobby, early, schedule, routine, game, youtube, pick]                              |\n",
      "|5    |[media, social, phone, meditation, task, app, practice, attention, present, fight, mind, apps, instagram, account, busy]              |\n",
      "|6    |[value, happiness, decision, path, happy, success, purpose, worth, comfort, dream, relationship, successful, direction, future, money]|\n",
      "|7    |[woman, man, gym, weight, body, porn, sex, water, cold, muscle, workout, fat, meal, diet, skin]                                       |\n",
      "|8    |[word, ampxb, behavior, mistakes, belief, aware, view, response, desire, process, mind, trauma, understanding, feeling, emotion]      |\n",
      "|9    |[girl, guy, relationship, group, conversation, talk, date, confidence, friend, nice, meet, partner, social, confident, interest]      |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 15\n",
    "\n",
    "topics = lda_model3.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e9ba11f-6e5c-4b4d-81d9-0e91949f224c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/25 21:23:53 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                            |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[emotion, brain, anxiety, stress, feeling, depression, emotional, drug, effect, anger, anxious, pattern, addiction, memory, tired]    |\n",
      "|1    |[story, support, family, voice, weird, hair, adult, brother, future, young, boundaries, moment, accept, balance, trust]               |\n",
      "|2    |[job, school, college, parent, class, kid, high, home, study, degree, money, house, university, student, country]                     |\n",
      "|3    |[fear, choice, pain, mindset, comment, decision, project, growth, nobody, pressure, perfect, journey, version, challenge, constant]   |\n",
      "|4    |[face, car, drive, light, half, potential, pick, space, skill, risk, shame, trouble, successful, turn, fall]                          |\n",
      "|5    |[phone, meditation, practice, task, list, app, attention, fight, break, present, mind, environment, apps, room, power]                |\n",
      "|6    |[value, money, happiness, path, worth, purpose, happy, success, esteem, direction, connection, career, company, individual, achieve]  |\n",
      "|7    |[woman, man, sex, porn, cold, attractive, male, standard, average, dude, bro, guy, beautiful, society, relationship]                  |\n",
      "|8    |[word, motivation, mistakes, desire, discipline, aware, learn, language, knowledgeable, view, plan, character, method, process, smart]|\n",
      "|9    |[girl, guy, conversation, confidence, group, talk, nice, confident, date, meet, friend, eye, contact, party, joke]                    |\n",
      "|10   |[social, game, video, media, youtube, hobbies, music, fun, hobby, comfort, play, activity, sport, interest, zone]                     |\n",
      "|11   |[mental, health, ampxb, physical, push, useful, training, strength, trauma, glad, amazing, struggle, necessary, suggestion, capable]  |\n",
      "|12   |[therapy, partner, therapist, drink, relationship, water, op, treat, energy, ready, grateful, judge, coffee, force, walk]             |\n",
      "|13   |[fuck, sad, personality, response, regret, respect, actual, several, cycle, expectation, name, deserve, childhood, open, anyways]     |\n",
      "|14   |[belief, truth, god, improve, reality, sound, true, responsibility, human, joy, wife, information, keep, source, edit]                |\n",
      "|15   |[okay, shit, yeah, suck, lol, wrong, fix, dream, stupid, heart, tough, idk, awesome, hate, cut]                                       |\n",
      "|16   |[opinion, failure, behavior, child, toxic, proud, side, complete, none, serious, boring, tend, doubt, level, relationship]            |\n",
      "|17   |[weight, food, gym, sleep, exercise, body, bed, wake, routine, healthy, diet, eat, workout, early, schedule]                          |\n",
      "|18   |[area, smoke, solution, system, doctor, quit, internet, test, rule, research, community, resource, development, limit, professional]  |\n",
      "|19   |[book, read, recommend, note, dad, term, journal, mom, message, busy, content, account, picture, miserable, short]                    |\n",
      "+-----+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## LDA\n",
    "lda4 = LDA(k=20, maxIter=50, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', topicConcentration= 0.1, seed=2503)\n",
    "lda_model4 = lda4.fit(tfidf_result)\n",
    "evaluate_model(lda_model4, tfidf_result)\n",
    "num_top_words = 15\n",
    "topics = lda_model4.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b5e291c-d7a8-4181-9743-e6e97350871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/25 21:25:35 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 792:==================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                           |\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[brain, emotion, food, stress, tired, drug, effect, anger, healthy, lazy, feeling, alcohol, eat, memory, emotional]                  |\n",
      "|1    |[anxiety, story, okay, family, friend, voice, child, feeling, fine, afraid, support, depression, boundaries, accept, heart]          |\n",
      "|2    |[job, school, college, parent, class, kid, money, home, high, mom, degree, study, house, dad, family]                                |\n",
      "|3    |[fear, choice, comment, mindset, pain, project, perfect, version, responsibility, growth, piece, journal, journey, failure, note]    |\n",
      "|4    |[book, read, sleep, bed, wake, early, schedule, car, light, pick, routine, late, risk, drive, information]                           |\n",
      "|5    |[phone, meditation, practice, task, list, app, present, attention, mind, fight, environment, apps, room, busy, break]                |\n",
      "|6    |[value, happiness, decision, path, purpose, happy, success, money, worth, dream, direction, successful, career, esteem, relationship]|\n",
      "|7    |[woman, man, sex, porn, gym, weight, cold, body, fat, skin, dude, male, bro, average, workout]                                       |\n",
      "|8    |[word, behavior, mistakes, belief, desire, view, aware, god, discipline, learn, character, true, understanding, truth, process]      |\n",
      "|9    |[girl, guy, conversation, group, talk, confidence, nice, date, meet, confident, friend, relationship, interested, interest, contact] |\n",
      "|10   |[game, social, video, media, youtube, hobbies, smoke, music, hobby, content, play, fun, comfort, internet, sport]                    |\n",
      "|11   |[health, mental, ampxb, physical, solution, doctor, exercise, system, push, useful, training, body, professional, weight, muscle]    |\n",
      "|12   |[therapy, drink, partner, therapist, water, op, relationship, energy, walk, ready, meal, trauma, coffee, grateful, treat]            |\n",
      "|13   |[fuck, shit, sad, respect, response, regret, idk, cycle, several, actual, deserve, kinda, stupid, yeah, test]                        |\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## LDA\n",
    "lda5 = LDA(k=14, maxIter=50, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', topicConcentration= 0.05, seed=2503)\n",
    "lda_model5 = lda5.fit(tfidf_result)\n",
    "evaluate_model(lda_model5, tfidf_result)\n",
    "num_top_words = 15\n",
    "topics = lda_model5.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1bde30a-4732-4758-b851-8fa355fc7dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-174550461.59692672, 5.843130400315031)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(lda_model5, tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff81b8a4-6ea9-4f39-aaf3-d6ad16cd5342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/25 21:27:41 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-175227136.81462666, 5.86578231139856)\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                             |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[brain, emotion, food, energy, stress, tired, healthy, drink, drug, effect, anger, cycle, lazy, feeling, alcohol]                      |\n",
      "|1    |[anxiety, family, feeling, story, okay, friend, voice, child, angry, afraid, fine, wrong, depression, anxious, everything]             |\n",
      "|2    |[job, school, college, parent, class, kid, home, money, high, house, mom, degree, study, dad, family]                                  |\n",
      "|3    |[fear, comment, pain, choice, project, mindset, perfect, nobody, piece, journal, version, grateful, deserve, pressure, shit]           |\n",
      "|4    |[book, read, bed, sleep, wake, early, schedule, failure, car, test, pick, light, routine, late, risk]                                  |\n",
      "|5    |[phone, meditation, task, practice, list, app, present, mind, attention, fight, room, apps, break, environment, moment]                |\n",
      "|6    |[value, happiness, decision, path, money, purpose, success, happy, worth, dream, career, successful, direction, relationship, esteem]  |\n",
      "|7    |[woman, man, weight, gym, body, porn, sex, water, cold, muscle, workout, fat, skin, bro, hair]                                         |\n",
      "|8    |[word, behavior, mistakes, belief, aware, desire, view, response, discipline, god, understanding, learn, process, true, mind]          |\n",
      "|9    |[girl, guy, relationship, conversation, group, talk, date, confidence, nice, meet, partner, friend, confident, personality, attractive]|\n",
      "|10   |[game, social, video, media, youtube, hobbies, music, smoke, hobby, play, content, fun, comfort, internet, watch]                      |\n",
      "|11   |[mental, health, therapy, ampxb, therapist, op, solution, professional, toxic, doctor, useful, trauma, physical, glad, system]         |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## LDA\n",
    "lda5 = LDA(k=12, maxIter=50, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', topicConcentration= 0.05, seed=2503)\n",
    "lda_model5 = lda5.fit(tfidf_result)\n",
    "print(evaluate_model(lda_model5, tfidf_result))\n",
    "num_top_words = 15\n",
    "topics = lda_model5.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bafc948-4902-4fb0-aa80-945389b93cc6",
   "metadata": {},
   "source": [
    "Create dataframe with topic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e46a1d-2988-46e3-a55e-539a7598fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to get topic distributions\n",
    "df_with_lda = lda_model.transform(tfidf_result)\n",
    "\n",
    "df_with_lda.select(\"topicDistribution\").show(1, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
