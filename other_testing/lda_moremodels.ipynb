{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cab5220-7c97-4a2e-8721-fe427bf0f309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "24/11/28 11:29:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/software/spark-3.3.2-el8-x86_64/jars/spark-core_2.12-3.3.2.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, cleaned_text: string, finished_unigrams: array<string>, finished_pos: array<string>, filtered_unigrams_by_pos: array<string>, joined_tokens: string, finished_final: array<string>, tf_features: vector, tf_idf_features: vector]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import Tokenizer, PerceptronModel\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from sparknlp.annotator import StopWordsCleaner\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from sparknlp.annotator import NGramGenerator\n",
    "from sparknlp.base import Finisher\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "data = spark.read.csv(\"../cleaned_moral_scores.csv\", header= True).select([\"id\", \"cleaned_text\"])\n",
    "\n",
    "#Preprocessing\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "     .setInputCol(\"cleaned_text\")\\\n",
    "     .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols(['document'])\\\n",
    "            .setOutputCol('tokenized')\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['tokenized']) \\\n",
    "     .setOutputCol('normalized') \n",
    "\n",
    "english = [\n",
    "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \n",
    "    \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"did\", \n",
    "    \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n",
    "    \"he\", \"her\", \"here\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \n",
    "    \"its\", \"itself\", \"let\", \"me\", \"more\", \"most\", \"must\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \n",
    "    \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"some\", \"such\", \n",
    "    \"than\", \"that\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \n",
    "    \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \n",
    "    \"while\", \"who\", \"whom\", \"why\", \"with\", \"would\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"will\", \"ll\", \n",
    "    \"re\", \"ve\", \"d\", \"s\", \"m\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \n",
    "    \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\", \"many\", \"us\", \"ok\", \"hows\", \"ive\", \"ill\", \"im\", \"cant\", \"topics\", \"topic\",\n",
    "    \"discuss\", \"thoughts\", \"yo\", \"thats\", \"whats\", \"lets\", \"nothing\", \"oh\", \"omg\", \n",
    "         \"things\", \"stuff\", \"yall\", \"haha\", \"yes\", \"no\", \"wo\", \"like\", 'good', \n",
    "         'work', 'got', 'going', 'dont', 'really', 'want', 'make', 'think', \n",
    "         'know', 'feel', 'people', 'life', \"getting\", \"lot\" \"great\", \"i\", \"me\", \n",
    "         \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "        \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \n",
    "        \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "        \"they\", \"them\", \"their\", \"theirs\",\"themselves\", \"what\", \"which\", \"who\", \n",
    "        \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "        \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "        \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"may\",\n",
    "        \"might\", \"must\", \"shall\", \"ought\", \"about\", \"above\", \"across\", \"after\", \n",
    "        \"against\", \"along\", \"amid\", \"among\", \"around\", \"as\", \"at\", \"before\", \"behind\",\n",
    "        \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\", \"by\", \n",
    "        \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"for\",\n",
    "        \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"next\", \"notwithstanding\",\n",
    "        \"of\", \"off\", \"on\", \"onto\", \"opposite\", \"out\", \"outside\", \"over\", \"past\",\n",
    "        \"regarding\", \"round\", \"since\", \"than\", \"through\", \"throughout\", \"till\", \n",
    "        \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"unlike\", \"until\", \"up\",\n",
    "        \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\", \"cant\", \"cannot\", \n",
    "        \"couldve\", \"couldnt\", \"didnt\", \"doesnt\", \"dont\", \"hadnt\", \"hasnt\", \n",
    "        \"havent\", \"hed\", \"hell\", \"hes\", \"howd\", \"howll\", \"hows\", \"id\", \"ill\", \n",
    "        \"im\", \"ive\", \"isnt\", \"itd\", \"itll\", \"its\", \"lets\", \"mightve\", \"mustve\", \n",
    "        \"mustnt\", \"shant\", \"shed\", \"shell\", \"shes\", \"shouldve\", \"shouldnt\", \n",
    "        \"thatll\", \"thats\", \"thered\", \"therell\", \"therere\", \"theres\", \"theyd\", \n",
    "        \"theyll\", \"theyre\", \"theyve\", \"wed\", \"well\", \"were\", \"weve\", \"werent\", \n",
    "        \"whatd\", \"whatll\", \"whatre\", \"whats\", \"whatve\", \"whend\", \"whenll\", \n",
    "        \"whens\", \"whered\", \"wherell\", \"wheres\", \"whichd\", \"whichll\", \"whichre\", \n",
    "        \"whichs\", \"whod\", \"wholl\", \"whore\", \"whos\", \"whove\", \"whyd\", \"whyll\", \n",
    "        \"whys\", \"wont\", \"wouldve\", \"wouldnt\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "        \"f\", \"m\", \"because\", \"go\", \"lot\", \"get\", \"still\", \"way\", \"something\", \"much\",\n",
    "        \"thing\", \"someone\", \"person\", \"anything\", \"goes\", \"ok\", \"so\", \"just\", \"mostly\", \n",
    "        \"put\", \"also\", \"lots\", \"yet\", \"ha\", \"etc\"]\n",
    "\n",
    "time = [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \n",
    "        \"sunday\", \"morning\", \"noon\", \"afternoon\", \"evening\", \"night\", \"midnight\",\n",
    "        \"dawn\", \"dusk\", \"week\", \"weekend\", \"weekends\",\"weekly\", \"today\", \n",
    "        \"yesterday\", \"tomorrow\", \"yesterdays\", \"todays\", \"mondays\", \"tuesdays\",\n",
    "        \"wednesdays\", \"thursdays\", \"fridays\", \"saturdays\", \"sundays\", \"day\",\n",
    "        \"everyday\", \"daily\", \"workday\", 'time', 'month', 'year', 'pm', 'am', \"ago\",\n",
    "        \"year\"]\n",
    "\n",
    "reddit = [\"welcome\", \"hi\", \"hello\", \"sub\", \"reddit\", \"thanks\", \"thank\", \"maybe\",\n",
    "          \"wo30\", \"mods\", \"mod\", \"moderators\", \"subreddit\", \"btw\", \"aw\", \"aww\", \n",
    "          \"aww\", \"hey\", \"hello\", \"join\", \"joined\", \"post\", \"rselfimprovement\", \"op\"]\n",
    "\n",
    "topic_specific = [\"self\", \"improvement\", \"change\", \"action\",\n",
    "    'change', 'start', 'goal', 'habit', 'new', 'old', \n",
    "    'care', 'world', 'everyone', 'love', 'u', 'right', 'mean', 'matter',\n",
    "    'best', 'step', 'focus', 'hard', 'small',\n",
    "    'bad', 'help', 'time', 'problem', 'issue', 'advice',\n",
    "    'bit', 'experience', 'different',\n",
    "    'point', 'situation', 'negative', 'control', 'positive',\n",
    "    'use', 'question', 'idea', 'amp', 'medium', 'hour', 'day', 'minute',\n",
    "    'aaaaloot', \"selfimprovement\", \"_\", \"ampxb\"]\n",
    "\n",
    "stopwords = english + time + reddit + topic_specific\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['normalized']) \\\n",
    "     .setOutputCol('unigrams') \\\n",
    "     .setStopWords(stopwords)\n",
    "\n",
    "pos = PerceptronModel.load(\"/project/macs40123/spark-jars/pos_anc_en_3.0.0_3.0_1614962126490/\")\\\n",
    "      .setInputCols(\"document\", \"unigrams\")\\\n",
    "      .setOutputCol(\"pos\")\n",
    "\n",
    "finisher = Finisher().setInputCols(['unigrams', 'pos'])\n",
    "\n",
    "my_pipeline = Pipeline(\n",
    "      stages = [\n",
    "          documentAssembler,\n",
    "          tokenizer,\n",
    "          normalizer,\n",
    "          stopwords_cleaner,\n",
    "          #ngrammer,\n",
    "          pos,\n",
    "          finisher\n",
    "      ])\n",
    "\n",
    "pipelineModel = my_pipeline.fit(data)\n",
    "processed_data = pipelineModel.transform(data)\n",
    "processed_data.persist()\n",
    "\n",
    "#Filter by POS\n",
    "def filter_unigrams(finished_unigrams, finished_pos):\n",
    "    '''Filters individual words based on their POS tag'''\n",
    "    return [word for word, pos in zip(finished_unigrams, finished_pos)\n",
    "            if pos in ['JJ', 'NN', 'NNS', 'NNPS']]\n",
    "\n",
    "udf_filter_unigrams = F.udf(filter_unigrams, T.ArrayType(T.StringType()))\n",
    "\n",
    "processed_data = processed_data.withColumn('filtered_unigrams_by_pos', udf_filter_unigrams(\n",
    "                                                   F.col('finished_unigrams'),\n",
    "                                                   F.col('finished_pos')))\n",
    "\n",
    "#Now that POS was done, lemmatization makes more sense at this point\n",
    "\n",
    "#Merge tokens as just one string to be able to take it as a document in the new Pipeline\n",
    "tokens_as_string = F.udf(lambda x: ' '.join(x), T.StringType())\n",
    "processed_data = processed_data.withColumn('joined_tokens', tokens_as_string(F.col('filtered_unigrams_by_pos')))\n",
    "\n",
    "last_documentAssembler = DocumentAssembler() \\\n",
    "     .setInputCol('joined_tokens') \\\n",
    "     .setOutputCol('joined_document')\n",
    "\n",
    "last_tokenizer = Tokenizer() \\\n",
    "     .setInputCols(['joined_document']) \\\n",
    "     .setOutputCol('tokenized')\n",
    "     \n",
    "lemmatizer = LemmatizerModel.load(\"../models/lemma_ewt_en_3.4.3_3.0_1651416655397/\")\\\n",
    "      .setInputCols(\"tokenized\")\\\n",
    "      .setOutputCol(\"lemmatized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "     .setInputCols(['lemmatized']) \\\n",
    "     .setOutputCol('final') \\\n",
    "     .setStopWords(stopwords)\n",
    "\n",
    "last_finisher = Finisher() \\\n",
    "     .setInputCols(['final']) \\\n",
    "\n",
    "last_pipeline = Pipeline() \\\n",
    "     .setStages([last_documentAssembler,                  \n",
    "                 last_tokenizer,\n",
    "                 lemmatizer,\n",
    "                 stopwords_cleaner,\n",
    "                 last_finisher])\n",
    "\n",
    "final_data = last_pipeline.fit(processed_data).transform(processed_data)\n",
    "\n",
    "processed_data.unpersist()\n",
    "final_data.persist()\n",
    "\n",
    "## Vectorization\n",
    "#Apply TF-IDF filtering\n",
    "tfizer = CountVectorizer(inputCol='finished_final', outputCol='tf_features', minDF=0.01, maxDF=0.80, vocabSize= 2000)\n",
    "tf_model = tfizer.fit(final_data)\n",
    "tf_result = tf_model.transform(final_data)\n",
    "\n",
    "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "idf_model = idfizer.fit(tf_result)\n",
    "tfidf_result = idf_model.transform(tf_result)\n",
    "\n",
    "final_data.unpersist()\n",
    "tfidf_result.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0e578-e01d-4d37-93a9-3791c25d73c8",
   "metadata": {},
   "source": [
    "Functions to evaluate and interpret model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ebfb836-5472-4074-8347-d0ff646c8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tf_model.vocabulary\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    log_likelihood = model.logLikelihood(data)\n",
    "    perplexity = model.logPerplexity(data)\n",
    "    return log_likelihood, perplexity\n",
    "\n",
    "def get_words(token_list):\n",
    "     return [vocab[token_id] for token_id in token_list]\n",
    "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d4c87b-466e-4cb8-a523-24ed912feb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/28 11:40:33 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n",
      "24/11/28 11:40:33 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/11/28 11:40:33 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## LDA\n",
    "lda = LDA(k=10, maxIter=50, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', seed=2503)\n",
    "lda_model = lda.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98a1193f-020d-42ef-a12b-a458ae46a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-175306522.5911957, 5.891725616579532)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(lda_model, tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3527c0b4-7449-40c2-8926-43e123213eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                                         |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[book, read, fear, mind, practice, brain, meditation, behavior, power, process, failure, challenge, growth, ability, personal]                     |\n",
      "|1    |[woman, man, girl, guy, sex, porn, attractive, dude, partner, addiction, car, shit, fuck, apps, male]                                              |\n",
      "|2    |[relationship, therapy, therapist, happy, happiness, date, toxic, personality, expectation, tough, friend, friendships, honest, mindset, character]|\n",
      "|3    |[kid, parent, child, family, mom, god, dream, dad, money, angry, adult, anger, mother, house, stupid]                                              |\n",
      "|4    |[feeling, anxiety, pain, list, task, emotion, moment, depression, comfort, response, sound, anxious, relationship, deal, everything]               |\n",
      "|5    |[conversation, social, talk, fun, group, interesting, interest, hobbies, music, voice, hobby, movie, interested, funny, weird]                     |\n",
      "|6    |[game, food, sleep, video, bed, exercise, wake, routine, healthy, diet, phone, drink, water, body, eat]                                            |\n",
      "|7    |[job, school, college, career, money, class, high, business, degree, online, community, university, student, country, grade]                       |\n",
      "|8    |[value, choice, decision, purpose, media, social, content, belief, discipline, test, term, environment, study, nature, balance]                    |\n",
      "|9    |[confidence, gym, confident, weight, body, muscle, low, hair, esteem, skin, training, shape, insecure, look, strength]                             |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 15\n",
    "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18546dcc-13c2-4a6e-a6a8-fc5fcdfb254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/28 11:43:35 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## LDA\n",
    "lda3 = LDA(k=10, maxIter=50, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', topicConcentration= 0.1, seed=2503)\n",
    "lda_model3 = lda3.fit(tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "129c6c73-28f0-44b9-b722-427908284e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-175306522.5911957, 5.891725616579532)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(lda_model3, tfidf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1572aed9-4e50-42ee-b76f-20c7515426fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                                         |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[book, read, fear, mind, practice, brain, meditation, behavior, power, process, failure, challenge, growth, ability, personal]                     |\n",
      "|1    |[woman, man, girl, guy, sex, porn, attractive, dude, partner, addiction, car, shit, fuck, apps, male]                                              |\n",
      "|2    |[relationship, therapy, therapist, happy, happiness, date, toxic, personality, expectation, tough, friend, friendships, honest, mindset, character]|\n",
      "|3    |[kid, parent, child, family, mom, god, dream, dad, money, angry, adult, anger, mother, house, stupid]                                              |\n",
      "|4    |[feeling, anxiety, pain, list, task, emotion, moment, depression, comfort, response, sound, anxious, relationship, deal, everything]               |\n",
      "|5    |[conversation, social, talk, fun, group, interesting, interest, hobbies, music, voice, hobby, movie, interested, funny, weird]                     |\n",
      "|6    |[game, food, sleep, video, bed, exercise, wake, routine, healthy, diet, phone, drink, water, body, eat]                                            |\n",
      "|7    |[job, school, college, career, money, class, high, business, degree, online, community, university, student, country, grade]                       |\n",
      "|8    |[value, choice, decision, purpose, media, social, content, belief, discipline, test, term, environment, study, nature, balance]                    |\n",
      "|9    |[confidence, gym, confident, weight, body, muscle, low, hair, esteem, skin, training, shape, insecure, look, strength]                             |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 15\n",
    "topics = lda_model3.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e10af4d-bf00-4a19-adeb-61c3ee7a5caa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccf5d6-1e22-49f1-9cdc-197ae746b85d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff81b8a4-6ea9-4f39-aaf3-d6ad16cd5342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/28 11:46:41 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 630:=======================================>                (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-175494930.79244724, 5.898057665206587)\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                                          |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[book, read, mind, practice, fear, belief, purpose, power, success, result, learn, personal, challenge, important, ability]                         |\n",
      "|1    |[woman, man, girl, guy, sex, porn, attractive, date, addiction, fuck, shit, partner, dude, drug, girlfriend]                                        |\n",
      "|2    |[relationship, confidence, therapy, value, therapist, happy, happiness, confident, worth, opinion, toxic, personality, respect, esteem, expectation]|\n",
      "|3    |[kid, parent, family, child, mom, stupid, god, dad, adult, mistakes, angry, anger, mother, grateful, dream]                                         |\n",
      "|4    |[feeling, anxiety, list, depression, emotion, pain, moment, task, mind, stress, comfort, response, break, anxious, deal]                            |\n",
      "|5    |[social, group, media, conversation, fun, talk, interest, music, interesting, hobbies, game, video, interested, hobby, voice]                       |\n",
      "|6    |[gym, weight, food, body, sleep, exercise, bed, drink, healthy, wake, routine, water, game, diet, motivation]                                       |\n",
      "|7    |[job, school, money, college, career, class, high, study, business, degree, university, student, country, grade, passion]                           |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## LDA\n",
    "lda5 = LDA(k=8, maxIter=100, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', topicConcentration= 0.05, seed=2503)\n",
    "lda_model5 = lda5.fit(tfidf_result)\n",
    "print(evaluate_model(lda_model5, tfidf_result))\n",
    "num_top_words = 15\n",
    "topics = lda_model5.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bafc948-4902-4fb0-aa80-945389b93cc6",
   "metadata": {},
   "source": [
    "Create dataframe with topic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "761d86a4-e415-4aea-87da-559a9d83d33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/28 11:53:06 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1242:======================================>                (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-175071829.05512756, 5.883838004081948)\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                                      |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[book, read, practice, fear, mind, brain, meditation, power, challenge, learn, process, behavior, failure, ability, success]                    |\n",
      "|1    |[woman, man, girl, guy, sex, porn, partner, date, attractive, relationship, fuck, shit, dude, girlfriend, male]                                 |\n",
      "|2    |[confidence, therapy, relationship, happy, happiness, therapist, confident, toxic, worth, opinion, esteem, low, personality, expectation, tough]|\n",
      "|3    |[kid, parent, family, child, mom, god, stupid, adult, dad, dream, angry, anger, mother, house, mistakes]                                        |\n",
      "|4    |[feeling, anxiety, emotion, moment, pain, depression, list, mind, emotional, response, deal, comfort, anxious, sound, everything]               |\n",
      "|5    |[group, conversation, fun, talk, interest, hobbies, music, interesting, social, game, interested, hobby, meet, friend, voice]                   |\n",
      "|6    |[gym, weight, body, food, sleep, exercise, bed, drink, game, wake, routine, healthy, video, water, diet]                                        |\n",
      "|7    |[job, school, college, money, career, class, high, business, degree, study, community, university, student, country, grade]                     |\n",
      "|8    |[media, social, decision, choice, phone, purpose, value, term, content, discipline, motivation, app, environment, nature, productive]           |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## LDA\n",
    "lda5 = LDA(k=9, maxIter=100, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', topicConcentration= 0.05, seed=2503)\n",
    "lda_model5 = lda5.fit(tfidf_result)\n",
    "print(evaluate_model(lda_model5, tfidf_result))\n",
    "num_top_words = 15\n",
    "topics = lda_model5.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a31086cb-8010-4847-8ef2-f9a14fc8e47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/28 11:56:14 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1548:======================================>                (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-175494930.79244724, 5.898057665206587)\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                                          |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[book, read, mind, practice, fear, belief, purpose, power, success, result, learn, personal, challenge, important, ability]                         |\n",
      "|1    |[woman, man, girl, guy, sex, porn, attractive, date, addiction, fuck, shit, partner, dude, drug, girlfriend]                                        |\n",
      "|2    |[relationship, confidence, therapy, value, therapist, happy, happiness, confident, worth, opinion, toxic, personality, respect, esteem, expectation]|\n",
      "|3    |[kid, parent, family, child, mom, stupid, god, dad, adult, mistakes, angry, anger, mother, grateful, dream]                                         |\n",
      "|4    |[feeling, anxiety, list, depression, emotion, pain, moment, task, mind, stress, comfort, response, break, anxious, deal]                            |\n",
      "|5    |[social, group, media, conversation, fun, talk, interest, music, interesting, hobbies, game, video, interested, hobby, voice]                       |\n",
      "|6    |[gym, weight, food, body, sleep, exercise, bed, drink, healthy, wake, routine, water, game, diet, motivation]                                       |\n",
      "|7    |[job, school, money, college, career, class, high, study, business, degree, university, student, country, grade, passion]                           |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lda5 = LDA(k=8, maxIter=100, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', topicConcentration= 0.05, seed=2503)\n",
    "lda_model5 = lda5.fit(tfidf_result)\n",
    "print(evaluate_model(lda_model5, tfidf_result))\n",
    "num_top_words = 15\n",
    "topics = lda_model5.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d7867f5-0e6c-4d79-8484-425b3070fa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/11/28 11:59:22 WARN OnlineLDAOptimizer: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1854:======================================>                (7 + 3) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-175494930.79244724, 5.898057665206586)\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                                          |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[book, read, mind, practice, fear, belief, purpose, power, success, result, learn, personal, challenge, important, ability]                         |\n",
      "|1    |[woman, man, girl, guy, sex, porn, attractive, date, addiction, fuck, shit, partner, dude, drug, girlfriend]                                        |\n",
      "|2    |[relationship, confidence, therapy, value, therapist, happy, happiness, confident, worth, opinion, toxic, personality, respect, esteem, expectation]|\n",
      "|3    |[kid, parent, family, child, mom, stupid, god, dad, adult, mistakes, angry, anger, mother, grateful, dream]                                         |\n",
      "|4    |[feeling, anxiety, list, depression, emotion, pain, moment, task, mind, stress, comfort, response, break, anxious, deal]                            |\n",
      "|5    |[social, group, media, conversation, fun, talk, interest, music, interesting, hobbies, game, video, interested, hobby, voice]                       |\n",
      "|6    |[gym, weight, food, body, sleep, exercise, bed, drink, healthy, wake, routine, water, game, diet, motivation]                                       |\n",
      "|7    |[job, school, money, college, career, class, high, study, business, degree, university, student, country, grade, passion]                           |\n",
      "+-----+----------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lda_13 = LDA(k=13, maxIter=100, learningDecay=0.5, learningOffset = 50, featuresCol='tf_idf_features', topicConcentration= 0.05, seed=2503)\n",
    "lda_model3 = lda_13.fit(tfidf_result)\n",
    "print(evaluate_model(lda_model5, tfidf_result))\n",
    "num_top_words = 15\n",
    "topics = lda_model5.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2f12a0f-c202-44e8-87b0-8c9d26f2fb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|topicWords                                                                                                                                   |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[emotion, fear, behavior, mind, failure, mistakes, power, process, desire, emotional, ability, respect, feeling, challenge, attention]       |\n",
      "|1    |[sex, porn, partner, car, addiction, drug, smart, drive, dude, number, risk, shit, normal, guy, fuck]                                        |\n",
      "|2    |[woman, man, girl, relationship, guy, date, personality, attractive, happiness, toxic, beautiful, male, expectation, happy, girlfriend]      |\n",
      "|3    |[parent, kid, family, child, money, house, dream, mom, adult, god, dad, angry, anger, mother, brother]                                       |\n",
      "|4    |[feeling, friend, relationship, sound, pain, yeah, wasnt, trust, sorry, comment, honest, wrong, happy, comfort, everything]                  |\n",
      "|5    |[group, conversation, fun, talk, interest, interesting, hobbies, social, kinda, voice, lol, interested, drink, movie, meet]                  |\n",
      "|6    |[food, healthy, weight, low, diet, eat, smoke, fat, excuse, quit, esteem, meal, body, idk, rid]                                              |\n",
      "|7    |[job, school, college, class, career, high, money, degree, business, community, study, university, country, student, grade]                  |\n",
      "|8    |[value, decision, choice, purpose, belief, path, content, human, environment, direction, important, nature, opinion, lesson, test]           |\n",
      "|9    |[gym, confidence, confident, body, face, muscle, fix, hair, improve, build, training, look, workout, insecure, shape]                        |\n",
      "|10   |[book, game, read, video, media, list, social, youtube, motivation, skill, project, play, passion, plan, internet]                           |\n",
      "|11   |[anxiety, therapy, depression, therapist, meditation, mental, helpful, music, anxious, practice, health, doctor, journal, professional, rule]|\n",
      "|12   |[sleep, phone, bed, task, wake, routine, energy, app, cold, tired, exercise, schedule, early, break, track]                                  |\n",
      "+-----+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics = lda_model3.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
    "topics.select('topic', 'topicWords').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67c82e9-32eb-4a55-aa30-aae3e78834b8",
   "metadata": {},
   "source": [
    "Try to get coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6c3ed48-fb7f-45e2-9ed8-eb8762ff3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def calculate_coherence_score(lda_model, vectorized_data, vocab, top_n=10):\n",
    "    \"\"\"\n",
    "    Calculate coherence scores for a PySpark LDA topic model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    lda_model : pyspark.ml.clustering.LDAModel\n",
    "        The trained PySpark LDA model.\n",
    "    vectorized_data : pyspark.sql.DataFrame\n",
    "        The vectorized data used for training the LDA model.\n",
    "    vocab : list\n",
    "        Vocabulary from the CountVectorizer.\n",
    "    top_n : int, optional\n",
    "        Number of top words to consider for each topic (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        A list of coherence scores for each topic.\n",
    "    \"\"\"\n",
    "    # Extract topics and their top words\n",
    "    topics = lda_model.describeTopics(maxTermsPerTopic=top_n).collect()\n",
    "    \n",
    "    # Convert sparse vector to a word-document matrix\n",
    "    word_doc_matrix = vectorized_data.select(\"tf_idf_features\").rdd \\\n",
    "        .flatMap(lambda x: [(i, 1) for i in x[\"tf_idf_features\"].indices]) \\\n",
    "        .toDF([\"word_id\", \"count\"]) \\\n",
    "        .groupBy(\"word_id\").agg(F.sum(\"count\").alias(\"doc_count\"))\n",
    "    \n",
    "    # Map word_id back to vocabulary\n",
    "    vocab_df = spark.createDataFrame([(i, w) for i, w in enumerate(vocab)], [\"word_id\", \"word\"])\n",
    "    word_doc_matrix = word_doc_matrix.join(vocab_df, on=\"word_id\")\n",
    "    \n",
    "    # Calculate total number of documents\n",
    "    total_docs = vectorized_data.count()\n",
    "    \n",
    "    coherence_scores = []\n",
    "    for topic in topics:\n",
    "        topic_words = topic[\"termIndices\"]\n",
    "        \n",
    "        # Calculate word co-occurrence\n",
    "        pairwise_coherence = []\n",
    "        for i, word1 in enumerate(topic_words):\n",
    "            for word2 in topic_words[i + 1:]:\n",
    "                # Document counts for each word\n",
    "                doc_count_word1 = word_doc_matrix.filter(F.col(\"word_id\") == word1).select(\"doc_count\").first()[0]\n",
    "                doc_count_word2 = word_doc_matrix.filter(F.col(\"word_id\") == word2).select(\"doc_count\").first()[0]\n",
    "                \n",
    "                # Joint document count for both words\n",
    "                joint_doc_count = vectorized_data.select(\"features\").rdd \\\n",
    "                    .filter(lambda row: word1 in row[\"features\"].indices and word2 in row[\"features\"].indices) \\\n",
    "                    .count()\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if joint_doc_count > 0:\n",
    "                    p_word1_word2 = joint_doc_count / total_docs\n",
    "                    p_word1 = doc_count_word1 / total_docs\n",
    "                    pairwise_coherence.append(np.log((p_word1_word2 + 1e-12) / p_word1))\n",
    "        \n",
    "        # Average coherence for the topic\n",
    "        coherence_scores.append(np.mean(pairwise_coherence) if pairwise_coherence else 0)\n",
    "    \n",
    "    return coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dedf4b5f-ea31-4fac-99ce-38428eeeadff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unable to infer the type of the field word_id.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/types.py:1270\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/types.py:1302\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[1;32m   1304\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'numpy.int32'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/types.py:1308\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m-> 1308\u001b[0m         StructField(k, \u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1309\u001b[0m     )\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/types.py:1272\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot supported type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(obj))\n",
      "\u001b[0;31mTypeError\u001b[0m: not supported type: <class 'numpy.int32'>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcalculate_coherence_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mcalculate_coherence_score\u001b[0;34m(lda_model, vectorized_data, vocab, top_n)\u001b[0m\n\u001b[1;32m     27\u001b[0m topics \u001b[38;5;241m=\u001b[39m lda_model\u001b[38;5;241m.\u001b[39mdescribeTopics(maxTermsPerTopic\u001b[38;5;241m=\u001b[39mtop_n)\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Convert sparse vector to a word-document matrix\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m word_doc_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorized_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf_idf_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf_idf_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_count\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Map word_id back to vocabulary\u001b[39;00m\n\u001b[1;32m     36\u001b[0m vocab_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([(i, w) \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vocab)], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/session.py:102\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=1)]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/session.py:934\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m--> 934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/session.py:600\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 600\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    602\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/session.py:553\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    551\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m samplingRatio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rdd\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m100\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[0;32m/software/spark-3.3.2-el8-x86_64/python/pyspark/sql/types.py:1311\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1307\u001b[0m         fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1308\u001b[0m             StructField(k, _infer_type(v, infer_dict_as_struct, prefer_timestamp_ntz), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1309\u001b[0m         )\n\u001b[1;32m   1310\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to infer the type of the field \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to infer the type of the field word_id."
     ]
    }
   ],
   "source": [
    "calculate_coherence_score(lda_model3, tfidf_result, vocab, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e46a1d-2988-46e3-a55e-539a7598fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to get topic distributions\n",
    "df_with_lda = lda_model.transform(tfidf_result)\n",
    "\n",
    "df_with_lda.select(\"topicDistribution\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a22be-31ab-4b08-bbb8-087f13e21ac3",
   "metadata": {},
   "source": [
    "Create topic labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9669ae-e8c1-446f-8b40-4bc9ae914f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the function to get the topic label\n",
    "def get_topic_label(vector):\n",
    "    '''\n",
    "    Takes the topic distribution for each document and returns a label\n",
    "\n",
    "    Input (numpy array): topic probabilities distribution\n",
    "    Output (list of int): list of integers corresponding to topics\n",
    "    '''\n",
    "\n",
    "    #Convert numpy array into a DenseVector\n",
    "    dense_vector = DenseVector(vector)\n",
    "        \n",
    "    #Create columns for each topic "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
